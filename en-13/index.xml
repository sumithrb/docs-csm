<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Cray System Management Documentation on Cray System Management (CSM)</title>
    <link>/docs-csm/en-13/</link>
    <description>Recent content in Cray System Management Documentation on Cray System Management (CSM)</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-US</language>
    <lastBuildDate>Mon, 17 Jul 2023 03:16:14 +0000</lastBuildDate><atom:link href="/docs-csm/en-13/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>SLS Utils Library</title>
      <link>/docs-csm/en-13/upgrade/scripts/sls/sls_utils/readme/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:48 +0000</pubDate>
      
      <guid>/docs-csm/en-13/upgrade/scripts/sls/sls_utils/readme/</guid>
      <description>sls_utils Library This is a reusable Python library for safely interacting with SLS network data (in JSON format).
The library has been tested against Python version 3.6 and up.</description>
    </item>
    
    <item>
      <title>Upgrade Automation</title>
      <link>/docs-csm/en-13/upgrade/scripts/upgrade/upgrade_automation/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:48 +0000</pubDate>
      
      <guid>/docs-csm/en-13/upgrade/scripts/upgrade/upgrade_automation/</guid>
      <description>Upgrade Automation This document describes how the CSM upgrade process is automated.
Overview In a nutshell, an upgrade is:
 Upload new artifacts into a running system. Rebuild each NCN with the newly uploaded images. Deploy new charts after all NCNs are upgraded.  Prerequisites Everything that is needed before upgrading an NCN.
 NOTE:
The prerequisites.sh script is required to run on both ncn-m001 and ncn-m002 (at different points in the upgrade process).</description>
    </item>
    
    <item>
      <title>Cephadm</title>
      <link>/docs-csm/en-13/upgrade/resource_material/storage/cephadm-reference/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:47 +0000</pubDate>
      
      <guid>/docs-csm/en-13/upgrade/resource_material/storage/cephadm-reference/</guid>
      <description>CEPHADM cephadm is a new function introduced in Ceph Octopus 15. It allows for an easier method to install and manage Ceph nodes.
Traditional Ceph commands On ncn-s001, ncn-s002, or ncn-s003:
cephadm shell The previous command creates a container and opens an interactive shell with access to run Ceph commands the traditional way.
Alternatively, execute the command as follows:
cephadm shell -- ceph -s Ceph Device Operations There are multiple ways to do Ceph device operations now.</description>
    </item>
    
    <item>
      <title>Resource Materials</title>
      <link>/docs-csm/en-13/upgrade/resource_material/readme/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:47 +0000</pubDate>
      
      <guid>/docs-csm/en-13/upgrade/resource_material/readme/</guid>
      <description>Resource Materials Files in this directory (and its sub-directories) are provided as reference material in support of the automated scripts which are intended to execute as much as possible during the upgrade process.
Topics  Worker Reference Cephadm Reference  </description>
    </item>
    
    <item>
      <title>Worker-specific Manual Steps</title>
      <link>/docs-csm/en-13/upgrade/resource_material/k8s/worker-reference/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:47 +0000</pubDate>
      
      <guid>/docs-csm/en-13/upgrade/resource_material/k8s/worker-reference/</guid>
      <description>Worker-Specific Manual Steps   Determine if the worker being rebuilt is running a cray-cps-cm-pm pod, by running the cray cps deployment list command below. If so, there is a final step to redeploy this pod once the worker is rebuilt. In the example below, nodes ncn-w001, ncn-w002, and ncn-w003 have the pod.
 NOTE: If the command below does not return any pod names, proceed to step 2. NOTE: A 404 error is expected if the COS product is not installed on the system.</description>
    </item>
    
    <item>
      <title>1.2.0 Or Later To 1.3.0 Upgrade Process</title>
      <link>/docs-csm/en-13/upgrade/upgrade_management_nodes_and_csm_services/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:46 +0000</pubDate>
      
      <guid>/docs-csm/en-13/upgrade/upgrade_management_nodes_and_csm_services/</guid>
      <description>CSM 1.2.0 or later to 1.3.0 Upgrade Process Introduction This document guides an administrator through the upgrade of Cray Systems Management from v1.0 to v1.2. When upgrading a system, follow this top-level file from top to bottom. The content on this top-level page is meant to be terse. For additional reference material on the upgrade processes and scripts mentioned explicitly on this page, see resource material.
Important Notes Service request adjustments are needed for small systems   For systems with only three worker nodes (typically Testing and Development Systems (TDS)), prior to proceeding with this upgrade, CPU limits MUST be lowered on several services in order for this upgrade to succeed.</description>
    </item>
    
    <item>
      <title>Prepare For Upgrade</title>
      <link>/docs-csm/en-13/upgrade/prepare_for_upgrade/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:46 +0000</pubDate>
      
      <guid>/docs-csm/en-13/upgrade/prepare_for_upgrade/</guid>
      <description>Prepare For Upgrade Before beginning an upgrade to a new version of CSM, there are a few things to do on the system first.
 Reduced resiliency during upgrade Export Nexus data Disable encryption for upgrade if enabled Start typescript Running sessions Health validation Stop typescript Preparation completed  Reduced resiliency during upgrade Warning: Management service resiliency is reduced during the upgrade.
Although it is expected that compute nodes and application nodes will continue to provide their services without interruption, it is important to be aware that the degree of management services resiliency is reduced during the upgrade.</description>
    </item>
    
    <item>
      <title>Stage 1 - Ceph Image Upgrade</title>
      <link>/docs-csm/en-13/upgrade/stage_1/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:46 +0000</pubDate>
      
      <guid>/docs-csm/en-13/upgrade/stage_1/</guid>
      <description>Stage 1 - Ceph image upgrade Reminder: If any problems are encountered and the procedure or command output does not provide relevant guidance, see Relevant troubleshooting links for upgrade-related issues.
 Stage 1 - Ceph image upgrade  Start typescript Run Ceph Latency Repair Script Apply boot order workaround Argo workflows CSM Upgrade requirement for upgrades staying within a CSM release version Storage node image upgrade Ensure that rbd stats monitoring is enabled Stop typescript Stage completed    Start typescript   (ncn-m001#) If a typescript session is already running in the shell, then first stop it with the exit command.</description>
    </item>
    
    <item>
      <title>Stage 2 - Kubernetes Upgrade</title>
      <link>/docs-csm/en-13/upgrade/stage_2/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:46 +0000</pubDate>
      
      <guid>/docs-csm/en-13/upgrade/stage_2/</guid>
      <description>Stage 2 - Kubernetes Upgrade Reminder: If any problems are encountered and the procedure or command output does not provide relevant guidance, see Relevant troubleshooting links for upgrade-related issues.
 Start typescript on ncn-m001 Stage 2.1 - Master node image upgrade Argo workflows Stage 2.2 - Worker node image upgrade  Option 1 - Serial upgrade Option 2 - Parallel upgrade (Tech preview)  Restrictions Example     Stage 2.</description>
    </item>
    
    <item>
      <title>Stage 3 - Service Upgrades</title>
      <link>/docs-csm/en-13/upgrade/stage_3/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:46 +0000</pubDate>
      
      <guid>/docs-csm/en-13/upgrade/stage_3/</guid>
      <description>Stage 3 - CSM Service Upgrades Reminder: If any problems are encountered and the procedure or command output does not provide relevant guidance, see Relevant troubleshooting links for upgrade-related issues.
 Start typescript Perform upgrade Verify Keycloak users Take Etcd Manual Backup Stop typescript Stage completed  Start typescript   (ncn-m002#) If a typescript session is already running in the shell, then first stop it with the exit command.</description>
    </item>
    
    <item>
      <title>Stage 4 - Ceph Upgrade</title>
      <link>/docs-csm/en-13/upgrade/stage_4/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:46 +0000</pubDate>
      
      <guid>/docs-csm/en-13/upgrade/stage_4/</guid>
      <description>Stage 4 - Ceph Upgrade Important: Ceph does not need to be upgraded if this is an upgrade is staying within a CSM release, e.g. CSM-1.3.0-rc1 to CSM-1.3.0-rc2. If this is an upgrade staying within a CSM release, then Ceph is already running v16.2.9. See instructions in stage completed for next steps.
Reminder: If any problems are encountered and the procedure or command output does not provide relevant guidance, then see Relevant troubleshooting links for upgrade-related issues.</description>
    </item>
    
    <item>
      <title>1.2.x To 1.3.x Upgrade Process</title>
      <link>/docs-csm/en-13/upgrade/readme/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:45 +0000</pubDate>
      
      <guid>/docs-csm/en-13/upgrade/readme/</guid>
      <description>CSM 1.2.x to 1.3.x Upgrade Process The process for upgrading Cray Systems Management (CSM) has many steps in multiple procedures which should be done in a specific order.
After the upgrade of CSM software, the CSM health checks will validate the system before doing any other operational tasks like the check and update of firmware on system components. Once the CSM upgrade has completed, other product streams for the HPE Cray EX system can be installed or upgraded.</description>
    </item>
    
    <item>
      <title>1.3.1 Patch Installation Instructions</title>
      <link>/docs-csm/en-13/upgrade/1.3.1/readme/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:45 +0000</pubDate>
      
      <guid>/docs-csm/en-13/upgrade/1.3.1/readme/</guid>
      <description>CSM 1.3.1 Patch Installation Instructions Introduction This document guides an administrator through the patch update to Cray Systems Management v1.3.1 from v1.3.0. If upgrading from CSM v1.2.2 directly to v1.3.1, follow the procedures described in Upgrade CSM instead.
Bug Fixes and Improvements  Update cfs-operator for fixed session memory limits. Fix HSN NIC numbering in SMD for devices managed by HPE Proliant iLO (Redfish). Restrict accessible Keycloak endpoints in OPA Policy.</description>
    </item>
    
    <item>
      <title>1.3.2 Patch Installation Instructions</title>
      <link>/docs-csm/en-13/upgrade/1.3.2/readme/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:45 +0000</pubDate>
      
      <guid>/docs-csm/en-13/upgrade/1.3.2/readme/</guid>
      <description>CSM 1.3.2 Patch Installation Instructions Introduction This document guides an administrator through the patch update to Cray Systems Management (CSM) v1.3.2 from v1.3.0 or v1.3.1. If upgrading from CSM v1.2.2 directly to v1.3.2, follow the procedures described in Upgrade CSM instead.
Bug Fixes and Improvements  Fixed a scale issue with the /v1/hardware API in the System Layout Service (SLS) causing the service to become deadlocked. Updated the hms-hmcollector to be accessible at the hostname hmcollector.</description>
    </item>
    
    <item>
      <title>1.3.3 Patch Installation Instructions</title>
      <link>/docs-csm/en-13/upgrade/1.3.3/readme/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:45 +0000</pubDate>
      
      <guid>/docs-csm/en-13/upgrade/1.3.3/readme/</guid>
      <description>CSM 1.3.3 Patch Installation Instructions Introduction This document guides an administrator through the patch update to Cray Systems Management (CSM) v1.3.3 from v1.3.0, v1.3.1, or v1.3.2. If upgrading from CSM v1.2.2 directly to v1.3.3, follow the procedures described in Upgrade CSM instead.
Bug Fixes and Improvements  Added monitoring and a grafana dashboard for SMF kafka server and zookeeper metrics in prometheus. Fixed authentication failure with the keycloak integration into Nexus due to a change to the keycloak opa policy which was recently patched.</description>
    </item>
    
    <item>
      <title>1.3.4 Patch Installation Instructions</title>
      <link>/docs-csm/en-13/upgrade/1.3.4/readme/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:45 +0000</pubDate>
      
      <guid>/docs-csm/en-13/upgrade/1.3.4/readme/</guid>
      <description>CSM 1.3.4 Patch Installation Instructions Introduction This document guides an administrator through the patch update to Cray Systems Management (CSM) v1.3.4 from v1.3.0, v1.3.1, v1.3.2 or v1.3.3. If upgrading from CSM v1.2.2 directly to v1.3.4, follow the procedures described in Upgrade CSM instead.
Bug Fixes and Improvements  Added monitoring and a grafana dashboard for SMF kafka server and zookeeper metrics in prometheus. Fixed authentication failure with the keycloak integration into Nexus due to a change to the keycloak opa policy which was recently patched.</description>
    </item>
    
    <item>
      <title>Stage 0 - Prerequisites And Preflight Checks</title>
      <link>/docs-csm/en-13/upgrade/stage_0_prerequisites/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:45 +0000</pubDate>
      
      <guid>/docs-csm/en-13/upgrade/stage_0_prerequisites/</guid>
      <description>Stage 0 - Prerequisites and Preflight Checks  Reminders:
 CSM 1.2.0 or higher is required in order to upgrade to CSM 1.3.0. If any problems are encountered and the procedure or command output does not provide relevant guidance, see Relevant troubleshooting links for upgrade-related issues.   Stage 0 has several critical procedures which prepare the environment and verify if the environment is ready for the upgrade.
 Start typescript Stage 0.</description>
    </item>
    
    <item>
      <title>PXE Booting Runbook</title>
      <link>/docs-csm/en-13/troubleshooting/pxe_runbook/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:44 +0000</pubDate>
      
      <guid>/docs-csm/en-13/troubleshooting/pxe_runbook/</guid>
      <description>PXE Booting Runbook PXE booting is a key component of a working Shasta system. There are a lot of different components involved, which increases the complexity. This guide runs through the most common issues and shows what is needed in order to have a successful PXE boot.
 NCNs on install ncn-m001 on reboot or NCN boot 2.1. Verify DHCP packets can be forwarded from the workers to the MTL network (VLAN1) 2.</description>
    </item>
    
    <item>
      <title>Troubleshoot Kubernetes Pods Not Starting</title>
      <link>/docs-csm/en-13/troubleshooting/kubernetes/troubleshoot_kubernetes_pods_not_starting/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:44 +0000</pubDate>
      
      <guid>/docs-csm/en-13/troubleshooting/kubernetes/troubleshoot_kubernetes_pods_not_starting/</guid>
      <description>Troubleshoot Kubernetes Pods Not Starting Use this procedure to check if Kubernetes pods get scheduled on an NCN, but do not eventually reach the Running state.
Prerequisites The kubectl get pod command returns pods that seem to be stuck in the Init or ContainerCreating state.
Identify the node in question   Run the kubectl get pod -o wide command to identify the node where the pod is not starting.</description>
    </item>
    
    <item>
      <title>Troubleshoot Liveliness Or Readiness Probe Failures</title>
      <link>/docs-csm/en-13/troubleshooting/kubernetes/troubleshoot_liveliness_readiness_probe_failures/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:44 +0000</pubDate>
      
      <guid>/docs-csm/en-13/troubleshooting/kubernetes/troubleshoot_liveliness_readiness_probe_failures/</guid>
      <description>Troubleshoot Liveliness or Readiness Probe Failures Identify and troubleshoot Readiness or Liveliness probes that report services as unhealthy intermittently.
This is a known issue and can be classified into two categories, connection refused and client timeout. The commands in this procedure assume the user is logged into either a master or worker non-compute node (NCN).
 Troubleshoot a refused connection  Refused connection - symptom Refused connection - procedure   Troubleshoot a client timeout  Client timeout - symptom Client timeout - procedure   Next steps  Troubleshoot a refused connection Refused connection - symptom (ncn-mw#) The symptom of this problem is a connection refused message in the event log.</description>
    </item>
    
    <item>
      <title>Troubleshoot Unresponsive Kubectl Commands</title>
      <link>/docs-csm/en-13/troubleshooting/kubernetes/troubleshoot_unresponsive_kubectl_commands/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:44 +0000</pubDate>
      
      <guid>/docs-csm/en-13/troubleshooting/kubernetes/troubleshoot_unresponsive_kubectl_commands/</guid>
      <description>Troubleshoot Unresponsive kubectl Commands Use this procedure to check if any kworkers are in an error state because of a high load. Once the error has been identified, workaround the issue by returning the high load to a normal level.
Symptoms One or more of the following issues are possible symptoms of this issue.
 The kubectl command can become unresponsive because of a high load. ps aux cannot return or complete because of aspects of the /proc file system being locked.</description>
    </item>
    
    <item>
      <title>Update Product Stream</title>
      <link>/docs-csm/en-13/update_product_stream/readme/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:44 +0000</pubDate>
      
      <guid>/docs-csm/en-13/update_product_stream/readme/</guid>
      <description>Update CSM Product Stream The software included in the CSM product stream is released in more than one way. The initial product release may be augmented with patches, documentation updates, or hotfixes after the release.
The CSM documentation is included within the CSM product release tarball inside the docs-csm RPM. After the RPM has been installed, the documentation will be available at /usr/share/doc/csm.
 Download and extract CSM product release Apply patch to CSM release  Prerequisites Procedure   Check for latest documentation Check for field notices about hotfixes  Download and extract CSM product release Acquire a CSM software release tarball for installation on the HPE Cray EX supercomputer.</description>
    </item>
    
    <item>
      <title></title>
      <link>/docs-csm/en-13/troubleshooting/known_issues/wait_for_unbound_hang/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:43 +0000</pubDate>
      
      <guid>/docs-csm/en-13/troubleshooting/known_issues/wait_for_unbound_hang/</guid>
      <description>#Wait_for_unbound or cray-dns-unbound-manager hangs
  Run the following command:
kubectl get jobs -n services | grep cray-dns-unbound-manager services cray-dns-unbound-manager-1635352560 0/1 26h 26h services cray-dns-unbound-manager-1635448680 1/1 35s 8m37s services cray-dns-unbound-manager-1635448860 1/1 51s 5m36s services cray-dns-unbound-manager-1635449040 1/1 61s 2m35s    If you see one of the jobs show 0/1 for more than 10 minutes and there are other runs with 1/1. That means that job is hung. You can delete the job with:</description>
    </item>
    
    <item>
      <title>Known Issue Velero Version Mismatch</title>
      <link>/docs-csm/en-13/troubleshooting/known_issues/velero_version_mismatch/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:43 +0000</pubDate>
      
      <guid>/docs-csm/en-13/troubleshooting/known_issues/velero_version_mismatch/</guid>
      <description>Known Issue: Velero Version Mismatch In CSM 1.3 the Velero client and server versions differ after CSM is installed. This is not known to cause any problems with backup and restore functionality, but this page will document how to correct this situation if needed.
ncn-m001:~ # velero version Client: Version: v1.5.2 Git commit: e115e5a191b1fdb5d379b62a35916115e77124a4 Server: Version: v1.6.3 Fix Run the following command on master and worker nodes to deploy the v1.</description>
    </item>
    
    <item>
      <title>Kubernetes Log File Locations</title>
      <link>/docs-csm/en-13/troubleshooting/kubernetes/kubernetes_log_file_locations/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:43 +0000</pubDate>
      
      <guid>/docs-csm/en-13/troubleshooting/kubernetes/kubernetes_log_file_locations/</guid>
      <description>Kubernetes Log File Locations Locations of various K8s log types on the system.
   Log Type Component Purpose Location     Kubernetes Master API server Responsible for serving the API kubectl -n kube-system logs -l component=kube-apiserver   Scheduler Responsible for making scheduling decisions kubectl -n kube-system logs -l component=kube-scheduler    Controller Manages replication controllers kubectl -n kube-system logs -l component=kube-controller-manager    Kubernetes Worker Kubelet Responsible for running containers on the node journalctl -xeu kubelet   Kube proxy Responsible for service load balancing kubectl -n kube-system logs -l k8s-app=kube-proxy     </description>
    </item>
    
    <item>
      <title>Kubernetes Troubleshooting Information</title>
      <link>/docs-csm/en-13/troubleshooting/kubernetes/kubernetes_troubleshooting_information/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:43 +0000</pubDate>
      
      <guid>/docs-csm/en-13/troubleshooting/kubernetes/kubernetes_troubleshooting_information/</guid>
      <description>Kubernetes Troubleshooting Information Commands for performing basic Kubernetes cluster troubleshooting.
 Access pod logs Describe a node Describe a pod Open a shell on a pod Run a single command on a pod Connect to a running container Scale a deployment Remove a deployment with the manifest and reapply the deployment Delete a pod  Access pod logs (ncn-mw#) Use one of the following commands to retrieve pod-related logs:
kubectl logs POD_NAME kubectl logs POD_NAME -c CONTAINER_NAME (ncn-mw#) If the pods keeps crashing, open a log for the previous instance using the following command:</description>
    </item>
    
    <item>
      <title>Ssl Certificate Validation Issues</title>
      <link>/docs-csm/en-13/troubleshooting/known_issues/ssl_certificate_validation_issues/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:43 +0000</pubDate>
      
      <guid>/docs-csm/en-13/troubleshooting/known_issues/ssl_certificate_validation_issues/</guid>
      <description>SSL Certificate Validation Issues 1 SSL validation fails during the installation process If the intermediate CA that is used to sign service certificates changes after the NCNs are brought up, then this causes the platform-ca on the NCNs to no longer be valid. This is due to the platform-ca only being pulled via cloud-init on first boot. Run the following Goss test to validate this is the case.
1.1 Error messages /opt/cray/tests/ncn-resources/hms/hms-test/hms_run_ct_smoke_tests_ncn-resources.</description>
    </item>
    
    <item>
      <title>Troubleshoot Kubernetes Master Or Worker Node In Notready State</title>
      <link>/docs-csm/en-13/troubleshooting/kubernetes/troubleshoot_kubernetes_node_notready/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:43 +0000</pubDate>
      
      <guid>/docs-csm/en-13/troubleshooting/kubernetes/troubleshoot_kubernetes_node_notready/</guid>
      <description>Troubleshoot Kubernetes Master or Worker node in NotReady state Use this procedure to check if a Kubernetes master or worker node is in a NotReady state.
Identify the node in question   (ncn-mw#) Identify the node in NotReady state.
kubectl get nodes Example output:
NAME STATUS ROLES AGE VERSION ncn-m001 Ready control-plane,master 27h v1.20.13 ncn-m002 Ready control-plane,master 8d v1.20.13 ncn-m003 Ready control-plane,master 8d v1.20.13 ncn-w001 NotReady &amp;lt;none&amp;gt; 8d v1.20.13 ncn-w002 Ready &amp;lt;none&amp;gt; 8d v1.</description>
    </item>
    
    <item>
      <title>Known Issues With NCN Resource Checks</title>
      <link>/docs-csm/en-13/troubleshooting/known_issues/ncn_resource_checks/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:42 +0000</pubDate>
      
      <guid>/docs-csm/en-13/troubleshooting/known_issues/ncn_resource_checks/</guid>
      <description>Known issues with NCN resource checks   pods_not_running
  If the output of pods_not_running indicates that there are pods in the Evicted state, it may be because of the root file system being filled up on the Kubernetes node in question. Kubernetes will begin evicting pods once the root file system space is at 85% full, and will continue to evict them until it is back under 80%. This commonly happens on ncn-m001, because it is a location where install and documentation files may have been downloaded.</description>
    </item>
    
    <item>
      <title>Kubernetes Master Or Worker Node&#39;s Root Filesystem Is Out Of Space</title>
      <link>/docs-csm/en-13/troubleshooting/known_issues/kubernetes_node_rootfs_out_of_space/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:42 +0000</pubDate>
      
      <guid>/docs-csm/en-13/troubleshooting/known_issues/kubernetes_node_rootfs_out_of_space/</guid>
      <description>Kubernetes Master or Worker node&amp;rsquo;s root filesystem is out of space Description There is a known bug in Kubernetes 1.19.9 where movement of a pod with an attached volume may not complete in time and cause the kubelet service to stream error messages to the /var/log/messages log file. If this goes unchecked, it will fill up the root file system.
Fix   Log into the node that has space issues.</description>
    </item>
    
    <item>
      <title>Mellanox Lacp-individual Limitations</title>
      <link>/docs-csm/en-13/troubleshooting/known_issues/mellanox_lacp_individual/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:42 +0000</pubDate>
      
      <guid>/docs-csm/en-13/troubleshooting/known_issues/mellanox_lacp_individual/</guid>
      <description>Mellanox lacp-individual limitations Description In some failover/maintenance scenarios admins may want to shutdown one port of the bond on an NCN. Due to the way Mellanox handles lacp-individual mode the ports need to be shutdown from the switch instead of the NCN.
Fix Shut down the port on the switch instead of the NCN.</description>
    </item>
    
    <item>
      <title>Qlogic Driver Crash</title>
      <link>/docs-csm/en-13/troubleshooting/known_issues/qlogic_driver_crash/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:42 +0000</pubDate>
      
      <guid>/docs-csm/en-13/troubleshooting/known_issues/qlogic_driver_crash/</guid>
      <description>QLogic driver crash  Description  Signatures  Driver Recovery Kernel Crash     Workaround Fix  Description In some failover/maintenance scenarios users may experience QLogic driver crash with the following symptoms:
 Sudden loss of connectivity, mgmt0 and/or mgmt1 will lose connectivity and the bond will fail. Kernel crashing  This is known to be a result of network events, such as a reboot of a switch in the VSX pair, or a sudden flood of packets due to CEPH recovery.</description>
    </item>
    
    <item>
      <title>Software Management Services Health Checks</title>
      <link>/docs-csm/en-13/troubleshooting/known_issues/sms_health_check/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:42 +0000</pubDate>
      
      <guid>/docs-csm/en-13/troubleshooting/known_issues/sms_health_check/</guid>
      <description>Software Management Services health checks  SMS test execution Interpreting cmsdev Results Known issues with SMS tests  Cray CLI Etcd-restores    SMS test execution This test requires that the Cray CLI is configured on nodes where the test is executed. See Cray command line interface.
The following test can be run on any Kubernetes node (any master or worker node, but not on the PIT node).
/usr/local/bin/cmsdev test -q all  The cmsdev tool logs to /opt/cray/tests/cmsdev.</description>
    </item>
    
    <item>
      <title>Spire Database Cluster DNS Lookup Failure</title>
      <link>/docs-csm/en-13/troubleshooting/known_issues/spire_database_lookup_error/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:42 +0000</pubDate>
      
      <guid>/docs-csm/en-13/troubleshooting/known_issues/spire_database_lookup_error/</guid>
      <description>Spire Database Cluster DNS Lookup Failure Description There is a known issue where if Unbound is configured to forward to an invalid or inaccessible site DNS server, the Spire server may be unable to resolve the hostname of its PostgreSQL cluster.
Symptoms   The spire-server pods may be in a CrashLoopBackOff state.
  API calls to services may fail with HTTP 503 errors.
  The spire-server pods contain the following error in the logs.</description>
    </item>
    
    <item>
      <title>Spire Database Connection Pool Configuration In An Air-gapped Environment</title>
      <link>/docs-csm/en-13/troubleshooting/known_issues/spire_database_airgap_configuration/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:42 +0000</pubDate>
      
      <guid>/docs-csm/en-13/troubleshooting/known_issues/spire_database_airgap_configuration/</guid>
      <description>Spire database connection pool configuration in an air-gapped environment Description Due to the way the resolver code works in certain versions of Alpine Linux, it may be necessary to reconfigure the spire-postgres-pooler to use the fully qualified domain name of the database in order to prevent DNS lookup errors.
Symptoms   The spire-server pods are logging query_wait_timeout errors.
time=&amp;#34;2022-11-15T09:39:38Z&amp;#34; level=error msg=&amp;#34;Fatal run error&amp;#34; error=&amp;#34;datastore-sql: pq: query_wait_timeout&amp;#34; time=&amp;#34;2022-11-15T09:39:38Z&amp;#34; level=error msg=&amp;#34;Server crashed&amp;#34; error=&amp;#34;datastore-sql: pq: query_wait_timeout&amp;#34;   The spire-postgres-pooler pods are logging DNS lookup failure errors.</description>
    </item>
    
    <item>
      <title>HPE Ilo Dropping Event Subscriptions And Not Properly Transitioning Power State In Software</title>
      <link>/docs-csm/en-13/troubleshooting/known_issues/hpe_systems_not_transitioning_power_state/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:41 +0000</pubDate>
      
      <guid>/docs-csm/en-13/troubleshooting/known_issues/hpe_systems_not_transitioning_power_state/</guid>
      <description>HPE iLO dropping event subscriptions and not properly transitioning power state in CSM software HPE Systems impacted:
 DL325 DL385 Apollo 6500  When HPE iLO systems are not properly transitioning power state in HMS/SAT this could indicate that Redfish events are not being received by the HMS HM-Collector. When this occurs, the HPE iLO receives an error back from its attempt to send events and will delete the subscription if there are enough failures.</description>
    </item>
    
    <item>
      <title>Known Issue IMS Image Creation Failure</title>
      <link>/docs-csm/en-13/troubleshooting/known_issues/ims_image_creation_failure/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:41 +0000</pubDate>
      
      <guid>/docs-csm/en-13/troubleshooting/known_issues/ims_image_creation_failure/</guid>
      <description>Known Issue: IMS image creation failure On some systems, IMS image creation will fail with the following error in the CFS pod log:
Traceback (most recent call last): File &amp;#34;/usr/lib/python3.9/multiprocessing/process.py&amp;#34;, line 315, in _bootstrap self.run() File &amp;#34;/usr/lib/python3.9/multiprocessing/process.py&amp;#34;, line 108, in run self._target(*self._args, **self._kwargs) File &amp;#34;/usr/lib/python3.9/site-packages/cray/cfs/inventory/image/__init__.py&amp;#34;, line 239, in _request_ims_ssh mpq.put(ImageRootInventory._wait_for_ssh_container(ims_id, job_id, cfs_session)) File &amp;#34;/usr/lib/python3.9/site-packages/cray/cfs/inventory/image/__init__.py&amp;#34;, line 290, in _wait_for_ssh_container raise CFSInventoryError( cray.cfs.inventory.CFSInventoryError: (&amp;#39;IMS status=error for IMS image=%r job=%r, SSH container was not created.</description>
    </item>
    
    <item>
      <title>Known Issue Initrd.img.xz Not Found</title>
      <link>/docs-csm/en-13/troubleshooting/known_issues/initrd.img.zx_not_found/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:41 +0000</pubDate>
      
      <guid>/docs-csm/en-13/troubleshooting/known_issues/initrd.img.zx_not_found/</guid>
      <description>Known Issue: initrd.img.xz Not Found This is a problem that is fixed in CSM 1.0 and later, but if your system was upgraded from CSM 0.9 you may run into this. Below is the full error seen when attempting to boot:
Loading Linux ... Loading initial ramdisk ... error: file `/boot/grub2/../initrd.img.xz&#39; not found. Press any key to continue... [ 2.528752] Kernel panic - not syncing: VFS: Unable to mount root fs on unknown-block(0,0) [ 2.</description>
    </item>
    
    <item>
      <title>Known Issue Kubectl Logs -f Returns No Space Left On Device</title>
      <link>/docs-csm/en-13/troubleshooting/known_issues/kubectl_logs_no_space_left_on_device/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:41 +0000</pubDate>
      
      <guid>/docs-csm/en-13/troubleshooting/known_issues/kubectl_logs_no_space_left_on_device/</guid>
      <description>Known issue: kubectl logs -f returns no space left on device On some systems, running kubectl logs -n &amp;lt;NAMESPACE&amp;gt; &amp;lt;PODNAME&amp;gt; -f returns no space left on device. This can be caused by a lower limit for the sysctl setting fs.inotify.max_user_watches (defaults to 65536) in some kernel releases. This can be fixed by increasing this setting. Note that later versions of the kernel increase this setting by default.
Fix Run the following command from a master node.</description>
    </item>
    
    <item>
      <title>Known Issue Logging Into The Gitea Web Ui Requires Logging In Twice</title>
      <link>/docs-csm/en-13/troubleshooting/known_issues/gitea_web_ui_requires_two_logins/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:41 +0000</pubDate>
      
      <guid>/docs-csm/en-13/troubleshooting/known_issues/gitea_web_ui_requires_two_logins/</guid>
      <description>Known Issue: Logging into the Gitea web UI requires logging in twice When using the Gitea web UI, users are redirected to a keycloak login. The redirect is expected; however, the expectation is that logging in on this page should log users into Gitea. Unfortunately, that does not happen. Instead users are being redirected back to Gitea and have to login again through the Gitea web page using the existing git credentials.</description>
    </item>
    
    <item>
      <title>Known Issues With NCN Health Checks</title>
      <link>/docs-csm/en-13/troubleshooting/known_issues/issues_with_ncn_health_checks/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:41 +0000</pubDate>
      
      <guid>/docs-csm/en-13/troubleshooting/known_issues/issues_with_ncn_health_checks/</guid>
      <description>Known issues with NCN health checks   The first pass of running these tests may fail due to cloud-init not being completed on the storage nodes. In the case of failure, wait for five minutes and rerun the tests.
  Some of the tests will fail if the Cray CLI is not configured on the management NCNs. See Cray command line interface.
  For any failures related to SSL certificates, see the SSL Certificate Validation Issues troubleshooting guide.</description>
    </item>
    
    <item>
      <title>Antero Node Nid Allocation</title>
      <link>/docs-csm/en-13/troubleshooting/known_issues/antero_node_nid_allocation/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:40 +0000</pubDate>
      
      <guid>/docs-csm/en-13/troubleshooting/known_issues/antero_node_nid_allocation/</guid>
      <description>Antero node NID allocation There is a known issue with Antero nodes where node NIDs are not correctly allocated. When Cray Site Init (CSI) generates hardware for the liquid-cooled cabinets for the System Layout Service (SLS) input file it assumes all blades in the cabinet are Windom compute blades. Even though both Antero and Windom blades both have 4 nodes, they have different physical layouts.
 Windom blades have 2 node BMCs with 2 nodes per node BMC with the following nodes: b0n0, b0n1, b1n0, b1n1 Antero blades have 1 Node BMC with 4 nodes per node BMC with the following nodes: b0n0, b0n1, b0n2, b0n3  SLS has NIDS only allocated for nodes b0n0, b0n1, b1n0, b1n1 on a compute node blade.</description>
    </item>
    
    <item>
      <title>Cray Cli 403 Forbidden Errors</title>
      <link>/docs-csm/en-13/troubleshooting/known_issues/craycli_403_forbidden_errors/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:40 +0000</pubDate>
      
      <guid>/docs-csm/en-13/troubleshooting/known_issues/craycli_403_forbidden_errors/</guid>
      <description>Cray CLI 403 Forbidden Errors There is a known issue where the Keycloak configuration obtained from LDAP is incomplete causing the keycloak-users-localize job to fail to complete. This, in turn, causes 403 Forbidden errors when trying to use the cray CLI. This can also cause a Keycloak test to fail during CSM health validation.
Fix To recover from this situation, the following can be done.
  Log into the Keycloak admin console.</description>
    </item>
    
    <item>
      <title>Hms Discovery Job Not Creating Redfishendpoints In Hardware State Manager</title>
      <link>/docs-csm/en-13/troubleshooting/known_issues/discovery_job_not_creating_redfish_endpoints/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:40 +0000</pubDate>
      
      <guid>/docs-csm/en-13/troubleshooting/known_issues/discovery_job_not_creating_redfish_endpoints/</guid>
      <description>HMS Discovery Job Not Creating RedfishEndpoints In Hardware State Manager It is a known issue with the HMS Discovery cronjob that when a BMC does not respond by its IP address, the discovery job will not create a RedfishEndpoint for the BMC in Hardware State Manager (HSM). However, it does update the BMC MAC address in HSM with its component name (xname). The discovery job only creates a new RedfishEndpoints when it encounters an unknown MAC address without a component name (xname) associated with it.</description>
    </item>
    
    <item>
      <title>Known Issue Admin-client-auth Not Found</title>
      <link>/docs-csm/en-13/troubleshooting/known_issues/admin_client_auth_not_found/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:40 +0000</pubDate>
      
      <guid>/docs-csm/en-13/troubleshooting/known_issues/admin_client_auth_not_found/</guid>
      <description>Known Issue: admin-client-auth Not Found Running the Install CSM Services script, the following error may occur:
ERROR Step: Set Management NCNs to use Unbound --- Checking Precondition + Getting admin-client-auth secret Error from server (NotFound): secrets &amp;#34;admin-client-auth&amp;#34; not found + Obtaining access token Fix This can occur if the keycloak-users-localize pod has not completed, and that can be caused by an intermittent Istio issue. Remediate the issue with the following procedure:</description>
    </item>
    
    <item>
      <title>Known Issue Ceph Osd Latency</title>
      <link>/docs-csm/en-13/troubleshooting/known_issues/ceph_osd_latency/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:40 +0000</pubDate>
      
      <guid>/docs-csm/en-13/troubleshooting/known_issues/ceph_osd_latency/</guid>
      <description>Known Issue: Ceph OSD latency On some systems, Ceph can begin to exhibit latency over time, and if this occurs it can eventually cause services like slurm and services that are backed by etcd clusters to exhibit slowness and possible timeouts. In order to determine if this is occurring, run the ceph osd perf command on a master node over a period of about ten seconds, and if an OSD consistently shows latency of above 100ms (as follows), the OSDs exhibiting this latency should be restarted:</description>
    </item>
    
    <item>
      <title>SAT/HSM/CAPMC Component Power State Mismatch</title>
      <link>/docs-csm/en-13/troubleshooting/known_issues/component_power_state_mismatch/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:40 +0000</pubDate>
      
      <guid>/docs-csm/en-13/troubleshooting/known_issues/component_power_state_mismatch/</guid>
      <description>SAT/HSM/CAPMC Component Power State Mismatch Because of various hardware or communication issues, the node state reported by SAT and HSM (Hardware State Manager) may become out of sync with the actual hardware state reported by CAPMC or Redfish. In most cases this will be noticed when trying to power on or off nodes with BOS/BOA, and will present as SAT or HSM reporting nodes are On while CAPMC reports them as Off (or vice versa).</description>
    </item>
    
    <item>
      <title>SLS Not Working During Node Rebuild</title>
      <link>/docs-csm/en-13/troubleshooting/known_issues/sls_not_working_during_node_rebuild/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:40 +0000</pubDate>
      
      <guid>/docs-csm/en-13/troubleshooting/known_issues/sls_not_working_during_node_rebuild/</guid>
      <description>SLS Not Working During Node Rebuild During some node rebuilds (including those that happen during Stage 1 and Stage 2 of the CSM upgrade process), the SLS Postgres database gets into a bad state, causing SLS to become unhealthy. This page outlines how to detect if this has happened and provides a remediation procedure.
Note: If encountering this during a CSM upgrade, then at this point of the upgrade process, the system has not yet upgraded the CSM services themselves.</description>
    </item>
    
    <item>
      <title>Gigabyte BMC Missing Redfish Data</title>
      <link>/docs-csm/en-13/troubleshooting/known_issues/gigabye_missing_redfish_data/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:39 +0000</pubDate>
      
      <guid>/docs-csm/en-13/troubleshooting/known_issues/gigabye_missing_redfish_data/</guid>
      <description>Gigabyte BMC Missing Redfish Data Follow this procedure if you notice data from Gigabyte nodes is missing from Hardware State Manager (HSM) or other CSM tools.
If data from Gigabyte nodes is missing from HSM or other CSM tools, check the Redfish endpoint on the BMC to see if the data is present.
If the data is not present in the Redfish, then a cold reset of the BMC is needed to refresh the Redfish values.</description>
    </item>
    
    <item>
      <title>HSM Discovery Storefailed Error</title>
      <link>/docs-csm/en-13/troubleshooting/known_issues/hsm_discovery_storefailed_error/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:39 +0000</pubDate>
      
      <guid>/docs-csm/en-13/troubleshooting/known_issues/hsm_discovery_storefailed_error/</guid>
      <description>HSM Discovery StoreFailed Error This problem occurs when hardware exists in a system across multiple BMCs that have bogus non-empty values for serial numbers and those BMCs are being discovered at the same time. This causes the same non-unique FRUID to be generated for that hardware. Concurrent discovery of theses BMCs causes database deadlocks to occur and causes HSM discovery for most of those BMCs to fail with StoreFailed.
Known Causes One of the known types of hardware to cause this is Castle nodes with HBM.</description>
    </item>
    
    <item>
      <title>Interpreting Hms Health Check Results</title>
      <link>/docs-csm/en-13/troubleshooting/interpreting_hms_health_check_results/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:39 +0000</pubDate>
      
      <guid>/docs-csm/en-13/troubleshooting/interpreting_hms_health_check_results/</guid>
      <description>Interpreting HMS Health Check Results  Introduction Prerequisites Overview Execution Failure analysis  Smoke test failure Functional test failure   Tavern output Additional troubleshooting  run_hms_ct_tests.sh  cray-hms-smd-test-functional cray-hms-firmware-action-test-functional   hsm_discovery_status_test.sh  HTTPsGetFailed ChildVerificationFailed DiscoveryStarted     Install blocking vs. Non-blocking failures  Introduction This document describes how to interpret the results of the HMS Health Check scripts and techniques for troubleshooting when failures occur.
Prerequisites The HMS health checks will not fail if the Cray CLI is not configured.</description>
    </item>
    
    <item>
      <title>Runbook - DNS Troubleshooting</title>
      <link>/docs-csm/en-13/troubleshooting/dns_runbook/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:39 +0000</pubDate>
      
      <guid>/docs-csm/en-13/troubleshooting/dns_runbook/</guid>
      <description>Runbook - DNS Troubleshooting 1. Confirm the status of the cray-dns-unbound pods/services 1.1 Confirm cray-dns-unbound pods On any worker/manager, run:
kubectl get -n services pods | grep unbound |grep -v unbound-manager You should see the following services as output:
NAME READY STATUS RESTARTS AGE cray-dns-unbound-6d7cdf6cdb-8cwlw 2/2 Running 0 2d13h cray-dns-unbound-6d7cdf6cdb-nn96q 2/2 Running 0 2d13h cray-dns-unbound-6d7cdf6cdb-xsxrr 2/2 Running 0 2d13h  Confirm Running status for pods  1.3 Confirm cray-dnspunbound-coredns Job kubectl get job -n services -l app.</description>
    </item>
    
    <item>
      <title>Running Hms Ct Tests Manually</title>
      <link>/docs-csm/en-13/troubleshooting/hms_ct_manual_run/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:39 +0000</pubDate>
      
      <guid>/docs-csm/en-13/troubleshooting/hms_ct_manual_run/</guid>
      <description>Running HMS CT Tests Manually Use the following commands to run the HMS CT tests manually. These are the same tests that run as part of the CSM health validation procedures.
Note: The information in this documentation generally, and on this page in particular, differs based on the version of CSM installed on the system. If viewing this documentation online, be sure that the CSM version of this documentation matches the CSM version on the system.</description>
    </item>
    
    <item>
      <title>Boot Order Workaround</title>
      <link>/docs-csm/en-13/scripts/workarounds/boot-order/readme/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:38 +0000</pubDate>
      
      <guid>/docs-csm/en-13/scripts/workarounds/boot-order/readme/</guid>
      <description>Boot Order Workaround This directory includes the script and the library necessary for applying the workaround to all reachable NCNs.
Usage   Run run.sh.
ncn# /usr/share/doc/csm/scripts/workarounds/boot-order/run.sh Example output:
Failed to ping [ncn-w004]; skipping hotfix for [ncn-w004] Uploading new metal-lib.sh to ncn-m001:/srv/cray/scripts/metal/ ... Done Uploading new metal-lib.sh to ncn-m002:/srv/cray/scripts/metal/ ... Done Uploading new metal-lib.sh to ncn-m003:/srv/cray/scripts/metal/ ... Done Uploading new metal-lib.sh to ncn-s001:/srv/cray/scripts/metal/ ... Done Uploading new metal-lib.sh to ncn-s002:/srv/cray/scripts/metal/ .</description>
    </item>
    
    <item>
      <title>Manual SSH Key Setting Process</title>
      <link>/docs-csm/en-13/troubleshooting/bmc_ssh_key_manual_fixup/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:38 +0000</pubDate>
      
      <guid>/docs-csm/en-13/troubleshooting/bmc_ssh_key_manual_fixup/</guid>
      <description>Manual SSH Key Setting Process If for whatever reason this script fails, SSH keys can be set manually using the following process:
  Save the public SSH key for the root user.
export SCSD_SSH_KEY=$(cat /root/.ssh/id_rsa.pub | sed &amp;#39;s/[[:space:]]*$//&amp;#39;) If a different SSH key is to be used (for example from conman) set the SCSD_SSH_KEY environment variable to that key value.
  Generate a System Configuration Service configuration via the scsd tool.</description>
    </item>
    
    <item>
      <title>Runbook - DHCP Troubleshooting</title>
      <link>/docs-csm/en-13/troubleshooting/dhcp_runbook/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:38 +0000</pubDate>
      
      <guid>/docs-csm/en-13/troubleshooting/dhcp_runbook/</guid>
      <description>Runbook - DHCP Troubleshooting  Get an API token 1 Confirm the status of the cray-dhcp-kea services  Check cray-dchp-kea pods Check cray-dhcp-kea service endpoints Verify that cray-dhcp-kea configuration is valid Review cray-dhcp-kea running configuration Verify that cray-dhcp-kea has active DHCP leases Check dhcp-helper.py output Check cray-dhcp-kea logs   2 Troubleshooting DHCP for a specific node  Check current DHCP leases Check the Hardware State Manager  System Layout Service State Manager Daemon   Duplicate IP address Numerous DHCP decline messages during node boot   3 Network troubleshooting  Check BGP/MetalLB  Mellanox spine switches Aruba spine switches   tcpdump  Dell/Leaf CDU switches      Get an API token (ncn-mw#) Some of the commands in these procedures require an API token to be acquired and stored in the TOKEN environment variable.</description>
    </item>
    
    <item>
      <title>Troubleshoot The Cms Barebones Image Boot Test</title>
      <link>/docs-csm/en-13/troubleshooting/cms_barebones_image_boot/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:38 +0000</pubDate>
      
      <guid>/docs-csm/en-13/troubleshooting/cms_barebones_image_boot/</guid>
      <description>Troubleshoot the CMS Barebones Image Boot Test The CSM barebones image boot test verifies that the CSM services needed to boot a node are available and working properly. This test is very important to run, particularly during the CSM install prior to rebooting the PIT node, because it validates all of the services required for nodes to PXE boot from the cluster.
This page gives some information about the CSM barebones image, describes how the barebonesImageTest script works, explains how to interpret the results of the script, and provides a procedure to manually perform the test, if needed.</description>
    </item>
    
    <item>
      <title>Troubleshooting Information</title>
      <link>/docs-csm/en-13/troubleshooting/readme/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:38 +0000</pubDate>
      
      <guid>/docs-csm/en-13/troubleshooting/readme/</guid>
      <description>CSM Troubleshooting Information This document provides links to troubleshooting information for services and functionality provided by CSM.
 Helpful tips for navigating the CSM repository Known issues Booting  UAN boot issues Compute node boot issues   Compute rolling upgrades Configuration management ConMan Customer Management Network (CMN) Domain Name Service (DNS) Grafana dashboards Kubernetes MetalLB Node management Security and authentication Spire User Access Service (UAS) Utility storage  Helpful tips for navigating the CSM repository In the main repository landing page, change the branch to the CSM version being used on the system (for example, release/1.</description>
    </item>
    
    <item>
      <title>Troubleshoot A Down Osd</title>
      <link>/docs-csm/en-13/operations/utility_storage/troubleshoot_a_down_osd/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:36 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/utility_storage/troubleshoot_a_down_osd/</guid>
      <description>Troubleshoot a Down OSD Identify down OSDs and manually bring them back up.
Troubleshoot the Ceph health detail reporting down OSDs. Ensuring that OSDs are operational and data is balanced across them will help remove the likelihood of hotspots being created.
Prerequisites This procedure requires admin privileges.
Procedure   Identify the down OSDs.
ncn-m/s(001/2/3)# ceph osd tree down Example output:
ID CLASS WEIGHT TYPE NAME STATUS REWEIGHT PRI-AFF -1 62.</description>
    </item>
    
    <item>
      <title>Troubleshoot An Unresponsive Rados-gateway (radosgw) S3 Endpoint</title>
      <link>/docs-csm/en-13/operations/utility_storage/troubleshoot_an_unresponsive_s3_endpoint/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:36 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/utility_storage/troubleshoot_an_unresponsive_s3_endpoint/</guid>
      <description>Troubleshoot an Unresponsive Rados-Gateway (radosgw) S3 Endpoint The following section includes various issues causing an unresponsive radosgw S3 endpoint and how to resolve them.
Issue 1: Rados-Gateway/s3 endpoint is Not Accessible response=$(curl --write-out &amp;#39;%{http_code}&amp;#39; --silent --output /dev/null http://rgw-vip)|echo &amp;#34;Curl Response Code: $response&amp;#34; Curl Response Code: 200 Expected Responses: 2xx, 3xx
Procedure   Check the individual endpoints.
num_storage_nodes=$(craysys metadata get num_storage_nodes);for node_num in $(seq 1 &amp;#34;$num_storage_nodes&amp;#34;); do nodename=$(printf &amp;#34;ncn-s%03d&amp;#34; &amp;#34;$node_num&amp;#34;); response=$(curl --write-out &amp;#39;%{http_code}&amp;#39; --silent --output /dev/null http://$nodename:8080); echo &amp;#34;Curl Response Code for ncn-s00$endpoint: $response&amp;#34;; done Curl Response Code for ncn-s003: 200 Curl Response Code for ncn-s003: 200 Curl Response Code for ncn-s003: 200 Troubleshooting: If an error occurs with the above script, then echo $num_storage_nodes.</description>
    </item>
    
    <item>
      <title>Utility Storage</title>
      <link>/docs-csm/en-13/operations/utility_storage/utility_storage/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:36 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/utility_storage/utility_storage/</guid>
      <description>Utility Storage  Overview Storage tools and information references Storage troubleshooting references  Overview Utility storage is designed to support Kubernetes and the System Management Services (SMS) it orchestrates. Utility storage is a cost-effective solution for storing the large amounts of telemetry and log data collected.
Ceph is the utility storage platform that is used to enable pods to store persistent data. It is deployed to provide block, object, and file storage to the management services running on Kubernetes, as well as for telemetry data coming from the compute nodes.</description>
    </item>
    
    <item>
      <title>Validate Health</title>
      <link>/docs-csm/en-13/operations/validate_csm_health/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:36 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/validate_csm_health/</guid>
      <description>Validate CSM Health Anytime after the installation of the CSM services, the health of the management nodes and all CSM services can be validated.
The following are examples of when to run health checks:
 After completing the Install CSM Services Before and after NCN reboots After the system is brought back up Any time there is unexpected behavior observed In order to provide relevant information to create support tickets  The areas should be tested in the order they are listed on this page.</description>
    </item>
    
    <item>
      <title>Fixing Incorrect Number Of Pg Issues</title>
      <link>/docs-csm/en-13/operations/utility_storage/troubleshoot_pools_have_many_more_objects_per_pg_than_average/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:35 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/utility_storage/troubleshoot_pools_have_many_more_objects_per_pg_than_average/</guid>
      <description>Fixing incorrect number of PG Issues Scenario 1 Symptom: Ceph is reporting a HEALTH_WARN and the warning message is 1 pools have many more objects per pg than average.
Cause:
When an additional pool to the store the CSM artifacts was introduced, it caused the amount of PGs created per OSD to not allow for the amount of data stored in that device. This will typically be seen on three node Ceph clusters since the number of PGs is set by the number of OSDs.</description>
    </item>
    
    <item>
      <title>Troubleshoot If Rgw Health Check Fails</title>
      <link>/docs-csm/en-13/operations/utility_storage/troubleshoot_rgw_health_check_fail/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:35 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/utility_storage/troubleshoot_rgw_health_check_fail/</guid>
      <description>Troubleshoot if RGW Health Check Fails Use this procedure to determine why the RGW health check failed and what needs to be fixed.
Procedure In the goss test output, look at the value of x in Expected \&amp;lt; int \&amp;gt;: x (possible values are 1, 2, 3, 4, 5). Based on the value, navigate to the corresponding numbered item below for troubleshooting this issue.
(Optional) Manually run the rgw health check script to see descriptive output.</description>
    </item>
    
    <item>
      <title>Troubleshoot Large Object Map Objects In Ceph Health</title>
      <link>/docs-csm/en-13/operations/utility_storage/troubleshoot_large_object_map_objects_in_ceph_health/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:35 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/utility_storage/troubleshoot_large_object_map_objects_in_ceph_health/</guid>
      <description>Troubleshoot Large Object Map Objects in Ceph Health Troubleshoot an issue where Ceph reports a HEALTH_WARN of 1 large omap objects. Adjust the omap object key threshold or number of placement groups (PG) to resolve this issue.
Prerequisites Ceph health is reporting a HEALTH_WARN for large Object Map (omap) objects.
ceph -s Example output:
 cluster: id: 464f8ee0-667d-49ac-a82b-43ba8d377f81 health: HEALTH_WARN 1 large omap objects clock skew detected on mon.ncn-m002, mon.ncn-m003 services: mon: 3 daemons, quorum ncn-s001,ncn-s002,ncn-s003 (age 20h) mgr: ncn-s003(active, since 9d), standbys: ncn-s001, ncn-s002 mds: cephfs:1 {0=ncn-s002=up:active} 2 up:standby osd: 18 osds: 18 up (since 20h), 18 in (since 9d) rgw: 3 daemons active (ncn-s001.</description>
    </item>
    
    <item>
      <title>Troubleshoot Pods Failing To Restart On Other Worker Nodes</title>
      <link>/docs-csm/en-13/operations/utility_storage/troubleshoot_pods_multi-attach_error/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:35 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/utility_storage/troubleshoot_pods_multi-attach_error/</guid>
      <description>Troubleshoot Pods Failing to Restart on Other Worker Nodes Troubleshoot an issue where pods cannot restart on another worker node because of the &amp;ldquo;Volume is already exclusively attached to one node and can&amp;rsquo;t be attached to another&amp;rdquo; error. Kubernetes does not currently support &amp;ldquo;readwritemany&amp;rdquo; access mode for Rados Block Device (RBD) devices, which causes an issue where devices fail to unmap correctly.
The issue occurs when unmounting the mounts tied to the RBD devices, which causes the rbd-task (watcher) to not stop for the RBD device.</description>
    </item>
    
    <item>
      <title>Troubleshoot S3fs Mount Issues</title>
      <link>/docs-csm/en-13/operations/utility_storage/troubleshoot_s3fs_mounts/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:35 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/utility_storage/troubleshoot_s3fs_mounts/</guid>
      <description>Troubleshoot S3FS Mount Issues The following procedure includes steps to troubleshoot issues with S3FS mount points on worker and master NCNs. Beginning in the CSM 1.2 release, S3FS is deployed as tool to reduce space usage on NCNs. Below is a list of the mount points on masters and workers:
Master Node Mount Points Master nodes should host the following three mount points:
/var/opt/cray/config-data (config-data S3 bucket) /var/lib/admin-tools (admin-tools S3 bucket) /var/opt/cray/sdu/collection-mount (sds S3 bucket) Worker Node Mount Points Worker nodes should host the following mount point:</description>
    </item>
    
    <item>
      <title>Troubleshoot System Clock Skew</title>
      <link>/docs-csm/en-13/operations/utility_storage/troubleshoot_system_clock_skew/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:35 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/utility_storage/troubleshoot_system_clock_skew/</guid>
      <description>Troubleshoot System Clock Skew Resynchronize system clocks after Ceph reports a clock skew.
Systems use chronyd to synchronize their system clocks. If systems are not able to communicate, then the clocks can drift, causing clock skew. Clock skew can also be caused by an individual or an automated task manually changing the clocks. In this case, chronyd may require a series of steps (time adjustments) to resynchronize the clocks.
Major time jumps where the clock is set back in time will require a full restart of all Ceph services.</description>
    </item>
    
    <item>
      <title>Troubleshoot Ceph Mds Client Connectivity Issues</title>
      <link>/docs-csm/en-13/operations/utility_storage/troubleshoot_ceph_fs_client_connectivity_issues/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:34 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/utility_storage/troubleshoot_ceph_fs_client_connectivity_issues/</guid>
      <description>Troubleshoot Ceph MDS Client Connectivity Issues Use this procedure to diagnose and fix clients not logging into Ceph FS.
NOTE This section does not diagnose nor fix network issues. Please ensure that all networking is functional before proceeding.
IMPORTANT: The following commands can be run from ncn-m001/2/3 or ncn-s001/2/3.
Procedure   Identify if clients are not logged into Ceph FS.
ceph fs status Example output:
cephfs - 0 clients &amp;lt;---- This indicates we have no clients connected ====== RANK STATE MDS ACTIVITY DNS INOS 0 active cephfs.</description>
    </item>
    
    <item>
      <title>Troubleshoot Ceph Osds Reporting Full</title>
      <link>/docs-csm/en-13/operations/utility_storage/troubleshoot_ceph_osds_reporting_full/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:34 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/utility_storage/troubleshoot_ceph_osds_reporting_full/</guid>
      <description>Troubleshoot Ceph OSDs Reporting Full Use this procedure to examine the Ceph cluster and troubleshoot issues where Ceph runs out of space and the Kubernetes cluster cannot write data. The OSDs need to be reweighed to move data from the drive and get it back under the warning threshold.
When a single OSD for a pool fills up, the pool will go into read-only mode to protect the data. This can occur if the data distribution is unbalanced or if more storage nodes are needed.</description>
    </item>
    
    <item>
      <title>Troubleshoot Ceph Services Not Starting After A Server Crash</title>
      <link>/docs-csm/en-13/operations/utility_storage/troubleshoot_ceph_services_not_starting/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:34 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/utility_storage/troubleshoot_ceph_services_not_starting/</guid>
      <description>Troubleshoot Ceph Services Not Starting After a Server Crash Issue There is a known issue where the Ceph container images will not start after a power failure or server component failure that causes the server to crash and not boot back up
There will be a message similar to the following in the journalctl logs for the Ceph services on the machine that crashed:
ceph daemons will not start due to: Error: readlink /var/lib/containers/storage/overlay/l/CXMD7IEI4LUKBJKX5BPVGZLY3Y: no such file or directory When the issue materializes, then it is highly likely the Ceph container images have been corrupted.</description>
    </item>
    
    <item>
      <title>Troubleshoot Ceph-mon Processes Stopping And Exceeding Max Restarts</title>
      <link>/docs-csm/en-13/operations/utility_storage/troubleshoot_ceph-mon_processes_stopping_and_exceeding_max_restarts/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:34 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/utility_storage/troubleshoot_ceph-mon_processes_stopping_and_exceeding_max_restarts/</guid>
      <description>Troubleshoot Ceph-Mon Processes Stopping and Exceeding Max Restarts Troubleshoot an issue where all of the ceph-mon processes stop and exceed their maximum amount of attempts at restarting. This bug corrupts the health of the Ceph cluster.
Return the Ceph cluster to a healthy state by resolving issues with ceph-mon processes.
Prerequisites This procedure requires admin privileges.
Procedure See Collect Information about the Ceph Cluster for more information on how to interpret the output of the Ceph commands used in this procedure.</description>
    </item>
    
    <item>
      <title>Troubleshoot Failure To Get Ceph Health</title>
      <link>/docs-csm/en-13/operations/utility_storage/troubleshoot_failure_to_get_ceph_health/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:34 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/utility_storage/troubleshoot_failure_to_get_ceph_health/</guid>
      <description>Troubleshoot Failure to Get Ceph Health Inspect Ceph commands that are failing by looking into the Ceph monitor logs (ceph-mon). For example, the monitoring logs can help determine any issues causing the ceph -s command to hang.
Troubleshoot Ceph commands failing to run and determine how to make them operational again. These commands need to be operational to obtain critical information about the Ceph cluster on the system.
Prerequisites This procedure requires admin privileges.</description>
    </item>
    
    <item>
      <title>Troubleshoot Insufficient Standby Mds Daemons Available</title>
      <link>/docs-csm/en-13/operations/utility_storage/troubleshoot_insufficient_standby_mds_daemons_available/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:34 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/utility_storage/troubleshoot_insufficient_standby_mds_daemons_available/</guid>
      <description>Troubleshoot Insufficient Standby MDS Daemons Available Procedure   Log into a node running ceph-mon. Typically this will be ncn-s001/2/3.
  Check the ceph health.
ceph health detail Example Output:
HEALTH_WARN insufficient standby MDS daemons available [WRN] MDS_INSUFFICIENT_STANDBY: insufficient standby MDS daemons available have 0; want 1 more This output explicitly states that you need at least 1 more to clear the alert.
  Determine which MDS daemons are down.</description>
    </item>
    
    <item>
      <title>Troubleshooting Ceph Mds Reporting Slow Requests And Failure On Client</title>
      <link>/docs-csm/en-13/operations/utility_storage/troubleshoot_ceph_mds_reporting_slow_requests_and_failure_on_client/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:34 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/utility_storage/troubleshoot_ceph_mds_reporting_slow_requests_and_failure_on_client/</guid>
      <description>Troubleshooting Ceph MDS Reporting Slow Requests and Failure on Client Use this procedure to troubleshoot Ceph MDS reporting slow requests after following the Identify Ceph Latency Issues procedure.
 IMPORTANT: This procedure includes a mix of commands that need to be run on the host(s) running the MDS daemon(s) and other commands that can be run from any of the ceph-mon nodes. NOTICE: These steps are based off upstream Ceph documentation.</description>
    </item>
    
    <item>
      <title>Dump Ceph Crash Data</title>
      <link>/docs-csm/en-13/operations/utility_storage/dump_ceph_crash_data/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:33 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/utility_storage/dump_ceph_crash_data/</guid>
      <description>Dump Ceph Crash Data Ceph includes an option to dump crash data. Retrieve this data to get more information on a Ceph cluster that has crashed.
Prerequisites Ceph is reporting the cluster [WRN] overall HEALTH_WARN 1 daemons have recently crashed error in the output of the ceph -s or ceph health detail commands.
Procedure   Get the Ceph crash listing and the corresponding IDs.
ceph crash ls Example output:</description>
    </item>
    
    <item>
      <title>Identify Ceph Latency Issues</title>
      <link>/docs-csm/en-13/operations/utility_storage/identify_ceph_latency_issues/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:33 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/utility_storage/identify_ceph_latency_issues/</guid>
      <description>Identify Ceph Latency Issues Examine the output of the ceph -s command to get context for potential issues causing latency.
Troubleshoot the underlying causes for the ceph -s command reporting slow PGs.
Prerequisites This procedure requires admin privileges.
Procedure   View the status of Ceph.
ceph -s Example output:
cluster: id: 73084634-9534-434f-a28b-1d6f39cf1d3d health: HEALTH_WARN 1 filesystem is degraded 1 MDSs report slow metadata IOs Reduced data availability: 15 pgs inactive, 15 pgs peering 46 slow ops, oldest one blocked for 1395 sec, daemons [osd,2,osd,5,mon,ceph-1,mon,ceph-2,mon,ceph-3] have slow ops.</description>
    </item>
    
    <item>
      <title>Manage Ceph Services</title>
      <link>/docs-csm/en-13/operations/utility_storage/manage_ceph_services/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:33 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/utility_storage/manage_ceph_services/</guid>
      <description>Manage Ceph Services The following commands are required to start, stop, or restart Ceph services. Restarting Ceph services is helpful for troubleshoot issues with the utility storage platform.
List Ceph Services ncn-s00(1/2/3)# ceph orch ps Example output:
NAME HOST STATUS REFRESHED AGE VERSION IMAGE NAME IMAGE ID CONTAINER ID mds.cephfs.ncn-s001.zwptsg ncn-s001 running (3d) 7m ago 3d 15.2.8 registry.local/ceph/ceph:v15.2.8 5553b0cb212c bb08bcb2f034 mds.cephfs.ncn-s002.qyvoyv ncn-s002 running (3d) 7m ago 3d 15.2.8 registry.local/ceph/ceph:v15.2.8 5553b0cb212c 32c3ff10be42 mds.</description>
    </item>
    
    <item>
      <title>Restore Nexus Data After Data Corruption</title>
      <link>/docs-csm/en-13/operations/utility_storage/restore_corrupt_nexus/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:33 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/utility_storage/restore_corrupt_nexus/</guid>
      <description>Restore Nexus Data After Data Corruption In rare cases, if a Ceph upgrade is not completed successfully and has issues, the eventual Ceph health can end up with a damaged mds (cephfs) daemon. Ceph reports this as follows running the ceph -s command:
ceph -s Example output:
 cluster: id: 7ed70f4c-852e-494a-b9e7-5f722af6d6e7 health: HEALTH_ERR 1 filesystem is degraded 1 filesystem is offline 1 mds daemon damaged When Ceph is in this state, Nexus will likely not operate properly and can be recovered using the following procedure.</description>
    </item>
    
    <item>
      <title>Shrink Ceph Osds</title>
      <link>/docs-csm/en-13/operations/utility_storage/shrink_ceph_osds/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:33 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/utility_storage/shrink_ceph_osds/</guid>
      <description>Shrink Ceph OSDs This procedure describes how to remove an OSD(s) from a Ceph cluster. Once the OSD is removed, the cluster is also rebalanced to account for the changes. Use this procedure to reduce the size of a cluster or to replace hardware.
Prerequisites This procedure requires administrative privileges.
Procedure   Log in as root on the first master node (ncn-m001).
  Monitor the progress of the OSDs that have been added.</description>
    </item>
    
    <item>
      <title>Shrink The Ceph Cluster</title>
      <link>/docs-csm/en-13/operations/utility_storage/remove_ceph_node/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:33 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/utility_storage/remove_ceph_node/</guid>
      <description>Shrink the Ceph Cluster This procedure describes how to remove a Ceph node from the Ceph cluster. Once the node is removed, the cluster is also rebalanced to account for the changes. Use this procedure to reduce the size of a cluster.
Prerequisites  This procedure requires administrative privileges and three ssh sessions.  One to monitor the cluster. One to perform cluster wide actions from a ceph-mon node. One to perform node only actions on the node being removed.</description>
    </item>
    
    <item>
      <title>Ceph Orchestrator Usage</title>
      <link>/docs-csm/en-13/operations/utility_storage/ceph_orchestrator_usage/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:32 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/utility_storage/ceph_orchestrator_usage/</guid>
      <description>Ceph Orchestrator Usage The Ceph orchestrator provides a centralized interface for the management of the Ceph cluster. It orchestrates ceph-mgr modules that interface with external orchestration services.
Refer to the external Ceph documentation for more information.
The orchestrator manages Ceph clusters with the following capabilities:
 Single command upgrades (assuming all images are in place) Reduces the need to be on the physical server to address a large number of ceph service restarts or configuration changes Better integration with the Ceph Dashboard (coming soon) Ability to write custom orchestration modules  Troubleshoot Ceph Orchestrator Watch cephadm Log Messages Watching log messages is useful when making changes with the orchestrator, such as add/remove/scale services or upgrades.</description>
    </item>
    
    <item>
      <title>Ceph Service Check Script Usage</title>
      <link>/docs-csm/en-13/operations/utility_storage/ceph_service_check_script_usage/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:32 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/utility_storage/ceph_service_check_script_usage/</guid>
      <description>Ceph Service Check Script Usage A new Ceph service script that will check the status of Ceph and then verify that status against the individual Ceph storage nodes.
Location /opt/cray/tests/install/ncn/scripts/ceph-service-status.sh
Usage usage: ceph-service-status.sh # runs a simple Ceph health check ceph-service-status.sh -n &amp;lt;node&amp;gt; -s &amp;lt;service&amp;gt; # checks a single service on a single node ceph-service-status.sh -n &amp;lt;node&amp;gt; -a true # checks all Ceph services on a node ceph-service-status.sh -A true # checks all Ceph services on all nodes in a rolling fashion ceph-service-status.</description>
    </item>
    
    <item>
      <title>Ceph Storage Types</title>
      <link>/docs-csm/en-13/operations/utility_storage/ceph_storage_types/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:32 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/utility_storage/ceph_storage_types/</guid>
      <description>Ceph Storage Types As a reference, the following ceph and rbd commands are run from a master node or ncn-s001/2/3. Certain commands will work on different systems. For example, the rbd command can be used on the worker nodes if specifying the proper key.
Ceph Block (rbd) List block devices in a specific pool:
rbd -p POOL_NAME ls -l Example output:
NAME SIZE PARENT FMT PROT LOCK kube_vol 4 GiB 2 Create a block device:</description>
    </item>
    
    <item>
      <title>Cephadm Reference Material</title>
      <link>/docs-csm/en-13/operations/utility_storage/cephadm_reference_material/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:32 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/utility_storage/cephadm_reference_material/</guid>
      <description>Cephadm Reference Material cephadm is a new function introduced in Ceph Octopus 15. It allows for an easier method to install and manage Ceph nodes.
The following sections include common examples:
Invoke Shells to Run Traditional Ceph Commands On ncn-s001/2/3:
ncn-s00[123]# cephadm shell # creates a container with access to run ceph commands the traditional way Optionally, execute the following command:
ncn-s00[123]# cephadm shell -- ceph -s Ceph-Volume There are multiple ways to do Ceph device operations now.</description>
    </item>
    
    <item>
      <title>Collect Information About The Ceph Cluster</title>
      <link>/docs-csm/en-13/operations/utility_storage/collect_information_about_the_ceph_cluster/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:32 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/utility_storage/collect_information_about_the_ceph_cluster/</guid>
      <description>Collect Information about the Ceph Cluster These general commands for Ceph are helpful for obtaining information pertinent to troubleshooting issues.
As a reference, the Ceph commands below are run from a ceph-mon node. Certain commands will work on different systems. For example, the rbd command can be used on the worker nodes if specifying the proper key.
Ceph Log and File Locations  Ceph configurations are located under /etc/ceph/ceph.conf Ceph data structure and bootstrap is located under /var/lib/ceph// Ceph logs are now accessible by a couple of different methods  Utilizing cephadm ls to retrieve the systemd_unit on the node for the process, then utilize journalctl to dump the logs ceph log last [&amp;lt;num:int&amp;gt;] [debug|info|sec|warn|error] [*|cluster|audit|cephadm]  Note that that this will dump general cluster logs   cephadm logs [-h] [--fsid FSID] --name &amp;lt;systemd_unit&amp;gt;    Check the Status of Ceph Print the status of the Ceph cluster with the following command:</description>
    </item>
    
    <item>
      <title>Cubs Tool Usage</title>
      <link>/docs-csm/en-13/operations/utility_storage/cubs_tool_usage/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:32 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/utility_storage/cubs_tool_usage/</guid>
      <description>cubs_tool Usage Introduction cubs_tool is a python script developed as a second tier Ceph upgrade watching tool. This was to better integrate the Ceph upgrade process with the upgrade workflow tooling.
Glossary  in family - is referring to an upgrade staying within the same major version of Ceph. E.g, Any upgrade within the same CSM release will contain the same Major version of Ceph, but could have minor version bumps or patched containers.</description>
    </item>
    
    <item>
      <title>Add Ceph Osds</title>
      <link>/docs-csm/en-13/operations/utility_storage/add_ceph_osds/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:31 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/utility_storage/add_ceph_osds/</guid>
      <description>Add Ceph OSDs IMPORTANT: This document is addressing how to add an OSD when the OSD auto-discovery fails to add in new drives.
Check to ensure you have OSD auto-discovery enabled.
ncn-s00(1/2/3)# ceph orch ls osd Example output:
NAME RUNNING REFRESHED AGE PLACEMENT IMAGE NAME IMAGE ID osd.all-available-devices 9/9 4m ago 3d * registry.local/ceph/ceph:v15.2.8 5553b0cb212c  NOTE Ceph version 15.2.x and newer will utilize the ceph orchestrator to add any available drives on the storage nodes to the OSD pool.</description>
    </item>
    
    <item>
      <title>Adjust Ceph Pool Quotas</title>
      <link>/docs-csm/en-13/operations/utility_storage/adjust_ceph_pool_quotas/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:31 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/utility_storage/adjust_ceph_pool_quotas/</guid>
      <description>Adjust Ceph Pool Quotas Ceph pools are used for storing data. Use this procedure to set the Ceph pool quotas to determine the wanted number of bytes per pool. The smf Ceph pool now has replication factor of two.
Resolve Ceph health issues caused by a pool reaching its quota.
Prerequisites This procedure requires administrative privileges.
Limitations Currently, only smf includes a quota.
Procedure   Log in as root on ncn-m001.</description>
    </item>
    
    <item>
      <title>Alternate Storage Pools</title>
      <link>/docs-csm/en-13/operations/utility_storage/alternate_storage_pools/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:31 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/utility_storage/alternate_storage_pools/</guid>
      <description>Alternate Storage Pools  Description Use cases Best practices Procedures  Create a storage pool Create and map an rbd device Mount an rbd device Move an rbd device to another node Unmount, unmap, and delete an rbd device Remove a storage pool    Description Creating, maintaining, and removing Ceph storage pools.
Use cases  A landing space for the CSM tarball used for upgrades. Temporary space needed for maintenance or pre/post upgrade activities.</description>
    </item>
    
    <item>
      <title>Ceph Daemon Memory Profiling</title>
      <link>/docs-csm/en-13/operations/utility_storage/ceph_daemon_memory_profiling/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:31 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/utility_storage/ceph_daemon_memory_profiling/</guid>
      <description>Ceph Daemon Memory Profiling This procedure is meant as an instructional guide to provide information back to HPE Cray to assist in tuning and troubleshooting exercises.
Procedure  NOTE For this example, a ceph-mon process on ncn-s001 is used.
   Identify the process and location of the daemon to profile.
ncn-s00(1/2/3)# ceph orch ps --daemon_type mon Example output:
NAME HOST STATUS REFRESHED AGE VERSION IMAGE NAME IMAGE ID CONTAINER ID mon.</description>
    </item>
    
    <item>
      <title>Ceph Deep Scrubs</title>
      <link>/docs-csm/en-13/operations/utility_storage/ceph_deep_scrubs/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:31 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/utility_storage/ceph_deep_scrubs/</guid>
      <description>Ceph Deep Scrubs During normal operation, the Ceph cluster performs deep scrubs of the placement groups (PGs) during intervals of low I/O activity on the cluster. By default, these deep scrubs occur on a weekly interval. Scheduling of deep scrubs is staggered across the PGs in the Ceph cluster, so that all PGs are not deep-scrubbed at the same time.
Ceph Deep Scrub Behavior During Outages When one or more OSDs are down, the deep scrubbing of the PGs on those OSDs cannot be performed.</description>
    </item>
    
    <item>
      <title>Ceph Health States</title>
      <link>/docs-csm/en-13/operations/utility_storage/ceph_health_states/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:31 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/utility_storage/ceph_health_states/</guid>
      <description>Ceph Health States Ceph reports several different health states depending on the condition of a cluster. These health states can provide a lot of information about the current functionality of the Ceph cluster, what troubleshooting steps needs to be taken, and if a support ticket needs to be filed.
The health of a Ceph cluster can be viewed with the following command:
ceph -s Example output:
cluster: id: 5f3b4031-d6c0-4118-94c0-bffd90b534eb health: HEALTH_OK &amp;lt;&amp;lt;-- Health state [.</description>
    </item>
    
    <item>
      <title>Rbd Tool Usage</title>
      <link>/docs-csm/en-13/operations/utility_storage/csm_rbd_tool_usage/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:31 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/utility_storage/csm_rbd_tool_usage/</guid>
      <description>CSM RBD Tool Usage  Prerequisites  Preparing the Python environment Restoring the Python environment   Usage Examples  Checking device status Moving device to different node   Troubleshooting  mount system call fails when moving rbd device    Prerequisites  The tool must be run on a Kubernetes master NCN or one of the Ceph storage NCNs.  The Ceph storage NCNs are typically the first three storage NCNs: ncn-s001, ncn-s002, and ncn-s003.</description>
    </item>
    
    <item>
      <title>Adding A Ceph Node To The Ceph Cluster</title>
      <link>/docs-csm/en-13/operations/utility_storage/add_ceph_node/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:30 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/utility_storage/add_ceph_node/</guid>
      <description>Adding a Ceph Node to the Ceph Cluster NOTE This operation can be done to add more than one node at the same time.
Add Join Script   Copy join script from ncn-m001 to the storage node that was rebuilt or added.
 Run this command on the storage node that was rebuilt or added.
 mkdir -pv /usr/share/doc/csm/scripts &amp;amp;&amp;amp; scp -p ncn-m001:/usr/share/doc/csm/scripts/join_ceph_cluster.sh /usr/share/doc/csm/scripts   Start monitoring the Ceph health alongside the main procedure.</description>
    </item>
    
    <item>
      <title>Remove Kiali</title>
      <link>/docs-csm/en-13/operations/system_management_health/remove_kiali/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:30 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/system_management_health/remove_kiali/</guid>
      <description>Remove Kiali If not planning to use Kiali, then Kiali may be removed for CVE (Common Vulnerabilities and Exposures) remediation. NOTE: The removal will not persist after a CSM upgrade, so the removal procedure must be rerun after CSM upgrades.
Procedure This procedure can be performed on any master node.
  Delete kiali and kiali-operator deployments.
kubectl delete deployment kiali -n istio-system kubectl delete deployment cray-kiali-kiali-operator -n operators   Uninstall cray-kiali chart.</description>
    </item>
    
    <item>
      <title>System Management Health</title>
      <link>/docs-csm/en-13/operations/system_management_health/system_management_health/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:30 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/system_management_health/system_management_health/</guid>
      <description>System Management Health The primary goal of the System Management Health service is to enable system administrators to assess the health of their system. Operators need to quickly and efficiently troubleshoot system issues as they occur and be confident that a lack of issues indicates the system is operating normally. This service currently runs as a Helm chart on the system&amp;rsquo;s management Kubernetes cluster and monitors the health status of core system components, triggering alerts as potential issues are observed.</description>
    </item>
    
    <item>
      <title>System Management Health Checks And Alerts</title>
      <link>/docs-csm/en-13/operations/system_management_health/system_management_health_checks_and_alerts/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:30 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/system_management_health/system_management_health_checks_and_alerts/</guid>
      <description>System Management Health Checks and Alerts A health check corresponds to a Prometheus query against metrics aggregated to the Prometheus instance. Core platform components like Kubernetes and Istio collect service-related metrics by default, which enables the System Management Health service to implement generic service health checks without custom instrumentation. Health checks are intended to be coarse-grained and comprehensive, as opposed to fine-grained and exhaustive. Health checks related to infrastructure adhere to the Utilization Saturation Errors (USE) method whereas services follow the Rate Errors Duration (RED) method.</description>
    </item>
    
    <item>
      <title>Troubleshoot Grafana Dashboard</title>
      <link>/docs-csm/en-13/operations/system_management_health/troubleshoot_grafana_dashboard/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:30 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/system_management_health/troubleshoot_grafana_dashboard/</guid>
      <description>Troubleshoot Grafana Dashboard General Grafana dashboard troubleshooting topics
 Ceph - OSD Overview Dashboard Ceph - RBD Overview Dashboard Ceph - RGW Instance Detail Dashboard Ceph - RGW Overview Dashboard  Ceph - OSD Overview Dashboard: 3 panels not found This means that the cray-sysmgmt-health pie chart plugin is not installed. If the system is not airgapped, then it can be installed by commenting out or removing the plugins property in customizations.</description>
    </item>
    
    <item>
      <title>Troubleshoot Prometheus Alerts</title>
      <link>/docs-csm/en-13/operations/system_management_health/troubleshoot_prometheus_alerts/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:30 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/system_management_health/troubleshoot_prometheus_alerts/</guid>
      <description>Troubleshoot Prometheus Alerts  CephMgrIsAbsent and CephMgrIsMissingReplicas CephNetworkPacketsDropped CPUThrottlingHigh KubePodNotReady PostgresqlFollowerReplicationLagSMA PostgresqlHighRollbackRate PostgresqlInactiveReplicationSlot PostgresqlNotEnoughConnections TargetDown  CephMgrIsAbsent and CephMgrIsMissingReplicas If the CephMgrIsAbsent and/or CephMgrIsMissingReplicas alerts fire, use the following steps to ensure the prometheus module has been enabled for Ceph. The following steps should be executed on ncn-s001:
ceph mgr module ls | jq &amp;#39;.enabled_modules&amp;#39; Example output:
[ &amp;#34;cephadm&amp;#34;, &amp;#34;iostat&amp;#34;, &amp;#34;restful&amp;#34; ] If prometheus is missing from the output, enable with the following command:</description>
    </item>
    
    <item>
      <title>Access System Management Health Services</title>
      <link>/docs-csm/en-13/operations/system_management_health/access_system_management_health_services/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:29 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/system_management_health/access_system_management_health_services/</guid>
      <description>Access System Management Health Services All System Management Health services are exposed outside the cluster through the OAuth2 Proxy and Istio&amp;rsquo;s ingress gateway to enforce the authentication and authorization policies. The URLs to access these services are available on any system with CMN, BGP, MetalLB, and external DNS properly configured.
 Prerequisites System domain name System Management Health service links  Prometheus Alertmanager Grafana Kiali    Prerequisites  Access to the System Management Health web UIs is through Istio&amp;rsquo;s ingress gateway and requires clients (browsers) to set the appropriate HTTP Host header to route traffic to the desired service.</description>
    </item>
    
    <item>
      <title>Configure Prometheus Alerta Alert Notifications</title>
      <link>/docs-csm/en-13/operations/system_management_health/configure_prometheus_alerta_alert_notifications/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:29 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/system_management_health/configure_prometheus_alerta_alert_notifications/</guid>
      <description>Configure Prometheus Alerta Alert Notifications Configure an Alerta alert notification for Prometheus Alertmanager alerts.
System domain name The SYSTEM_DOMAIN_NAME value found in some of the URLs on this page is expected to be the system&amp;rsquo;s fully qualified domain name (FQDN).
(ncn-mw#) The FQDN can be found by running the following command on any Kubernetes NCN.
kubectl get secret site-init -n loftsman -o jsonpath=&amp;#39;{.data.customizations\.yaml}&amp;#39; | base64 -d | yq r - spec.</description>
    </item>
    
    <item>
      <title>Configure Prometheus Email Alert Notifications</title>
      <link>/docs-csm/en-13/operations/system_management_health/configure_prometheus_email_alert_notifications/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:29 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/system_management_health/configure_prometheus_email_alert_notifications/</guid>
      <description>Configure Prometheus Email Alert Notifications Configure an email alert notification for all Prometheus Postgres replication alerts: PostgresReplicationLagSMA, PostgresReplicationServices, PostgresqlFollowerReplicationLagSMA, and PostgresqlFollowerReplicationLagServices.
System domain name The SYSTEM_DOMAIN_NAME value found in some of the URLs on this page is expected to be the system&amp;rsquo;s fully qualified domain name (FQDN).
(ncn-mw#) The FQDN can be found by running the following command on any Kubernetes NCN.
kubectl get secret site-init -n loftsman -o jsonpath=&amp;#39;{.data.customizations\.yaml}&amp;#39; | base64 -d | yq r - spec.</description>
    </item>
    
    <item>
      <title>Grafana Dashboards By Component</title>
      <link>/docs-csm/en-13/operations/system_management_health/grafana_dashboards_by_component/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:29 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/system_management_health/grafana_dashboards_by_component/</guid>
      <description>Grafana Dashboards by Component This document show the dashboards that are available for each component in Grafana.
A Grafana dashboard is a powerful open source analytical and visualization tool that consists of multiple individual panels arranged in a grid. The panels interact with configured data sources, including the following:
 AWS CloudWatch Prometheus  NOTE: For known issues around some of the Grafana dashboards, see Troubleshoot Grafana Dashboards.
Ceph  Cluster Host  Overview Details   MDS Performance OSD  Overview Device Details   Pool  Overview Details   RBD Overview RGW  Overview Instance Details    CoreDNS  CoreDNS  ETCD  Main Clusters  Istio  Mesh Performance Pilot Service Workload  Kafka  Health Check System Throughput In/Out Replication Thread utilization Zookeeper ISR Shrinks / Expands Logs size Producer Performance Consumer Performance Fetch Follower Performance Connections Request rate Message Conversion  Kea  DHCP  Kubernetes  API server Compute Resources  Compute Resources / Cluster Compute Resources / Namespace (Pods) Compute Resources / Namespace (Workloads) Compute Resources / Node (Pods) Compute Resources / Pod Compute Resources / Workload   Controller Manager kubelet Networking  Networking / Cluster Networking / Namespace (Pods) Networking / Namespace (Workload) Networking / Pod Networking / Workload   Persistent Volumes Proxy Scheduler StatefulSets  Nodes  Exporter Full Main  Postgres  PostgreSQL Statistics  Prometheus  Prometheus  Use Method  Cluster Node  Zookeeper  Health Check System Request Latency  </description>
    </item>
    
    <item>
      <title>Grafterm</title>
      <link>/docs-csm/en-13/operations/system_management_health/grafterm/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:29 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/system_management_health/grafterm/</guid>
      <description>Grafterm Visualize metrics dashboards on the terminal, like a simplified and minimalist version of Grafana for terminal.
The utility(script) can be found in the /opt/cray/platform-utils directory in all NCNs.
Running options Exit with q or Esc.
Help ./grafterm.sh --help List available terminal dashboards ./grafterm.sh --list Default usage To view the dashboard, pass the value(dashboard json file) to the -c parameter to the script. The Grafterm will query for all data accessible in the datasource by default, and the dashboard refresh frequency is set to 10 seconds.</description>
    </item>
    
    <item>
      <title>Prometheus-kafka-adapter Errors During Installation</title>
      <link>/docs-csm/en-13/operations/system_management_health/prometheus_kafka_error/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:29 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/system_management_health/prometheus_kafka_error/</guid>
      <description>prometheus-kafka-adapter errors during installation Symptom On a fresh install of CSM, the Prometheus log has errors similar to the following:
ts=2022-12-05T13:35:53.495Z caller=dedupe.go:112 component=remote level=warn remote_name=2eb187 url=http://prometheus-kafka-adapter.sma.svc.cluster.local:80/receive msg=&amp;#34;Failed to send batch, retrying&amp;#34; err=&amp;#34;Post \&amp;#34;http://prometheus-kafka-adapter.sma.svc.cluster.local:80/receive\&amp;#34;: dial tcp: lookup prometheus-kafka-adapter.sma.svc.cluster.local on 10.16.0.10:53: no such host&amp;#34; Explanation This Kafka service does not exist, because SMA has not been installed yet. This causes the above errors for retry to be logged. Prometheus can operate without SMA Kafka and it will periodically retry the connection to Kafka.</description>
    </item>
    
    <item>
      <title>Update SLS With UAN Aliases</title>
      <link>/docs-csm/en-13/operations/system_layout_service/update_sls_with_uan_aliases/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:29 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/system_layout_service/update_sls_with_uan_aliases/</guid>
      <description>Update SLS with UAN Aliases This guide shows the process for manually adding an alias to a UAN in SLS and ensuring that the node is being monitored by conman for console logs.
Prerequisites  SLS is up and running and has been populated with data. Access to the API gateway api-gw-service (legacy: api-gw-service-nmn.local)  Procedure   Authenticate with Keycloak to obtain an API token:
export TOKEN=$(curl -k -s -S -d grant_type=client_credentials \ -d client_id=admin-client \ -d client_secret=`kubectl get secrets admin-client-auth -o jsonpath=&amp;#39;{.</description>
    </item>
    
    <item>
      <title>Create A Backup Of The SLS Postgres Database</title>
      <link>/docs-csm/en-13/operations/system_layout_service/create_a_backup_of_the_sls_postgres_database/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:28 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/system_layout_service/create_a_backup_of_the_sls_postgres_database/</guid>
      <description>Create a Backup of the SLS Postgres Database Perform a manual backup of the contents of the SLS Postgres database. This backup can be used to restore the contents of the SLS Postgres database at a later point in time using the Restoring SLS Postgres cluster from backup procedure.
Prerequisites  Healthy SLS Postgres Cluster.  Use patronictl list on the SLS Postgres cluster to determine the current state of the cluster, and a healthy cluster will look similar to the following:</description>
    </item>
    
    <item>
      <title>Dump SLS Information</title>
      <link>/docs-csm/en-13/operations/system_layout_service/dump_sls_information/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:28 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/system_layout_service/dump_sls_information/</guid>
      <description>Dump SLS Information Perform a dump of the System Layout Service (SLS) database.
This procedure will create the file sls_dump.json in the current directory.
This procedure preserves the information stored in SLS when backing up or reinstalling the system.
Prerequisites  The Cray Command Line Interface is configured. See Configure the Cray CLI. This procedure requires administrative privileges.  Procedure (ncn-mw#) Perform the SLS dump. The SLS dump will be stored in the sls_dump.</description>
    </item>
    
    <item>
      <title>Load SLS Database With Dump File</title>
      <link>/docs-csm/en-13/operations/system_layout_service/load_sls_database_with_dump_file/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:28 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/system_layout_service/load_sls_database_with_dump_file/</guid>
      <description>Load SLS Database with Dump File Load the contents of the SLS dump file to restore SLS to the state of the system at the time of the dump. This will upload and overwrite the current SLS database with the contents of the SLS dump file.
Use this procedure to restore SLS data after a system re-install.
Prerequisites  The System Layout Service (SLS) database has been dumped. See Dump SLS Information for more information.</description>
    </item>
    
    <item>
      <title>Restore SLS Postgres Database From Backup</title>
      <link>/docs-csm/en-13/operations/system_layout_service/restore_sls_postgres_database_from_backup/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:28 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/system_layout_service/restore_sls_postgres_database_from_backup/</guid>
      <description>Restore SLS Postgres Database from Backup This procedure can be used to restore the SLS Postgres database from a previously taken backup. This can be a manual backup created by the Create a Backup of the SLS Postgres Database procedure, or an automatic backup created by the cray-sls-postgresql-db-backup Kubernetes cronjob.
Prerequisites   Healthy Postgres Cluster.
 Use patronictl list on the SLS Postgres cluster to determine the current state of the cluster, and a healthy cluster will look similar to the following:</description>
    </item>
    
    <item>
      <title>Restore SLS Postgres Without An Existing Backup</title>
      <link>/docs-csm/en-13/operations/system_layout_service/restore_sls_postgres_without_an_existing_backup/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:28 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/system_layout_service/restore_sls_postgres_without_an_existing_backup/</guid>
      <description>Restore SLS Postgres without an Existing Backup This procedure is intended to repopulate SLS in the event when no Postgres backup exists.
Prerequisite   Healthy SLS Service.
Verify all 3 SLS replicas are up and running:
kubectl -n services get pods -l cluster-name=cray-sls-postgres Expected output should look similar to the following:
NAME READY STATUS RESTARTS AGE cray-sls-postgres-0 3/3 Running 0 18d cray-sls-postgres-1 3/3 Running 0 18d cray-sls-postgres-2 3/3 Running 0 18d   Procedure   Retrieve the initial sls_input_file.</description>
    </item>
    
    <item>
      <title>System Layout Service (SLS)</title>
      <link>/docs-csm/en-13/operations/system_layout_service/system_layout_service_sls/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:28 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/system_layout_service/system_layout_service_sls/</guid>
      <description>System Layout Service (SLS) The System Layout Service (SLS) holds information about the system design, such as the physical locations of network hardware, compute nodes, and cabinets. It also stores information about the network, such as which port on which switch should be connected to each compute node.
SLS stores a generalized abstraction of the system that other services can access. The Hardware State Manager (HSM) keeps track of information for hardware state or identifiers.</description>
    </item>
    
    <item>
      <title>Add An Alias To A Service</title>
      <link>/docs-csm/en-13/operations/system_layout_service/add_an_alias_to_a_service/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:27 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/system_layout_service/add_an_alias_to_a_service/</guid>
      <description>Add an alias to a service Add an alias for an existing service to the IP address reservations in the System Layout Service (SLS). Adding these IP addresses will propagate the data needed for the Domain Name Service (DNS).
Prerequisites This procedure requires administrative privileges.
Procedure This example will add an alias to the pbs_service in the Node Management Network (NMN).
  Retrieve the SLS data for the network the service resides in.</description>
    </item>
    
    <item>
      <title>Add Liquid-cooled Cabinets To SLS</title>
      <link>/docs-csm/en-13/operations/system_layout_service/add_liquid-cooled_cabinets_to_sls/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:27 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/system_layout_service/add_liquid-cooled_cabinets_to_sls/</guid>
      <description>Add Liquid-Cooled Cabinets to SLS This procedure adds one or more liquid-cooled cabinets and associated CDU management switches to SLS.
NOTES
 This procedure is intended to be used in conjunction with the top level Add additional Liquid-Cooled Cabinets to a System procedure. This procedure will only add the liquid-cooled hardware present in an EX2500 cabinet with a single liquid-cooled chassis and a single air-cooled chassis.  Prerequisites   The Cray command line interface (CLI) tool is initialized and configured on the system.</description>
    </item>
    
    <item>
      <title>Add UAN CAN Ip Addresses To SLS</title>
      <link>/docs-csm/en-13/operations/system_layout_service/add_uan_can_ip_addresses_to_sls/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:27 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/system_layout_service/add_uan_can_ip_addresses_to_sls/</guid>
      <description>Add UAN CAN IP Addresses to SLS Add the Customer Access Network (CAN) IP addresses for User Access Nodes (UANs) to the IP address reservations in the System Layout Service (SLS). Adding these IP addresses will propagate the data needed for the Domain Name Service (DNS).
For more information on CAN IP addresses, refer to the Customer Accessible Networks.
Prerequisites This procedure requires administrative privileges.
Procedure   Retrieve the SLS data for the CAN.</description>
    </item>
    
    <item>
      <title>Manage Parameters With The Scsd Service</title>
      <link>/docs-csm/en-13/operations/system_configuration_service/manage_parameters_with_the_scsd_service/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:27 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/system_configuration_service/manage_parameters_with_the_scsd_service/</guid>
      <description>Manage Parameters with the scsd Service The System Configuration Service commands below enable administrators to set various BMC and controller parameters. These parameters are controlled with the scsd command in the Cray CLI.
Retrieve Current Information from Targets Get the network protocol parameters (NTP/syslog server, SSH keys) and boot order for the targets in the payload. All fields are only applicable to Liquid Cooled controllers. Attempts to set them for Air Cooled BMCs will be ignored, and retrieving them for Air Cooled BMCs will return empty strings.</description>
    </item>
    
    <item>
      <title>Set BMC Credentials Using SAT</title>
      <link>/docs-csm/en-13/operations/system_configuration_service/set_bmc_credentials/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:27 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/system_configuration_service/set_bmc_credentials/</guid>
      <description>Set BMC Credentials Using SAT Redfish BMCs are installed on the system with default credentials. After the machine is shipped, all BMC credentials must be changed.
BMC credentials may be set with either the System Configuration Service (SCSD), or with the System Admin Toolkit&amp;rsquo;s (SAT) sat bmccreds command. Both methods enable administrators to set a unique value for each credential, or to set the same value for every credential.
This procedure describes how to set BMC credentials with sat bmccreds, which conveniently automates the steps of the SCSD procedure.</description>
    </item>
    
    <item>
      <title>System Configuration Service</title>
      <link>/docs-csm/en-13/operations/system_configuration_service/system_configuration_service/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:27 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/system_configuration_service/system_configuration_service/</guid>
      <description>System Configuration Service The System Configuration Service (SCSD) allows administrators to set various BMC and controller parameters. These parameters are typically set during discovery, but this tool enables parameters to be set before or after discovery. The operations to change these parameters are available in the Cray CLI under the scsd command.
The following are the parameters that most commonly must be set:
  SSH keys
IMPORTANT: If the scsd tool is used to update the SSHConsoleKey value outside of ConMan, it will disrupt the ConMan connection to the console and collection of console logs.</description>
    </item>
    
    <item>
      <title>Configure BMC And Controller Parameters With Scsd</title>
      <link>/docs-csm/en-13/operations/system_configuration_service/configure_bmc_and_controller_parameters_with_scsd/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:26 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/system_configuration_service/configure_bmc_and_controller_parameters_with_scsd/</guid>
      <description>Configure BMC and Controller Parameters with SCSD The System Configuration Service (SCSD) allows administrators to set various BMC and controller parameters for components in liquid-cooled cabinets. These parameters are typically set during discovery, but this tool enables parameters to be set before or after discovery. The operations to change these parameters are available in the cray CLI under the scsd command.
The parameters which can be set are:
 SSH key NTP server Syslog server BMC/Controller passwords SSH console key  IMPORTANT: If the scsd tool is used to update the SSHConsoleKey value outside of ConMan, it will disrupt the ConMan connection to the console and collection of console logs.</description>
    </item>
    
    <item>
      <title>Create A Backup Of The Spire Postgres Database</title>
      <link>/docs-csm/en-13/operations/spire/create_a_backup_of_the_spire_postgres_database/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:26 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/spire/create_a_backup_of_the_spire_postgres_database/</guid>
      <description>Create a Backup of the Spire Postgres Database Perform a manual backup of the contents of the Spire Postgres database. This backup can be used to restore the contents of the Spire Postgres database at a later point in time using the Restore Spire Postgres from Backup procedure.
Prerequisites   Healthy Spire Postgres Cluster.
Use patronictl list on the Spire Postgres cluster to determine the current state of the cluster and note which member is the Leader.</description>
    </item>
    
    <item>
      <title>Restore Missing Spire Metadata</title>
      <link>/docs-csm/en-13/operations/spire/restore_missing_spire_metadata/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:26 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/spire/restore_missing_spire_metadata/</guid>
      <description>Restore missing Spire metadata If the Boot Script Service (BSS) metadata server does contain the proper Spire metadata, then the computes will fail to boot. This is due to dracut pulling server data from the metadata during startup. To fix this issue, the spire-update-bss job needs to be rerun.
Error [ 557.513984] Apr 22 18:02:02 nid000004 dracut-initqueue[4177]: time=&amp;#34;2022-04-22T18:02:02Z&amp;#34; level=info msg=&amp;#34;SVID is not found. Starting node attestation&amp;#34; subsystem_name=attestor trust_domain_id=&amp;#34;spiffe://null&amp;#34; [ 557.514000] Apr 22 18:02:07 nid000004 dracut-initqueue[4194]: Agent is unavailable.</description>
    </item>
    
    <item>
      <title>Restore Spire Postgres Without An Existing Backup</title>
      <link>/docs-csm/en-13/operations/spire/restore_spire_postgres_without_a_backup/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:26 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/spire/restore_spire_postgres_without_a_backup/</guid>
      <description>Restore Spire Postgres without an Existing Backup Reinstall the Spire Helm chart in the event that spire-postgres databases cannot be restored from a backup.
Uninstall Spire   (ncn-mw#) Uninstall the Spire Helm chart.
helm uninstall -n spire spire   (ncn-mw#) Wait for the pods in the spire namespace to terminate. Once that is done, remove the spire-data-server PVCs.
kubectl get pvc -n spire | grep spire-data-spire-server | awk &amp;#39;{print $1}&amp;#39; | xargs kubectl delete -n spire pvc   (ncn-mw#) Disable spire-agent on all of the Kubernetes NCNs (all worker nodes and master nodes) and delete the join data.</description>
    </item>
    
    <item>
      <title>Troubleshoot Spire Failing To Start On NCNs</title>
      <link>/docs-csm/en-13/operations/spire/troubleshoot_spire_failing_to_start_on_ncns/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:26 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/spire/troubleshoot_spire_failing_to_start_on_ncns/</guid>
      <description>Troubleshoot Spire Failing to Start on NCNs The spire-agent service may fail to start on Kubernetes non-compute nodes (NCNs). A key indication of this failure is when logging errors occur with the journalctl command. The following are logging errors that will indicate if the spire-agent is failing to start:
 The join token does not exist or has already been used message is returned The last lines of the logs contain multiple lines of systemd[1]: spire-agent.</description>
    </item>
    
    <item>
      <title>Update Spire Intermediate Ca Certificate</title>
      <link>/docs-csm/en-13/operations/spire/update_spire_intermediate_ca_certificate/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:26 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/spire/update_spire_intermediate_ca_certificate/</guid>
      <description>Update Spire Intermediate CA Certificate Starting with CSM 1.2.5, there is a cronjob in the vault namespace named spire-intermediate that runs once a week. If the Spire intermediate CA certificate is set to expire in less than a month when that job runs, then it will replace the certificate with a new one. If this process fails you can take the manual steps listed below in order to update the certificate.</description>
    </item>
    
    <item>
      <title>Xname Validation</title>
      <link>/docs-csm/en-13/operations/spire/xname_validation/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:26 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/spire/xname_validation/</guid>
      <description>Xname Validation CSM 1.2.5 supports the ability to require API calls that contain xnames to be from the node with that xname. This is done by assigning unique workloads per node. This may impact performance and require the replica count of the spire-server statefulset to be increased.
Note: While spire is being reinstalled during the enable or disable process the OPA validation will fail. This will cause all API requests that go through the API Gateway to fail until the spire-jwks service is running again.</description>
    </item>
    
    <item>
      <title>Set NCN User Passwords</title>
      <link>/docs-csm/en-13/operations/security_and_authentication/update_ncn_passwords/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:25 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/security_and_authentication/update_ncn_passwords/</guid>
      <description>Set NCN User Passwords The management node images do not contain a default root password or default SSH keys.
Use one of these methods to change or set the root password in the image.
  If the PIT node is booted, see Change NCN Image Root Password and SSH Keys on PIT Node for more information.
  If the PIT node is not booted, see Change NCN Image Root Password and SSH Keys for more information.</description>
    </item>
    
    <item>
      <title>Troubleshoot Common Vault Cluster Issues</title>
      <link>/docs-csm/en-13/operations/security_and_authentication/troubleshoot_common_vault_cluster_issues/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:25 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/security_and_authentication/troubleshoot_common_vault_cluster_issues/</guid>
      <description>Troubleshoot Common Vault Cluster Issues Search for underlying issues causing unhealthy Vault clusters. Check the Vault statefulset and various pod logs to determine what is impacting the health of the Vault.
Procedure   (ncn-mw#) View the Vault statefulset.
kubectl -n vault get statefulset --show-labels Example output:
NAME READY AGE LABELS cray-vault 3/3 8d app.kubernetes.io/name=vault,vault_cr=cray-vault   (ncn-mw#) Check the pod logs for the bank-vaults container for Vault statefulset pods.</description>
    </item>
    
    <item>
      <title>Troubleshoot Kyverno Configuration Manually</title>
      <link>/docs-csm/en-13/operations/security_and_authentication/troubleshoot_kyverno_configuration_manually/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:25 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/security_and_authentication/troubleshoot_kyverno_configuration_manually/</guid>
      <description>Troubleshoot Kyverno configuration manually Check Kyverno pods (ncn-mw#) Run the following script to verify that the expected Kyverno pods are running:
/opt/cray/tests/install/livecd/scripts/k8s_kyverno_pods_running.sh -p Check Kyverno policy report (ncn-mw#) Run the following script in order to check the Kyverno policy report for any failures, warnings, errors, and skipped policies:
/opt/cray/tests/install/livecd/scripts/k8s_kyverno_pods_running.sh -p More information See Kyverno.</description>
    </item>
    
    <item>
      <title>Update Default Air-cooled BMC And Leaf-bmc Switch Snmp Credentials</title>
      <link>/docs-csm/en-13/operations/security_and_authentication/update_default_air-cooled_bmc_and_leaf_bmc_switch_snmp_credentials/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:25 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/security_and_authentication/update_default_air-cooled_bmc_and_leaf_bmc_switch_snmp_credentials/</guid>
      <description>Update Default Air-Cooled BMC and Leaf-BMC Switch SNMP Credentials This procedure updates the default credentials used when new air-cooled hardware is discovered for the first time. This includes the default Redfish credentials used for new air-cooled NodeBMCs and Slingshot switch BMCs (RouterBMCs), and SNMP credentials for new management leaf-BMC switches.
IMPORTANT After this procedure is completed, all future air-cooled hardware added to the system will be assumed to be configured with the new global default credential.</description>
    </item>
    
    <item>
      <title>Update Default Servertech Pdu Credentials Used By The Redfish Translation Service (rts)</title>
      <link>/docs-csm/en-13/operations/security_and_authentication/update_default_servertech_pdu_credentials_used_by_the_redfish_translation_service/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:25 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/security_and_authentication/update_default_servertech_pdu_credentials_used_by_the_redfish_translation_service/</guid>
      <description>Update Default ServerTech PDU Credentials used by the Redfish Translation Service (RTS) This procedure updates the default credentials used by the Redfish Translation Service (RTS) for when new ServerTech PDUs are discovered in a system.
The Redfish Translation Service provides a Redfish interface that the Hardware State Manager (HSM) and Cray Advanced Platform Monitoring and Control (CAPMC) services can use interact with ServerTech PDUs which do not natively support Redfish.</description>
    </item>
    
    <item>
      <title>Updating The Liquid-cooled Ex Cabinet Cec With Default Credentials After A Cec Password Change</title>
      <link>/docs-csm/en-13/operations/security_and_authentication/updating_the_liquid-cooled_ex_cabinet_default_credentials_after_a_cec_password_change/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:25 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/security_and_authentication/updating_the_liquid-cooled_ex_cabinet_default_credentials_after_a_cec_password_change/</guid>
      <description>Updating the Liquid-Cooled EX Cabinet CEC with Default Credentials after a CEC Password Change This procedure changes the credential for liquid-cooled EX cabinet chassis controllers and node controller (BMCs) used by CSM services after the CECs have been set to a new global default credential.
NOTE This procedure does not provision Slingshot switch BMCs (RouterBMCs). Slingshot switch BMC default credentials must be changed using the procedures in the Slingshot product documentation.</description>
    </item>
    
    <item>
      <title>Re-sync Keycloak Users To Compute Nodes</title>
      <link>/docs-csm/en-13/operations/security_and_authentication/resync_keycloak_users_to_compute_nodes/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:24 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/security_and_authentication/resync_keycloak_users_to_compute_nodes/</guid>
      <description>Re-Sync Keycloak Users to Compute Nodes Resubmit the keycloak-users-localize job and run keycloak-group-sync.sh and keycloak-passwd-sync.sh to synchronize the users and groups from Keycloak to the compute nodes. This procedure alters the /etc/passwd and /etc/group files used on compute nodes.
Use this procedure to quickly synchronize changes made in Keycloak to the compute nodes.
Prerequisites The Slurm or PBS product must be installed.
Procedure   (ncn-mw#) Resubmit the keycloak-users-localize job.</description>
    </item>
    
    <item>
      <title>Retrieve An Authentication Token</title>
      <link>/docs-csm/en-13/operations/security_and_authentication/retrieve_an_authentication_token/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:24 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/security_and_authentication/retrieve_an_authentication_token/</guid>
      <description>Retrieve an Authentication Token Retrieve a token for authenticating to one of the API gateways, for the shasta realm.
The following are important properties of authentication tokens:
 Keycloak access tokens remain valid for 365 days. Secrets do not expire; they are persistent in Keycloak. Tokens and secrets can be revoked at any time by an administrator.  The API gateways use OAuth2 for authentication. A token is required to authenticate with one of the gateways.</description>
    </item>
    
    <item>
      <title>Retrieve The Client Secret For Service Accounts</title>
      <link>/docs-csm/en-13/operations/security_and_authentication/retrieve_the_client_secret_for_service_accounts/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:24 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/security_and_authentication/retrieve_the_client_secret_for_service_accounts/</guid>
      <description>Retrieve the Client Secret for Service Accounts Get the client secret that is generated by Keycloak when the client or service account was created. The secret can be regenerated any time with an administrative action.
A client secret is needed to make requests using a new client or service account.
 Prerequisites Procedure  Use the Keycloak administration console UI Use the Keycloak REST API    Prerequisites A client or service account has been created.</description>
    </item>
    
    <item>
      <title>System Security And Authentication</title>
      <link>/docs-csm/en-13/operations/security_and_authentication/system_security_and_authentication/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:24 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/security_and_authentication/system_security_and_authentication/</guid>
      <description>System Security and Authentication The system uses a number of mechanisms to ensure the security and authentication of internal and external requests.
  API Gateway service - The Cray API Gateway service provides a common access gateway for all of the systems management REST APIs. Authentication is provided by an Identity and Access Management (IAM) service that integrates with Istio.
  Keycloak - Keycloak is an open source Identity and Access Management (IAM) solution.</description>
    </item>
    
    <item>
      <title>Transport Layer Security (tls) For Ingress Services</title>
      <link>/docs-csm/en-13/operations/security_and_authentication/transport_layer_security_for_ingress_services/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:24 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/security_and_authentication/transport_layer_security_for_ingress_services/</guid>
      <description>Transport Layer Security (TLS) for Ingress Services The Istio Secure Gateway and Keycloak Gatekeeper services utilize Cert-manager for their Transport Layer Security (TLS) certificate and private key. Certificate custom resource definitions are deployed as part of Helm Charts for these services.
To view properties of the Istio Secure Gateway certificate:
kubectl describe certificate -n istio-system ingress-gateway-cert To view the properties of the Keycloak Gatekeeper certificate:
kubectl describe certificate -n services keycloak-gatekeeper An outstanding bug in the Keycloak Gatekeeper service prevents it from updating its TLS certificate and key material upon Cert-manager renewal.</description>
    </item>
    
    <item>
      <title>Update NCN User SSH Keys</title>
      <link>/docs-csm/en-13/operations/security_and_authentication/ssh_keys/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:24 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/security_and_authentication/ssh_keys/</guid>
      <description>Update NCN User SSH Keys Change the SSH keys for users on non-compute nodes (NCNs) on the system using the rotate-ssh-keys-mgmt-nodes.yml Ansible playbook provided by CSM or through NCN node personalization (site.yml).
The NCNs deploy with SSH keys for the root user that are changed during the system install. See Change NCN Image Root Password and SSH Keys for more information on changing the default keys during install. It is a recommended best practice for system security to change the SSH keys after the install is complete on a schedule.</description>
    </item>
    
    <item>
      <title>Provisioning A Liquid-cooled Ex Cabinet Cec With Default Credentials</title>
      <link>/docs-csm/en-13/operations/security_and_authentication/provisioning_a_liquid-cooled_ex_cabinet_cec_with_default_credentials/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:23 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/security_and_authentication/provisioning_a_liquid-cooled_ex_cabinet_cec_with_default_credentials/</guid>
      <description>Provisioning a Liquid-Cooled EX Cabinet CEC with Default Credentials This procedure provisions a Glibc compatible SHA-512 administrative password hash to a cabinet environmental controller (CEC). This password becomes the Redfish default global credential to access the CMM controllers and node controllers (BMCs).
This procedure does not provision Slingshot switch BMCs. Slingshot switch BMC default credentials must be changed using the procedures in the Slingshot product documentation. Refer to &amp;ldquo;Change Rosetta Login and Redfish API Credentials&amp;rdquo; in the Slingshot Operations Guide (&amp;gt; 1.</description>
    </item>
    
    <item>
      <title>Public Key Infrastructure (PKI)</title>
      <link>/docs-csm/en-13/operations/security_and_authentication/public_key_infrastructure_pki/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:23 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/security_and_authentication/public_key_infrastructure_pki/</guid>
      <description>Public Key Infrastructure (PKI) Public Key Infrastructure (PKI) represents the algorithms, infrastructure, policies, and processes required to leverage applied public key cryptography methods for operational security use cases. The Rivest-Shamir-Adleman (RSA) and Elliptic-curve (ECC) are some example algorithm systems.
The use of PKI for the system is in the Transport Layer Security (TLS) protocol, which is the successor of the now deprecated Secure Sockets Layer (SSL). This is where trusted chains of Certificate Authorities (CAs) are used to authenticate the identity of servers, and sometimes clients (for example, mutual TLS) for relying parties.</description>
    </item>
    
    <item>
      <title>Recovering From Mismatched BMC Credentials</title>
      <link>/docs-csm/en-13/operations/security_and_authentication/recovering_from_mismatched_bmc_credentials/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:23 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/security_and_authentication/recovering_from_mismatched_bmc_credentials/</guid>
      <description>Recovering from Mismatched BMC Credentials Use this procedure to recover from the situation when new or replacement hardware has root credentials that do not match the system&amp;rsquo;s current default root user credentials.
This type of problem can occur in the following scenarios:
 The site has customized the default root credentials using either the Updating the Liquid-Cooled EX Cabinet CEC with Default Credentials after a CEC Password Change or Update Default Air-Cooled BMC and Leaf-BMC Switch SNMP Credentials procedures.</description>
    </item>
    
    <item>
      <title>Remove Internal Groups From The Keycloak Shasta Realm</title>
      <link>/docs-csm/en-13/operations/security_and_authentication/remove_internal_groups_from_the_keycloak_shasta_realm/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:23 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/security_and_authentication/remove_internal_groups_from_the_keycloak_shasta_realm/</guid>
      <description>Remove Internal Groups from the Keycloak Shasta Realm Remove a group in the Keycloak Shasta realm. Unused Keycloak groups can be removed.
Prerequisites This procedure assumes that the password for the Keycloak admin account is known. The Keycloak password is set during the software installation process.
(ncn-mw#) The password can be obtained using the following command:
kubectl get secret -n services keycloak-master-admin-auth --template={{.data.password}} | base64 --decode Procedure   Open the Keycloak user management interface.</description>
    </item>
    
    <item>
      <title>Remove The Email Mapper From The LDAP User Federation</title>
      <link>/docs-csm/en-13/operations/security_and_authentication/remove_the_email_mapper_from_the_ldap_user_federation/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:23 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/security_and_authentication/remove_the_email_mapper_from_the_ldap_user_federation/</guid>
      <description>Remove the Email Mapper from the LDAP User Federation The email mapper is automatically added to the LDAP user federation in Keycloak, but it can be removed. The system does not use the user&amp;rsquo;s email for anything, so this function can be removed.
If there are duplicate email addresses for LDAP users, it can cause Keycloak to have issues syncing with LDAP. Removing the email mapper will fix this problem.</description>
    </item>
    
    <item>
      <title>Remove The LDAP User Federation From Keycloak</title>
      <link>/docs-csm/en-13/operations/security_and_authentication/remove_the_ldap_user_federation_from_keycloak/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:23 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/security_and_authentication/remove_the_ldap_user_federation_from_keycloak/</guid>
      <description>Remove the LDAP User Federation from Keycloak Use the Keycloak UI or Keycloak REST API to remove the LDAP user federation from Keycloak.
Removing user federation is useful if the LDAP server was decommissioned or if the administrator would like to make changes to the Keycloak configuration using the Keycloak user localization tool.
Prerequisites LDAP user federation is currently configured in Keycloak.
Procedure Follow the steps in only one of the sections below:</description>
    </item>
    
    <item>
      <title>Keycloak User Management With Kcadm.sh</title>
      <link>/docs-csm/en-13/operations/security_and_authentication/keycloak_user_management_with_kcadm/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:22 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/security_and_authentication/keycloak_user_management_with_kcadm/</guid>
      <description>Keycloak User Management with kcadm.sh Overview The Cray CLI requires a valid Keycloak JWT token. If the token is not present it must be obtained by supplying valid user credentials at the time of CLI initialization. During the installation or upgrade of a Shasta system, the Keycloak user LDAP federation state or credentials may not be known, or the external LDAP system for which the user may have been federated may not be available.</description>
    </item>
    
    <item>
      <title>Make HTTPS Requests From Sources Outside The Management Kubernetes Cluster</title>
      <link>/docs-csm/en-13/operations/security_and_authentication/make_https_requests_from_sources_outside_the_management_kubernetes_cluster/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:22 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/security_and_authentication/make_https_requests_from_sources_outside_the_management_kubernetes_cluster/</guid>
      <description>Make HTTPS Requests from Sources Outside the Management Kubernetes Cluster Clients lying outside the system&amp;rsquo;s management cluster need to trust the Certificate Authority (CA) certificate or host certificate in order to make requests to a non-compute node (NCN). Getting the client system to trust the CA certificate depends on the operating system.
This procedure shows an example of how to have a client trust the system&amp;rsquo;s CA certificate on a Mac OS X system.</description>
    </item>
    
    <item>
      <title>Manage Sealed Secrets</title>
      <link>/docs-csm/en-13/operations/security_and_authentication/manage_sealed_secrets/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:22 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/security_and_authentication/manage_sealed_secrets/</guid>
      <description>Manage Sealed Secrets Sealed secrets are essential for managing sensitive information on the system. The following procedures for managing sealed secrets are included in this section:
 Generate Sealed Secrets Post-Install Prevent Regeneration of Tracked Sealed Secrets View Tracked Sealed Secrets Decrypt Sealed Secrets for Review Fix an Incorrect Value in a Sealed Secret  In the following sections, the term &amp;ldquo;tracked sealed secrets&amp;rdquo; is used to describe any existing secrets stored in spec.</description>
    </item>
    
    <item>
      <title>Manage System Passwords</title>
      <link>/docs-csm/en-13/operations/security_and_authentication/manage_system_passwords/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:22 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/security_and_authentication/manage_system_passwords/</guid>
      <description>Manage System Passwords Many system services require login credentials to gain access to them. The information below is a comprehensive list of system passwords and how to change them.
Contact HPE Cray service in order to obtain the default usernames and passwords for any of these components or services.
Keycloak Default Keycloak admin user login credentials:
  Username: admin
  The password can be obtained with the following command:</description>
    </item>
    
    <item>
      <title>PKI Certificate Authority (ca)</title>
      <link>/docs-csm/en-13/operations/security_and_authentication/pki_certificate_authority_ca/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:22 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/security_and_authentication/pki_certificate_authority_ca/</guid>
      <description>PKI Certificate Authority (CA) An instance of HashiCorp Vault, deployed via the Bitnami Bank-vaults operator, stores private and public Certificate Authority (CA) material, and serves APIs through a Public Key Infrastructure (PKI) engine instance.
CA material is injected as a start-up secret into Vault through a SealedSecret that translates into a Kubernetes Secret.
CA Certificate Distribution Trusted CA certificates are distributed via two channels:
 Cloud-init metadata Kubernetes ConfigMaps  Kubernetes-native workloads generally leverage ConfigMap-based distribution.</description>
    </item>
    
    <item>
      <title>PKI Services</title>
      <link>/docs-csm/en-13/operations/security_and_authentication/pki_services/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:22 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/security_and_authentication/pki_services/</guid>
      <description>PKI Services The services in this section are integral parts of the Public Key Infrastructure (PKI) implementation.
HashiCorp Vault A deployment of HashiCorp Vault, managed via the Bitnami Bank-vaults operator, stores private and public Certificate Authority (CA) material, and serves APIs through a PKI engine instance. This instance also serves as a general secrets engine for the system.
Kubernetes service account authorization is utilized to authenticate access to Vault. The configuration of Vault, as deployed on the system, can be viewed with the following command:</description>
    </item>
    
    <item>
      <title>Preserve Username Capitalization For Users Exported From Keycloak</title>
      <link>/docs-csm/en-13/operations/security_and_authentication/preserve_username_capitalization_for_users_exported_from_keycloak/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:22 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/security_and_authentication/preserve_username_capitalization_for_users_exported_from_keycloak/</guid>
      <description>Preserve Username Capitalization for Users Exported from Keycloak Keycloak converts all characters in a username to lowercase when users are exported. Use this procedure to update the keycloak-users-localize tool with a configuration option that enables administrators to preserve the username letter case when users are exported from Keycloak.
The LDAP server that provides password resolution and user account federation supports mixed case usernames. If the usernames are changed to lowercase when exported from Keycloak, it can cause issues.</description>
    </item>
    
    <item>
      <title>Default Keycloak Realms, Accounts, And Clients</title>
      <link>/docs-csm/en-13/operations/security_and_authentication/default_keycloak_realms_accounts_and_clients/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:21 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/security_and_authentication/default_keycloak_realms_accounts_and_clients/</guid>
      <description>Default Keycloak Realms, Accounts, and Clients This page details the default Keycloak realms, accounts, and clients that are created when the system software is installed.
 Default realms Default accounts Default clients  Private clients Public clients    Default realms  Master Shasta  Default accounts Username: admin
(ncn-mw#) The password can be obtained with the following command:
kubectl get secret -n services keycloak-master-admin-auth --template={{.data.password}} | base64 --decode The password for the admin account can be changed.</description>
    </item>
    
    <item>
      <title>Delete Internal User Accounts In The Keycloak Shasta Realm</title>
      <link>/docs-csm/en-13/operations/security_and_authentication/delete_internal_user_accounts_from_the_keycloak_shasta_realm/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:21 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/security_and_authentication/delete_internal_user_accounts_from_the_keycloak_shasta_realm/</guid>
      <description>Delete Internal User Accounts in the Keycloak Shasta Realm Manually delete a user account in the Keycloak Shasta realm. User accounts are maintained via the Keycloak user management UI.
Removing an account from Keycloak is a good way to revoke admin or user privileges.
Prerequisites This procedure assumes that the password for the Keycloak admin account is known. The Keycloak password is set during the software installation process.
(ncn-mw#) The password can be obtained with the following command:</description>
    </item>
    
    <item>
      <title>Get A Long-lived Token For A Service Account</title>
      <link>/docs-csm/en-13/operations/security_and_authentication/get_a_long-lived_token_for_a_service_account/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:21 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/security_and_authentication/get_a_long-lived_token_for_a_service_account/</guid>
      <description>Get a Long-Lived Token for a Service Account Set up a long-lived offline token for a service account using the Keycloak REST API. Keycloak implements the OpenID Connect protocol, so this is a standard procedure for any OpenID Connect server.
Refer to Offline Access in the official Keycloak documentation for more information.
Prerequisites  A client or service account has been created.  See Create a Service Account in Keycloak.   The CLIENT_SECRET variable has been set up.</description>
    </item>
    
    <item>
      <title>Hashicorp Vault</title>
      <link>/docs-csm/en-13/operations/security_and_authentication/hashicorp_vault/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:21 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/security_and_authentication/hashicorp_vault/</guid>
      <description>HashiCorp Vault A deployment of HashiCorp Vault, managed via the Bitnami Bank-vaults operator, stores private and public Certificate Authority (CA) material, and serves APIs through a PKI engine instance. This instance also serves as a general secrets engine for the system.
Kubernetes service account authorization is utilized to authenticate access to Vault. The configuration of Vault, as deployed on the system, can be viewed with the following command:
kubectl get vault -n vault cray-vault -o yaml A Kubernetes operator manages the deployment of Vault, based on this definition.</description>
    </item>
    
    <item>
      <title>Keycloak Operations</title>
      <link>/docs-csm/en-13/operations/security_and_authentication/keycloak_operations/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:21 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/security_and_authentication/keycloak_operations/</guid>
      <description>Keycloak Operations A service may need to access Keycloak to perform various tasks. These typical uses for a service to access Keycloak include creating a new service account, creating a new user, etc. These operations require Keycloak administrative access. As part of the System Management Services (SMS) installation process, Keycloak is initialized with a Master realm. An administrative client and user are created within this realm. The system installation process adds the information needed for the Keycloak administrator&amp;rsquo;s authentication into a Kubernetes secret that can be accessed by any pod.</description>
    </item>
    
    <item>
      <title>Keycloak User Localization</title>
      <link>/docs-csm/en-13/operations/security_and_authentication/keycloak_user_localization/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:21 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/security_and_authentication/keycloak_user_localization/</guid>
      <description>Keycloak User Localization Verification procedure Verify that the Keycloak users localize job has completed as expected.
 This section can be skipped if user localization is not required.
 After an upgrade, it is possible that all expected Keycloak users were not localized. The procedure below helps determine whether or not this has happened, and provides remediation steps if they are needed.
  Check to see if the Keycloak users localize job has completed.</description>
    </item>
    
    <item>
      <title>Configure Keycloak For LDAP/ad Authentication</title>
      <link>/docs-csm/en-13/operations/security_and_authentication/configure_keycloak_for_ldapad_authentication/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:20 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/security_and_authentication/configure_keycloak_for_ldapad_authentication/</guid>
      <description>Configure Keycloak for LDAP/AD authentication Keycloak enables users to be in an LDAP or Active Directory (AD) server. This allows users to get their tokens using their regular username and password, and use those tokens to perform operations on the system&amp;rsquo;s REST API.
Configuring Keycloak can be done using the admin GUI or through Keycloak&amp;rsquo;s web API.
For more information on setting up LDAP federation, see the Keycloak administrative documentation in a section titled https://www.</description>
    </item>
    
    <item>
      <title>Configure Root User On HPE Ilo BMCs</title>
      <link>/docs-csm/en-13/operations/security_and_authentication/configure_root_user_on_hpe_ilo_bmcs/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:20 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/security_and_authentication/configure_root_user_on_hpe_ilo_bmcs/</guid>
      <description>Configure root user on HPE iLO BMCs By default, HPE BMC controllers have the Administrator user account. In order to discover this type of hardware, the root service account needs to be configured.
This procedure is applicable in the following situations:
 The root password is known and does not match the destination system default air-cooled BMC credentials.  For example, a node has been moved between systems and each system has different default global credentials.</description>
    </item>
    
    <item>
      <title>Configure The Rsa Plugin In Keycloak</title>
      <link>/docs-csm/en-13/operations/security_and_authentication/configure_the_rsa_plugin_in_keycloak/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:20 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/security_and_authentication/configure_the_rsa_plugin_in_keycloak/</guid>
      <description>Configure the RSA Plugin in Keycloak Use Keycloak to configure a plugin that enables RSA token authentication.
 Prerequisites Procedure Verification  Prerequisites Access to the Keycloak UI is needed.
Procedure   Verify the Shasta domain is being used.
This is indicated in the dropdown in the upper left of the UI.
  Click on Authentication under the Configure header of the navigation area on the left side of the page.</description>
    </item>
    
    <item>
      <title>Create A Backup Of The Keycloak Postgres Database</title>
      <link>/docs-csm/en-13/operations/security_and_authentication/create_a_backup_of_the_keycloak_postgres_database/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:20 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/security_and_authentication/create_a_backup_of_the_keycloak_postgres_database/</guid>
      <description>Create a Backup of the Keycloak Postgres Database Perform a manual backup of the contents of the Keycloak Postgres database. This backup can be used to restore the contents of the Keycloak Postgres database at a later point in time using the Restore Keycloak Postgres from Backup procedure.
Prerequisites   Healthy Keycloak Postgres Cluster.
Use patronictl list on the Keycloak Postgres cluster to determine the current state of the cluster and note which member is the Leader.</description>
    </item>
    
    <item>
      <title>Create A Service Account In Keycloak</title>
      <link>/docs-csm/en-13/operations/security_and_authentication/create_a_service_account_in_keycloak/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:20 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/security_and_authentication/create_a_service_account_in_keycloak/</guid>
      <description>Create a Service Account in Keycloak Set up a Keycloak service account using the Keycloak administration console or the Keycloak REST API. A service account can be used to get a long-lived token that is used by automation tools.
In Keycloak, service accounts are associated with a client. See Service Accounts for more information from the Keycloak documentation.
Procedure Follow the steps in only one of the following sections, depending on if it is preferred to use the Keycloak REST API or the Keycloak administration console UI.</description>
    </item>
    
    <item>
      <title>Create Internal Groups In The Keycloak Shasta Realm</title>
      <link>/docs-csm/en-13/operations/security_and_authentication/create_internal_groups_in_the_keycloak_shasta_realm/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:20 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/security_and_authentication/create_internal_groups_in_the_keycloak_shasta_realm/</guid>
      <description>Create Internal Groups in the Keycloak Shasta Realm Manually create a group in the Keycloak Shasta realm. New groups can be created with the Keycloak UI. In CSM, Keycloak groups must have the cn and gidNumber attributes, otherwise the keycloak-users-localize tool will fail to export the groups.
New Keycloak groups can be used to group users for authentication.
Prerequisites This procedure assumes that the password for the Keycloak admin account is known.</description>
    </item>
    
    <item>
      <title>Create Internal User Accounts In The Keycloak Shasta Realm</title>
      <link>/docs-csm/en-13/operations/security_and_authentication/create_internal_user_accounts_in_the_keycloak_shasta_realm/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:20 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/security_and_authentication/create_internal_user_accounts_in_the_keycloak_shasta_realm/</guid>
      <description>Create Internal User Accounts in the Keycloak Shasta Realm The following manual procedure can be used to create a user in the Keycloak Shasta realm. New accounts can be created with the Keycloak UI.
New administrator and user accounts are authenticated with Keycloak. Authenticated accounts are needed to use the Cray CLI.
Prerequisites This procedure assumes that the password for the Keycloak admin account is known. The Keycloak password is set during the software installation process.</description>
    </item>
    
    <item>
      <title>Change Root Passwords For Compute Nodes</title>
      <link>/docs-csm/en-13/operations/security_and_authentication/change_root_passwords_for_compute_nodes/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:19 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/security_and_authentication/change_root_passwords_for_compute_nodes/</guid>
      <description>Change Root Passwords for Compute Nodes Update the root password on the system for compute nodes.
Changing the root password at least once is a recommended best practice for system security.
Prerequisites The initial root password for compute nodes is not set. Use this procedure to initially set or later change the password.
Procedure   (ncn-mw#) Get an encrypted value for the new password.
Use the passwd command to update the password and get the encrypted hash of the new password.</description>
    </item>
    
    <item>
      <title>Change Snmp Credentials On Leaf-BMC Switches</title>
      <link>/docs-csm/en-13/operations/security_and_authentication/change_snmp_credentials_on_leaf_bmc_switches/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:19 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/security_and_authentication/change_snmp_credentials_on_leaf_bmc_switches/</guid>
      <description>Change SNMP Credentials on Leaf-BMC Switches This procedure changes the SNMP credentials on management leaf-BMC switches in the system. All SNMP credentials need to be the same as those found in the customizations.yaml sealed secret cray_reds_credentials.
NOTE This procedure will not update the default SNMP credentials used when new leaf BMC switches are added to the system. To update the default SNMP credentials for new hardware, follow the Update Default Air-Cooled BMC and Leaf-BMC Switch SNMP Credentials procedure.</description>
    </item>
    
    <item>
      <title>Change The Keycloak Admin Password</title>
      <link>/docs-csm/en-13/operations/security_and_authentication/change_the_keycloak_admin_password/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:19 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/security_and_authentication/change_the_keycloak_admin_password/</guid>
      <description>Change the Keycloak Admin Password Update the default password for the admin Keycloak account using the Keycloak user interface (UI). After updating the password in Keycloak, encrypt it on the system and verify that the change was made successfully.
 System domain name Procedure  System domain name The SYSTEM_DOMAIN_NAME value found in some of the URLs on this page is expected to be the system&amp;rsquo;s fully qualified domain name (FQDN).</description>
    </item>
    
    <item>
      <title>Change The LDAP Server Ip Address For Existing Ldap Server Content</title>
      <link>/docs-csm/en-13/operations/security_and_authentication/change_the_ldap_server_ip_address_for_existing_ldap_server_content/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:19 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/security_and_authentication/change_the_ldap_server_ip_address_for_existing_ldap_server_content/</guid>
      <description>Change the LDAP Server IP Address for Existing LDAP Server Content The IP address that Keycloak is using for the LDAP server can be changed. In the case where the new LDAP server has the same contents as the previous LDAP server, edit the LDAP user federation to switch Keycloak to use the new LDAP server.
Refer to Change the LDAP Server IP Address for New LDAP Server Content if the LDAP server is being replaced by a different LDAP server that has different content.</description>
    </item>
    
    <item>
      <title>Change The LDAP Server Ip Address For New Ldap Server Content</title>
      <link>/docs-csm/en-13/operations/security_and_authentication/change_the_ldap_server_ip_address_for_new_ldap_server_content/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:19 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/security_and_authentication/change_the_ldap_server_ip_address_for_new_ldap_server_content/</guid>
      <description>Change the LDAP Server IP Address for New LDAP Server Content Delete the old LDAP user federation and create a new one. This procedure should only be done if the LDAP server is being replaced by a different LDAP server that has different contents.
Refer to Change the LDAP Server IP Address for Existing LDAP Server Content if the new LDAP server content matches the previous LDAP server content.
Prerequisites The LDAP server is being replaced by a different LDAP server that has different contents.</description>
    </item>
    
    <item>
      <title>Set NCN Image Root Password, SSH Keys, And Timezone</title>
      <link>/docs-csm/en-13/operations/security_and_authentication/change_ncn_image_root_password_and_ssh_keys/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:19 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/security_and_authentication/change_ncn_image_root_password_and_ssh_keys/</guid>
      <description>Set NCN Image Root Password, SSH Keys, and Timezone This page outlines procedures to modify the following things for Non-Compute Nodes (NCNs):
 root user password root user SSH keys Timezone  All of the commands in this procedure are intended to be run on a single master or worker node.
 Prerequisites Changing root password and SSH keys Changing timezone  Prerequisites  This procedure can only be done after the PIT node is rebuilt to become a normal master node.</description>
    </item>
    
    <item>
      <title>Set NCN Image Root Password, SSH Keys, And Timezone On Pit Node</title>
      <link>/docs-csm/en-13/operations/security_and_authentication/change_ncn_image_root_password_and_ssh_keys_on_pit_node/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:19 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/security_and_authentication/change_ncn_image_root_password_and_ssh_keys_on_pit_node/</guid>
      <description>Set NCN Image Root Password, SSH Keys, and Timezone on PIT Node  NOTE: This procedure is required during initial CSM installs before management nodes are first deployed.
 Modify the NCN images by setting the root user password and adding SSH keys for the root user account. If desired, also change the timezone for the NCNs.
This page describes this procedure being performed on the PIT node during a first time installation of the CSM software.</description>
    </item>
    
    <item>
      <title>Backup And Restore Vault Clusters</title>
      <link>/docs-csm/en-13/operations/security_and_authentication/backup_and_restore_vault_clusters/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:18 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/security_and_authentication/backup_and_restore_vault_clusters/</guid>
      <description>Backup and Restore Vault Clusters View the existing Vault backups on the system and use a completed backup to perform a restore operation.
Velero is used to perform a nightly backup of Vault. The backup includes Kubernetes object state, in addition to pod volume data for the Vault statefulset. For more information on Velero, refer to the external Velero documentation.
CAUTION: A restore operation should only be performed in extreme situations.</description>
    </item>
    
    <item>
      <title>Certificate Types</title>
      <link>/docs-csm/en-13/operations/security_and_authentication/certificate_types/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:18 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/security_and_authentication/certificate_types/</guid>
      <description>Certificate Types The system software installation process creates an X.509 Certificate Authority (CA) on the primary non-compute node (NCN) and uses the CA to create an NCN host X.509 certificate. This host certificate is used during the installation process to configure the API gateway for TLS so that communications to the gateway can use HTTPS.
Clients should use HTTPS to talk to services behind the API gateway and need to ensure that the NCN CA certificate is known by the client software when making requests.</description>
    </item>
    
    <item>
      <title>Change Air-cooled Node BMC Credentials Using SAT</title>
      <link>/docs-csm/en-13/operations/security_and_authentication/change_air-cooled_node_bmc_credentials/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:18 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/security_and_authentication/change_air-cooled_node_bmc_credentials/</guid>
      <description>Change Air-Cooled Node BMC Credentials Using SAT This procedure describes how to use the System Admin Toolkit&amp;rsquo;s (SAT) sat bmccreds command to set a global credential for all BMCs on air-cooled nodes.
For more information including alternate methods of using sat bmccreds, see: Set BMC Credentials Using SAT, or the sat-bmccreds(8) man page by running sat-man bmccreds.
Limitations All air-cooled and liquid-cooled BMCs share the same global credentials. The air-cooled Slingshot switch controllers (Router BMCs) must have the same credentials as the liquid-cooled Slingshot switch controllers.</description>
    </item>
    
    <item>
      <title>Change Cray Ex Liquid-cooled Cabinet Global Default Password</title>
      <link>/docs-csm/en-13/operations/security_and_authentication/change_ex_liquid-cooled_cabinet_global_default_password/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:18 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/security_and_authentication/change_ex_liquid-cooled_cabinet_global_default_password/</guid>
      <description>Change Cray EX Liquid-Cooled Cabinet Global Default Password This procedure changes the global default root credential on HPE Cray EX liquid-cooled cabinet embedded controllers (BMCs). The chassis management module (CMM) controller (cC), node controller (nC), and Slingshot switch controller (sC) are generically referred to as &amp;ldquo;BMCs&amp;rdquo; in these procedures.
Prerequisites  The Cray command line interface (CLI) tool is initialized and configured on the system. See Configure the Cray Command Line Interface (cray CLI) for more information.</description>
    </item>
    
    <item>
      <title>Change Credentials On Servertech Pdus</title>
      <link>/docs-csm/en-13/operations/security_and_authentication/change_credentials_on_servertech_pdus/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:18 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/security_and_authentication/change_credentials_on_servertech_pdus/</guid>
      <description>Change Credentials on ServerTech PDUs This procedure changes password used by the admn user on ServerTech PDUs. Either a single PDU can be updated to a new credential, or all ServerTech PDUs in the system can be updated to the same global credentials.
NOTES:
 This procedure does not update the default credentials that RTS uses for new ServerTech PDUs added to a system. To change the default credentials, see Update default ServerTech PDU Credentials used by the Redfish Translation Service.</description>
    </item>
    
    <item>
      <title>Change The Keycloak Token Lifetime</title>
      <link>/docs-csm/en-13/operations/security_and_authentication/change_keycloak_token_lifetime/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:18 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/security_and_authentication/change_keycloak_token_lifetime/</guid>
      <description>Change the Keycloak Token Lifetime This document outlines how to change the Keycloak default token lifetime or the token lifetime for a specific client.
Note: The default value for these settings is 365 days.
Procedure Log in to Keycloak with the default admin credentials.
Point a browser at https://auth.cmn.SYSTEM_DOMAIN_NAME/keycloak/admin, replacing SYSTEM_DOMAIN_NAME with the actual NCN&amp;rsquo;s DNS name. Use of the auth.cmn. sub-domain is required for administrative access to Keycloak.
The following is an example URL for a system: https://auth.</description>
    </item>
    
    <item>
      <title>Access The Keycloak User Management Ui</title>
      <link>/docs-csm/en-13/operations/security_and_authentication/access_the_keycloak_user_management_ui/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:17 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/security_and_authentication/access_the_keycloak_user_management_ui/</guid>
      <description>Access the Keycloak User Management UI This procedure can be used to access the interface to manage Keycloak users. Users can be added with this interface. See Create Internal User Accounts in the Keycloak Shasta Realm.
Prerequisites  This procedure uses SYSTEM_DOMAIN_NAME as an example for the DNS name of the non-compute node (NCN). Replace this name with the actual NCN&amp;rsquo;s DNS name while executing this procedure. This procedure assumes that the password for the Keycloak admin account is known.</description>
    </item>
    
    <item>
      <title>Add LDAP User Federation</title>
      <link>/docs-csm/en-13/operations/security_and_authentication/add_ldap_user_federation/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:17 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/security_and_authentication/add_ldap_user_federation/</guid>
      <description>Add LDAP User Federation Add LDAP user federation using the Keycloak localization tool.
 Prerequisites System domain name Procedure  Prerequisites LDAP user federation is not currently configured in Keycloak. For example, if it was not configured in Keycloak when the system was initially installed or the LDAP user federation was removed.
System domain name The SYSTEM_DOMAIN_NAME value found in some of the URLs on this page is expected to be the system&amp;rsquo;s fully qualified domain name (FQDN).</description>
    </item>
    
    <item>
      <title>Add Root Service Account For Gigabyte Controllers</title>
      <link>/docs-csm/en-13/operations/security_and_authentication/add_root_service_account_for_gigabyte_controllers/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:17 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/security_and_authentication/add_root_service_account_for_gigabyte_controllers/</guid>
      <description>Add Root Service Account for Gigabyte Controllers By default, Gigabyte BMC and CMC controllers have the admin service account configured. In order to discover this type of hardware, the root service account needs to be configured.
Prerequisites  The BMC is accessible over the network via hostname or IP address.  Procedure   (ncn#) Retrieve the root user password for this BMC.
  If configuring a BMC already present in the system, then retrieve the device-specific root user password from Vault.</description>
    </item>
    
    <item>
      <title>Api Authorization</title>
      <link>/docs-csm/en-13/operations/security_and_authentication/api_authorization/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:17 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/security_and_authentication/api_authorization/</guid>
      <description>API Authorization Authorization for REST API calls is only done at the API gateway. This is facilitated through policy checks to the Open Policy Agent (OPA). Every REST API call into the system is sent to the OPA to make an authorization decision. The decision is based on the Authenticated JSON Web Token (JWT) passed into the request.
The following is a list of available personas and the supported REST API endpoints for each:</description>
    </item>
    
    <item>
      <title>Audit Logs</title>
      <link>/docs-csm/en-13/operations/security_and_authentication/audit_logs/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:17 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/security_and_authentication/audit_logs/</guid>
      <description>Audit Logs Overview Audit logs are used to monitor the system and search for suspicious behavior. Host and Kubernetes API audit logging can be enabled to produce extra audit logs for analysis. Enabling audit logging is optional. If enabled it generates some load and data on the non-compute nodes (NCNs).
By default, host and Kubernetes API audit logging are not enabled. It is not required for both to be enabled or disabled at the same time.</description>
    </item>
    
    <item>
      <title>Authenticate An Account With The Command Line</title>
      <link>/docs-csm/en-13/operations/security_and_authentication/authenticate_an_account_with_the_command_line/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:17 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/security_and_authentication/authenticate_an_account_with_the_command_line/</guid>
      <description>Authenticate an Account with the Command Line Retrieve a token to authenticate to the Cray CLI using the command line. If the Cray CLI is needed before localization occurs and Keycloak is setup, an administrator can use this procedure to authenticate to the Cray CLI.
Procedure   (ncn-mw#) Retrieve the Kubernetes secret to be used for authentication.
ADMIN_SECRET=$(kubectl get secrets admin-client-auth -ojsonpath=&amp;#39;{.data.client-secret}&amp;#39; | base64 -d)   (ncn-mw#) Create the setup-token.</description>
    </item>
    
    <item>
      <title>System Admin Toolkit (SAT) In</title>
      <link>/docs-csm/en-13/operations/sat/sat_in_csm/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:17 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/sat/sat_in_csm/</guid>
      <description>System Admin Toolkit (SAT) in CSM The System Admin Toolkit (SAT) is a command-line interface that can assist administrators with common tasks, such as troubleshooting and querying information about the HPE Cray EX System, system boot and shutdown, and replacing hardware components. In CSM 1.3 and newer, the sat command is available on the Kubernetes NCNs without installing the SAT product stream.
SAT product stream components It is still possible to install SAT as a separate product stream.</description>
    </item>
    
    <item>
      <title>NTP Resiliency</title>
      <link>/docs-csm/en-13/operations/resiliency/ntp_resiliency/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:16 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/resiliency/ntp_resiliency/</guid>
      <description>NTP Resiliency Sync the time on all non-compute nodes (NCNs) via Network Time Protocol (NTP). Avoid a single point of failure for NTP when testing system resiliency.
Prerequisites This procedure requires administrative privileges.
Procedure   Set the date manually if the time on NCNs is off by more than an a few hours, days, or more.
For example:
timedatectl set-time &amp;#34;2021-02-19 15:04:00&amp;#34;   Configure NTP on the Pre-install Toolkit (PIT).</description>
    </item>
    
    <item>
      <title>Recreate Statefulset Pods On Another Node</title>
      <link>/docs-csm/en-13/operations/resiliency/recreate_statefulset_pods_on_another_node/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:16 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/resiliency/recreate_statefulset_pods_on_another_node/</guid>
      <description>Recreate StatefulSet Pods on Another Node Some pods are members of StatefulSets, meaning that there is a very specific number of them, each likely running on a different node. Similar to DaemonSets, these pods will never be recreated on another node as long as they are sitting in a Terminating state.
Warning: This procedure should only be done for pods that are known to no longer be running. Corruption may occur if the worker node is still running when deleting the deleting the StatefulSet pod in a Terminating state.</description>
    </item>
    
    <item>
      <title>Resilience Of System Management Services</title>
      <link>/docs-csm/en-13/operations/resiliency/resilience_of_system_management_services/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:16 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/resiliency/resilience_of_system_management_services/</guid>
      <description>Resilience of System Management Services HPE Cray EX systems are designed so that system management services (SMS) are fully resilient and that there is no single point of failure. The design of the system allows for resiliency in the following ways:
 Three non-compute nodes (NCNs) are configured as Kubernetes master nodes. When one master goes down, operations (such as jobs running across compute nodes) are expected to continue. At least three utility storage nodes provide persistent storage for the services running on the Kubernetes management nodes.</description>
    </item>
    
    <item>
      <title>Resiliency</title>
      <link>/docs-csm/en-13/operations/resiliency/resiliency/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:16 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/resiliency/resiliency/</guid>
      <description>Resiliency HPE Cray EX systems are designed so that system management services (SMS) are fully resilient and that there is no single point of failure.</description>
    </item>
    
    <item>
      <title>Resiliency Testing Procedure</title>
      <link>/docs-csm/en-13/operations/resiliency/resiliency_testing_procedure/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:16 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/resiliency/resiliency_testing_procedure/</guid>
      <description>Resiliency Testing Procedure This document and the procedures contained within it are for the purposes of communicating the kind of testing done by the internal Cray System Management (CSM) team to ensure a basic level of system resiliency in the event of the loss of a single non-compute node (NCN).
It is assumed that some procedures are already known by admins and thus does not go into great detail or attempt to encompass every command necessary for execution.</description>
    </item>
    
    <item>
      <title>Restore System Functionality If A Kubernetes Worker Node Is Down</title>
      <link>/docs-csm/en-13/operations/resiliency/restore_system_functionality_if_a_kubernetes_worker_node_is_down/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:16 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/resiliency/restore_system_functionality_if_a_kubernetes_worker_node_is_down/</guid>
      <description>Restore System Functionality if a Kubernetes Worker Node is Down Services running on Kubernetes worker nodes can be properly restored if downtime occurs. Use this procedure to ensure that if a Kubernetes worker node is lost or restored after being down, then certain features the node was providing can also be restored or recovered on another node.
Capture the metadata for the unhealthy node before bringing down the node. The pods will successfully terminate when the node goes down, which should resolve most pods in an error state.</description>
    </item>
    
    <item>
      <title>Power Management</title>
      <link>/docs-csm/en-13/operations/power_management/power_management/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:15 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/power_management/power_management/</guid>
      <description>Power Management HPE Cray System Management (CSM) software manages and controls power out-of-band through Redfish APIs. Note that power management features are &amp;ldquo;asynchronous,&amp;rdquo; in that the client must determine whether the component status has changed after a power management API call returns.
In-band power management features are not supported in v1.4.
HPE supports Slurm as a workload manager which reports job energy usage and records it in the ITDB for system accounting purposes.</description>
    </item>
    
    <item>
      <title>Shut Down And Power Off The Management Kubernetes Cluster</title>
      <link>/docs-csm/en-13/operations/power_management/shut_down_and_power_off_the_management_kubernetes_cluster/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:15 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/power_management/shut_down_and_power_off_the_management_kubernetes_cluster/</guid>
      <description>Shut Down and Power Off the Management Kubernetes Cluster Shut down management services and power off the HPE Cray EX management Kubernetes cluster.
 Overview Prerequisites Check health of the management cluster Shut down the Kubernetes management cluster Next step  Overview Understand the following concepts before powering off the management non-compute nodes (NCNs) for the Kubernetes cluster and storage:
 The etcd cluster provides storage for the state of the management Kubernetes cluster.</description>
    </item>
    
    <item>
      <title>Standard Rack Node Power Management</title>
      <link>/docs-csm/en-13/operations/power_management/standard_rack_node_power_management/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:15 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/power_management/standard_rack_node_power_management/</guid>
      <description>Standard Rack Node Power Management HPE Cray EX standard EIA rack node power management is supported by the server vendor BMC firmware. The BMC exposes the power control API for a node through the node&amp;rsquo;s Redfish Power schema.
Out-of-band power management data is polled by a collector and published on a Kafka bus for entry into the Power Management Database (PMDB). Access to the data stored in the PMDB is available through the System Monitoring Application (SMA) Grafana instance.</description>
    </item>
    
    <item>
      <title>System Power Off Procedures</title>
      <link>/docs-csm/en-13/operations/power_management/system_power_off_procedures/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:15 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/power_management/system_power_off_procedures/</guid>
      <description>System Power Off Procedures The procedures in this section detail the high-level tasks required to power off an HPE Cray EX system.
Note about Services Used During System Power Off  The Cray Advanced Platform Monitoring and Control (CAPMC) service controls power to major components. CAPMC sequences the power off tasks in the correct order, but does not gracefully shut down software services. The Boot Orchestration Service (BOS) manages proper shutdown and power off tasks for compute nodes and User Access Nodes (UANs).</description>
    </item>
    
    <item>
      <title>System Power On Procedures</title>
      <link>/docs-csm/en-13/operations/power_management/system_power_on_procedures/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:15 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/power_management/system_power_on_procedures/</guid>
      <description>System Power On Procedures The procedures in this section detail the high-level tasks required to power on an HPE Cray EX system.
Important: If an emergency power off (EPO) event occurred, then see Recover from a Liquid-Cooled Cabinet EPO Event for recovery procedures.
If user IDs or passwords are needed, then see step 1 of the Prepare the System for Power Off procedure.
Note about services used during system power on  The Cray Advanced Platform Monitoring and Control (CAPMC) service controls power to major components.</description>
    </item>
    
    <item>
      <title>User Access To Compute Node Power Data</title>
      <link>/docs-csm/en-13/operations/power_management/user_access_to_compute_node_power_data/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:15 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/power_management/user_access_to_compute_node_power_data/</guid>
      <description>User Access to Compute Node Power Data Shasta Liquid Cooled AMD EPYC compute node power management data available to users.
Shasta Liquid Cooled compute blade power management counters (pm_counters) enable users access to energy usage over time for billing and job profiling.
The blade-level and node-level accumulated energy telemetry is point-in-time power data. Blade accumulated energy data is collected out-of-band and is made available via workload managers. Users have access to the data in-band at the node-level via a special sysfs files in /sys/cray/pm\_counters on the node.</description>
    </item>
    
    <item>
      <title>Power On And Start The Management Kubernetes Cluster</title>
      <link>/docs-csm/en-13/operations/power_management/power_on_and_start_the_management_kubernetes_cluster/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:14 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/power_management/power_on_and_start_the_management_kubernetes_cluster/</guid>
      <description>Power On and Start the Management Kubernetes Cluster Power on and start management services on the HPE Cray EX management Kubernetes cluster.
Prerequisites  All management rack PDUs are connected to facility power and facility power is on. An authentication token is required to access the API gateway and to use the sat command. See the &amp;ldquo;SAT Authentication&amp;rdquo; section of the HPE Cray EX System Admin Toolkit (SAT) product stream documentation (S-8031) for instructions on how to acquire a SAT authentication token.</description>
    </item>
    
    <item>
      <title>Power On The External Lustre File System</title>
      <link>/docs-csm/en-13/operations/power_management/power_on_the_external_lustre_file_system/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:14 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/power_management/power_on_the_external_lustre_file_system/</guid>
      <description>Power On the External Lustre File System Use this procedure as a general guide to power on an external ClusterStor system. Refer to the detailed procedures that support each ClusterStor hardware and software release:
 ClusterStor E1000 Administration Guide 4.2 - S-2758 for ClusterStor E1000 systems ClusterStor Administration Guide 3.4 - S-2756 for ClusterStor L300, L300N systems ClusterStor Administration Guide - S-2755 for Legacy ClusterStor systems  Power up storage nodes in the following sequence:</description>
    </item>
    
    <item>
      <title>Prepare The System For Power Off</title>
      <link>/docs-csm/en-13/operations/power_management/prepare_the_system_for_power_off/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:14 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/power_management/prepare_the_system_for_power_off/</guid>
      <description>Prepare the System for Power Off This procedure prepares the system to remove power from all system cabinets. Be sure the system is healthy and ready to be shut down and powered off.
The sat bootsys shutdown and sat bootsys boot commands are used to shut down the system.
Prerequisites An authentication token is required to access the API gateway and to use the sat command. See the &amp;ldquo;SAT Authentication&amp;rdquo; section of the HPE Cray EX System Admin Toolkit (SAT) product stream documentation (S-8031) for instructions on how to acquire a SAT authentication token.</description>
    </item>
    
    <item>
      <title>Recover From A Liquid Cooled Cabinet Epo Event</title>
      <link>/docs-csm/en-13/operations/power_management/recover_from_a_liquid_cooled_cabinet_epo_event/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:14 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/power_management/recover_from_a_liquid_cooled_cabinet_epo_event/</guid>
      <description>Recover from a Liquid Cooled Cabinet EPO Event Identify an emergency power off (EPO) has occurred and restore cabinets to a healthy state.
CAUTION: Verify the reason why the EPO occurred and resolve that problem before clearing the EPO state.
If a Cray EX liquid-cooled cabinet or cooling group experiences an EPO event, the compute nodes may not boot. Use CAPMC to force off all the chassis affected by the EPO event.</description>
    </item>
    
    <item>
      <title>Save Management Network Switch Configuration Settings</title>
      <link>/docs-csm/en-13/operations/power_management/save_management_network_switch_configurations/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:14 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/power_management/save_management_network_switch_configurations/</guid>
      <description>Save Management Network Switch Configuration Settings Switches must be powered on and operating. This procedure is optional if switch configurations have not changed.
Optional Task: Save management network switch configurations before removing power from cabinets or the CDU. Management switch names are listed in the /etc/hosts file.
Obtain the list of switches From the command line on any NCN run:
grep &amp;#39;sw-&amp;#39; /etc/hosts Example output:
10.252.0.2 sw-spine-001 10.252.0.3 sw-spine-002 10.252.0.4 sw-leaf-001 ncn-m001:~ # Save switch configs Aruba Switch and HPE Server Systems On Aruba-based systems all management network switches will be Aruba and the following procedure.</description>
    </item>
    
    <item>
      <title>Set The Turbo Boost Limit</title>
      <link>/docs-csm/en-13/operations/power_management/set_the_turbo_boost_limit/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:14 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/power_management/set_the_turbo_boost_limit/</guid>
      <description>Set the Turbo Boost Limit Turbo boost limiting is supported on the Intel® and AMD® processors. Because processors have a high degree of variability in the amount of turbo boost each processor can supply, limiting the amount of turbo boost can reduce performance variability and reduce power consumption.
Turbo boost can be limited by setting the turbo_boost_limit kernel parameter to one of these values:
 0 - Disable turbo boost 999 - (default) No limit is applied.</description>
    </item>
    
    <item>
      <title>Shut Down And Power Off Compute And User Access Nodes</title>
      <link>/docs-csm/en-13/operations/power_management/shut_down_and_power_off_compute_and_user_access_nodes/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:14 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/power_management/shut_down_and_power_off_compute_and_user_access_nodes/</guid>
      <description>Shut Down and Power Off Compute and User Access Nodes Shut down and power off compute and user access nodes (UANs). This procedure powers off all compute nodes in the context of an entire system shutdown.
Prerequisites The cray and sat commands must be initialized and authenticated with valid credentials for Keycloak. If these have not been prepared, then see Configure the Cray Command Line Interface (cray CLI) and refer to the &amp;ldquo;SAT Authentication&amp;rdquo; section of the HPE Cray EX System Admin Toolkit (SAT) (S-8031) product stream documentation for instructions on how to acquire a SAT authentication token.</description>
    </item>
    
    <item>
      <title>Ignore Nodes With CAPMC</title>
      <link>/docs-csm/en-13/operations/power_management/ignore_nodes_with_capmc/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:13 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/power_management/ignore_nodes_with_capmc/</guid>
      <description>Ignore Nodes with CAPMC Update the Cray Advanced Platform Monitoring and Control (CAPMC) configmap to ignore non-compute nodes (NCNs) and ensure they cannot be powered off or reset.
Modifying the CAPMC configmap to ignore nodes can prevent them from accidentally being power cycled.
Nodes can also be locked with the Hardware State Manager (HSM) API. Refer to Lock and Unlock Management Nodes for more information.
Prerequisites This procedure requires administrative privileges.</description>
    </item>
    
    <item>
      <title>Liquid-cooled Node Power Management</title>
      <link>/docs-csm/en-13/operations/power_management/liquid_cooled_node_card_power_management/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:13 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/power_management/liquid_cooled_node_card_power_management/</guid>
      <description>Liquid-cooled Node Power Management Liquid-cooled AMD EPYC compute blade node card power capabilities and limits.
Liquid-cooled cabinet node card power features are supported by the node controller (nC) firmware and CPU vendor. The nC exposes the power control API for each node via the node&amp;rsquo;s Redfish Control schema. Out-of-band power management data is produced and collected by the nC hardware and firmware. This data can be published to a collector using the Redfish EventService, or retrieved on-demand from the Redfish ChassisSensors resource.</description>
    </item>
    
    <item>
      <title>Power Off Compute Cabinets</title>
      <link>/docs-csm/en-13/operations/power_management/power_off_compute_cabinets/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:13 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/power_management/power_off_compute_cabinets/</guid>
      <description>Power Off Compute Cabinets Power off HPE Cray EX liquid-cooled and standard racks.
Cabinet/rack types Liquid-cooled cabinets HPE Cray EX liquid-cooled cabinet CDU and PDU circuit breakers are controlled manually.
When the PDU breakers are switched to OFF, the Chassis Management Modules (CMMs) and Cabinet Environmental Controllers (CECs) are also powered off.
Warning: The cabinet 480VAC power bus bars remain energized. Facility power must be disconnected to completely remove power from the cabinet.</description>
    </item>
    
    <item>
      <title>Power Off The External Lustre File System</title>
      <link>/docs-csm/en-13/operations/power_management/power_off_the_external_lustre_file_system/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:13 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/power_management/power_off_the_external_lustre_file_system/</guid>
      <description>Power Off the External Lustre File System General procedure for powering off an external ClusterStor system.
Use this procedure as a general guide to power off an external ClusterStor system. Refer to the detailed procedures in the appropriate ClusterStor administration guide:
   Title Model     ClusterStor E1000 Administration Guide 4.2 - S-2758 ClusterStor E1000   ClusterStor Administration Guide 3.4 - S-2756 ClusterStor L300/L300N   ClusterStor Administration Guide - S-2755 Legacy ClusterStor    Procedure   SSH to the primary MGMT node as admin.</description>
    </item>
    
    <item>
      <title>Power On And Boot Compute And User Access Nodes</title>
      <link>/docs-csm/en-13/operations/power_management/power_on_and_boot_compute_nodes_and_user_access_nodes/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:13 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/power_management/power_on_and_boot_compute_nodes_and_user_access_nodes/</guid>
      <description>Power On and Boot Compute and User Access Nodes Use Boot Orchestration Service (BOS) and choose the appropriate session template to power on and boot compute and UANs.
This procedure boots all compute nodes and user access nodes (UANs) in the context of a full system power-up.
Prerequisites  All compute cabinet PDUs, servers, and switches must be powered on. An authentication token is required to access the API gateway and to use the sat command.</description>
    </item>
    
    <item>
      <title>Power On Compute Cabinets</title>
      <link>/docs-csm/en-13/operations/power_management/power_on_compute_cabinets/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:13 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/power_management/power_on_compute_cabinets/</guid>
      <description>Power On Compute Cabinets Power on liquid-cooled and standard rack cabinet PDUs.
Liquid-cooled Cabinets - HPE Cray EX liquid-cooled cabinet CDU and PDU circuit breakers are controlled manually.
After the CDU is switched on and healthy, the liquid-cooled PDU circuit breakers can be switched ON. With PDU breakers ON, the Chassis Management Modules (CMM) and Cabinet Environmental Controllers (CEC) power on and boot. These devices can then communicate with the management cluster and larger system management network.</description>
    </item>
    
    <item>
      <title>Cray Advanced Platform Monitoring And Control (CAPMC)</title>
      <link>/docs-csm/en-13/operations/power_management/cray_advanced_platform_monitoring_and_control_capmc/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:12 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/power_management/cray_advanced_platform_monitoring_and_control_capmc/</guid>
      <description>Cray Advanced Platform Monitoring and Control (CAPMC) The Cray Advanced Platform Monitoring and Control (CAPMC) service enables direct hardware control of nodes, compute blades, router modules, and liquid-cooled chassis. CAPMC talks to BMCs via Redfish to control power, query status, and manage power capping on target components. These controls enable an administrator and third party software to more intelligently manage state and system-wide power consumption.
Administrators can use the cray CLI for power operations from any system that has HTTPS access to the System Management Services.</description>
    </item>
    
    <item>
      <title>Nexus Space Cleanup</title>
      <link>/docs-csm/en-13/operations/package_repository_management/nexus_space_cleanup/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:12 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/package_repository_management/nexus_space_cleanup/</guid>
      <description>Nexus Space Cleanup Nexus stores all data in a PVC called nexus-data. If this PVC fills up, then it will enter a read-only state. This read-only state causes issues for Nexus as well as all the services that rely on Nexus. There is no automatic cleanup of old data from Nexus.
During the install of any CSM version, a large amount data is added to Nexus. If there has not been a manual cleanup of old files in Nexus, then there is likely to be insufficient space for the next version of CSM to be installed.</description>
    </item>
    
    <item>
      <title>Package Repository Management</title>
      <link>/docs-csm/en-13/operations/package_repository_management/package_repository_management/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:12 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/package_repository_management/package_repository_management/</guid>
      <description>Package Repository Management Repositories are added to systems to extend the system functionality beyond what is initially delivered. The Sonatype Nexus Repository Manager is the primary method for repository management. Nexus hosts the Yum, Docker, raw, and Helm repositories for software and firmware content.
Refer to the following for more information about Nexus:
 The official Sonatype documentation Manage Repositories with Nexus  </description>
    </item>
    
    <item>
      <title>Package Repository Management With Nexus</title>
      <link>/docs-csm/en-13/operations/package_repository_management/package_repository_management_with_nexus/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:12 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/package_repository_management/package_repository_management_with_nexus/</guid>
      <description>Package Repository Management with Nexus Overview of RPM repositories and container registry in Nexus.
 RPM repositories Container registry  Adding images Registry mirror configuration Pull example using CRI Pull example using containerd Pull example using Podman    RPM repositories (ncn#) Repositories are available at https://packages.local/repository/REPO_NAME. For example, to configure the csm-sle-15sp2 repository on a non-compute node (NCN):
zypper addrepo -fG https://packages.local/repository/csm-sle-15sp2 csm-sle-15sp2 Example output:
Adding repository &amp;#39;csm-sle-15sp2&amp;#39; .</description>
    </item>
    
    <item>
      <title>Repair Yum Repository Metadata</title>
      <link>/docs-csm/en-13/operations/package_repository_management/repair_yum_repository_metadata/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:12 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/package_repository_management/repair_yum_repository_metadata/</guid>
      <description>Repair Yum Repository Metadata Nexus may have trouble generating or regenerating repository metadata (for example, repodata/repomd.xml), especially for larger repositories. Configure the Repair - Rebuild Yum repository metadata (repodata) task in Nexus to create the metadata if the standard generation fails. This is not typically needed, so it is considered to be a repair task.
The example in this procedure is for creating a repair task to rebuild Yum metadata for the mirror-1.</description>
    </item>
    
    <item>
      <title>Restrict Admin Privileges In Nexus</title>
      <link>/docs-csm/en-13/operations/package_repository_management/restrict_admin_privileges_in_nexus/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:12 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/package_repository_management/restrict_admin_privileges_in_nexus/</guid>
      <description>Restrict Admin Privileges in Nexus Prior to making the system available to users, change the ingress settings to disable connections to packages.local and registry.local from automatically gaining administrative privileges.
Connections to packages.local and registry.local automatically login clients as the admin user. Administrative privileges enable any user to make anonymous writes to Nexus, which means unauthenticated users can perform arbitrary actions on Nexus itself through the REST API, as well as in repositories by uploading or deleting assets.</description>
    </item>
    
    <item>
      <title>Troubleshoot Nexus</title>
      <link>/docs-csm/en-13/operations/package_repository_management/troubleshoot_nexus/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:12 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/package_repository_management/troubleshoot_nexus/</guid>
      <description>Troubleshoot Nexus This page contains general Nexus troubleshooting topics.
 lookup registry.local: no such host Error initiating layer upload &amp;hellip; in registry.local error: not ready: https://packages.local  lookup registry.local: no such host The following error may occur when running ./lib/setup-nexus.sh:
time=&amp;#34;2021-02-23T19:55:54Z&amp;#34; level=fatal msg=&amp;#34;Error copying tag \&amp;#34;dir:/image/grafana/grafana:7.0.3\&amp;#34;: Error writing blob: Head \&amp;#34;https://registry.local/v2/grafana/grafana/blobs/sha256:cf254eb90de2dc62aa7cce9737ad7e143c679f5486c46b742a1b55b168a736d3\&amp;#34;: dial tcp: lookup registry.local: no such host&amp;#34; + return Or a similar error:
time=&amp;#34;2021-03-04T22:45:07Z&amp;#34; level=fatal msg=&amp;#34;Error copying ref \&amp;#34;dir:/image/cray/cray-ims-load-artifacts:1.0.4\&amp;#34;: Error trying to reuse blob sha256:1ec886c351fa4c330217411b0095ccc933090aa2cd7ae7dcd33bb14b9f1fd217 at destination: Head \&amp;#34;https://registry.</description>
    </item>
    
    <item>
      <title>Clear Gigabyte Cmos</title>
      <link>/docs-csm/en-13/operations/node_management/clear_gigabyte_cmos/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:11 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/node_management/clear_gigabyte_cmos/</guid>
      <description>Clear Gigabyte CMOS Because of a bug in the Gigabyte firmware, the Shasta 1.5 install may negatively impact Gigabyte motherboards when attempting to boot using bonded Mellanox network cards. The result is a board that is unusable until a CMOS clear is physically done via a jumper on the board itself.
A patched firmware release (newer than C20 BIOS) is expected to be available for a future release of Shasta. It is recommended that Gigabyte users wait for this new firmware before attempting an installation of Shasta 1.</description>
    </item>
    
    <item>
      <title>Manage Repositories With Nexus</title>
      <link>/docs-csm/en-13/operations/package_repository_management/manage_repositories_with_nexus/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:11 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/package_repository_management/manage_repositories_with_nexus/</guid>
      <description>Manage Repositories with Nexus This section describes how to connect to Nexus with the Web UI, as well as how to access the REST API from non-compute nodes (NCNs) or compute nodes to manage repositories.
 System domain name Access Nexus with the web UI Use Keycloak to create and manage accounts Use the local Nexus admin account Access Nexus with the REST API  Pagination Check the status of Nexus List repositories List assets Create a repository Update a repository Delete a repository Create a blob store Delete a blob store Authenticate to access the Rest API    System domain name The SYSTEM_DOMAIN_NAME value found in some of the URLs on this page is expected to be the system&amp;rsquo;s fully qualified domain name (FQDN).</description>
    </item>
    
    <item>
      <title>Nexus Configuration</title>
      <link>/docs-csm/en-13/operations/package_repository_management/nexus_configuration/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:11 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/package_repository_management/nexus_configuration/</guid>
      <description>Nexus Configuration Expect each product to create and use its own File type blob store. For example, the Cray System Management (CSM) product uses csm.
The default blob store is also available, but Cray products are discouraged from using it.
Repositories CSM creates the registry (format docker) and charts (format helm) repositories for managing container images and Helm charts across all Cray products. However, each product&amp;rsquo;s release may contain a number of RPM repositories that are added to Nexus.</description>
    </item>
    
    <item>
      <title>Nexus Deployment</title>
      <link>/docs-csm/en-13/operations/package_repository_management/nexus_deployment/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:11 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/package_repository_management/nexus_deployment/</guid>
      <description>Nexus Deployment Nexus is deployed with the cray-nexus chart to the nexus namespace as part of the Cray System Management (CSM) release. Nexus is deployed after critical platform services are up and running. Product installers configure and populate Nexus blob stores and repositories using the cray-nexus-setup container image. As a result, there is no singular product that provides all Nexus repositories or assets; instead, individual products must be installed. However, CSM configures the charts Helm repository and the registry Docker repository, which all products may use.</description>
    </item>
    
    <item>
      <title>Nexus Export And Restore</title>
      <link>/docs-csm/en-13/operations/package_repository_management/nexus_export_and_restore/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:11 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/package_repository_management/nexus_export_and_restore/</guid>
      <description>Nexus Export and Restore The current process for ensuring the safety of the nexus-data PVC is a one time, space intensive, manual process, and is only recommended to be done while Nexus is in a known good state. An export is recommended to be done before an upgrade, in order to enable the ability to roll back. Taking an export can also be used to improve Nexus resiliency by allowing easy fixes for data corruption.</description>
    </item>
    
    <item>
      <title>The Embedded Repository</title>
      <link>/docs-csm/en-13/operations/package_repository_management/nexus_import_embedded_repo/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:11 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/package_repository_management/nexus_import_embedded_repo/</guid>
      <description>The Embedded Repository The &amp;ldquo;embedded repo&amp;rdquo; is the complete set of packages installed on the Kubernetes and Storage Ceph NCN images, as well as the packages found on the PIT ISO. The list of installed packages in these images is queried when building each CSM release tarball, and a repo is created from that list and included in the CSM release tarball. This procedure describes how to manually install the &amp;ldquo;embedded repo&amp;rdquo; into Nexus.</description>
    </item>
    
    <item>
      <title>Manual Wipe Procedures</title>
      <link>/docs-csm/en-13/operations/node_management/wipe_ncn_disks/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:10 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/node_management/wipe_ncn_disks/</guid>
      <description>Manual Wipe Procedures This page details how to wipe disks on NCNs installed with the current version of CSM.
Everything in this section should be considered DESTRUCTIVE.
 NOTE All types of disk wipe can be run from Linux or from an emergency shell.
 Topics  Basic wipe Advanced wipe Full wipe  Basic wipe This wipe erases the magic bits on the disk to prevent them from being recognized, as well as removing the common volume groups.</description>
    </item>
    
    <item>
      <title>Update The Gigabyte Node Bios Time</title>
      <link>/docs-csm/en-13/operations/node_management/update_the_gigabyte_node_bios_time/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:10 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/node_management/update_the_gigabyte_node_bios_time/</guid>
      <description>Update the Gigabyte Node BIOS Time Check and set the time for Gigabyte nodes.
If the console log indicates the time between the rest of the system and the compute nodes is off by several hours, then it prevents the spire-agent from getting a valid certificate, which causes the node boot to drop into the dracut emergency shell.
Procedure   (ncn-mw#) Retrieve the cray-console-operator pod ID.
CONPOD=$(kubectl get pods -n services -o wide|grep cray-console-operator|awk &amp;#39;{print $1}&amp;#39;); echo ${CONPOD} Example output:</description>
    </item>
    
    <item>
      <title>Update The HPE Node Bios Time</title>
      <link>/docs-csm/en-13/operations/node_management/update_the_hpe_node_bios_time/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:10 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/node_management/update_the_hpe_node_bios_time/</guid>
      <description>Update the HPE Node BIOS Time Check and set the time for HPE nodes.
If the time between the rest of the system and the node is off by several hours, then this will prevent the spire-agent from getting a valid certificate, which in turn will cause the node to drop into the dracut emergency shell when booting.
Procedure   Log in to a second terminal session in order to watch the node&amp;rsquo;s console.</description>
    </item>
    
    <item>
      <title>Updating Cabinet Routes On Management NCNs</title>
      <link>/docs-csm/en-13/operations/node_management/updating_cabinet_routes_on_management_ncns/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:10 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/node_management/updating_cabinet_routes_on_management_ncns/</guid>
      <description>Updating Cabinet Routes on Management NCNs This procedure will use configuration from System Layout Service (SLS) to set up the proper routing for all air and liquid-cooled cabinets present in the system on each of the Management NCNs.
Prerequisites   Passwordless SSH to all of the management NCNs is configured.
  Ensure cray-site-init (csi) is installed and available on ncn-m001.
csi version If the csi command is not available, then install it:</description>
    </item>
    
    <item>
      <title>Use The Physical KVM</title>
      <link>/docs-csm/en-13/operations/node_management/use_the_physical_kvm/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:10 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/node_management/use_the_physical_kvm/</guid>
      <description>Use the Physical KVM For those who prefer to stand in front of the system and use a physically connected keyboard, mouse, and monitor, Cray provides a rack-mount-extendable KVM unit installed in rack unit slot 23 (RU23) of the management cabinet. It is connected to the first non-compute node (NCN) by default.
To use it, pull it out and raise the lid.
To bring up the main menu (shown in following figure), press Prnt Scrn.</description>
    </item>
    
    <item>
      <title>Verify Node Removal</title>
      <link>/docs-csm/en-13/operations/node_management/verify_node_removal/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:10 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/node_management/verify_node_removal/</guid>
      <description>Verify Node Removal Use this procedure to verify that a node has been successfully removed from the system.
Prerequisites  The Cray command line interface (CLI) tool is initialized and configured on the system. This procedure requires the component name (xname) of the removed node to be known.  Procedure   Ensure that the Redfish endpoint of the removed node&amp;rsquo;s BMC has been disabled.
cray hsm inventory redfishEndpoints describe x3000c0s19b4 Example output:</description>
    </item>
    
    <item>
      <title>View Bios Logs For Liquid Cooled Nodes</title>
      <link>/docs-csm/en-13/operations/node_management/view_bios_logs_for_liquid_cooled_nodes/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:10 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/node_management/view_bios_logs_for_liquid_cooled_nodes/</guid>
      <description>View BIOS Logs for Liquid Cooled Nodes SSH to a Liquid Cooled node and view the BIOS logs. The BIOS logs for Liquid Cooled node controllers (nC) are stored in the /var/log/n0/current and /var/log/n1/current directories.
The BIOS logs for Liquid Cooled nodes are helpful for troubleshooting boot-related issues.
Prerequisites This procedure requires administrative privileges.
Procedure   Log in to the node.
ssh into the node controller for the host component name (xname).</description>
    </item>
    
    <item>
      <title>Switch PXE Boot From Onboard NIC To Pcie</title>
      <link>/docs-csm/en-13/operations/node_management/switch_pxe_boot_from_onboard_nics_to_pcie/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:09 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/node_management/switch_pxe_boot_from_onboard_nics_to_pcie/</guid>
      <description>Switch PXE Boot from Onboard NIC to PCIe This section details how to migrate NCNs from using their onboard NICs for PXE booting to booting over the PCIe cards.
 Switch PXE Boot from Onboard NIC to PCIe  Enabling UEFI PXE Mode  Mellanox  Print Current UEFI and SR-IOV State Setting Expected Values High-Speed Network  Obtaining Mellanox Tools     QLogic FastLinq  Kernel Modules     Disabling or Removing On-Board Connections    This applies to Newer systems (Spring 2020 or newer) where onboard NICs are still used.</description>
    </item>
    
    <item>
      <title>TLS Certificates For Redfish BMCs</title>
      <link>/docs-csm/en-13/operations/node_management/tls_certificates_for_redfish_bmcs/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:09 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/node_management/tls_certificates_for_redfish_bmcs/</guid>
      <description>TLS Certificates for Redfish BMCs Redfish HTTP communications are capable of using TLS certificates and Certificate Authority (CA) trust bundles to improve security. Several Hardware Management Services (HMS) have been modified to enable the HTTP transports used for Redfish communications to use a CA trust bundle.
The following services communicate with Redfish BMCs:
 State Manager Daemon (SMD) Cray Advanced Platform Monitoring and Control (CAPMC) Firmware Action Service (FAS) HMS Collector River Endpoint Discovery Service (REDS) Mountain Endpoint Discovery Service (MEDS)  Each Redfish BMC must have a TLS certificate in order to be useful.</description>
    </item>
    
    <item>
      <title>Troubleshoot Interfaces With Ip Address Issues</title>
      <link>/docs-csm/en-13/operations/node_management/troubleshoot_interfaces_with_ip_address_issues/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:09 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/node_management/troubleshoot_interfaces_with_ip_address_issues/</guid>
      <description>Troubleshoot Interfaces with IP Address Issues Correct NCNs that are failing to assigning a static IP address or detect a duplicate IP address.
The Wicked network manager tool will fail to bring an interface up if its assigned IP address already exists in the respective LAN. This can be detected by checking for signs of duplicate IP address messages in the log.
Prerequisites An NCN has an interface that is failing to assign a static IP address or that has a duplicate IP address.</description>
    </item>
    
    <item>
      <title>Troubleshoot Issues With Redfish Endpoint Discovery</title>
      <link>/docs-csm/en-13/operations/node_management/troubleshoot_issues_with_redfish_endpoint_discovery/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:09 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/node_management/troubleshoot_issues_with_redfish_endpoint_discovery/</guid>
      <description>Troubleshoot Issues with Redfish Endpoint Discovery If a Redfish endpoint is in the HTTPsGetFailed status, then the endpoint does not need to be fully rediscovered. The error indicates an issue in the inventory process done by the Hardware State Manager (HSM). Restart the inventory process to fix this issue.
Update the HSM inventory to resolve issues with discovering Redfish endpoints.
Error Symptom The following is an example of the HSM error:</description>
    </item>
    
    <item>
      <title>Troubleshoot Loss Of Console Connections And Logs On Gigabyte Nodes</title>
      <link>/docs-csm/en-13/operations/node_management/troubleshoot_loss_of_console_connections_and_logs_on_gigabyte_nodes/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:09 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/node_management/troubleshoot_loss_of_console_connections_and_logs_on_gigabyte_nodes/</guid>
      <description>Troubleshoot Loss of Console Connections and Logs on Gigabyte Nodes Problem Gigabyte console log information will no longer be collected. If attempting to initiate a console session through Cray console services, there will be an error reported. This error will occur every time the node is rebooted unless this workaround is applied.
Prerequisites Console log information is no longer being collected for Gigabyte nodes or ConMan is reporting an error.</description>
    </item>
    
    <item>
      <title>Update Compute Node Mellanox HSN NIC Firmware</title>
      <link>/docs-csm/en-13/operations/node_management/update_compute_node_mellanox_hsn_nic_firmware/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:09 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/node_management/update_compute_node_mellanox_hsn_nic_firmware/</guid>
      <description>Update Compute Node Mellanox HSN NIC Firmware This procedure updates liquid-cooled or standard rack compute node NIC mezzanine cards (NMC) firmware for Slingshot 10 Mellanox ConnectX-5 NICs. The deployed RPM on compute nodes contains the scripts and firmware images required to perform the firmware and configuration updates.
Attention: The NIC firmware update is performed while the node is running the compute image (in-band). Use the CX-5 NIC firmware that is deployed with the compute node RPMs and not from some other repository.</description>
    </item>
    
    <item>
      <title>Replace A Standard Rack Node From A System</title>
      <link>/docs-csm/en-13/operations/node_management/replace_a_standard_rack_node/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:08 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/node_management/replace_a_standard_rack_node/</guid>
      <description>Replace a Standard rack node from a System This procedure will replace a standard node from an HPE Cray EX system.
Procedure  Follow Removing a Standard Node from a System procedure procedure to remove the node from the system. Follow Add a Standard Node from a System procedure procedure to add the replacement node to the system.  </description>
    </item>
    
    <item>
      <title>Repurpose A Compute Node As A UAN</title>
      <link>/docs-csm/en-13/operations/node_management/repurpose_compute_as_uan/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:08 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/node_management/repurpose_compute_as_uan/</guid>
      <description>Repurpose a Compute Node as a UAN It is possible to repurpose a compute node to be used as a User Access Node (UAN). This is typically done when the processor type of the compute node is not yet available in a UAN server.
For more information, see the Repurposing a Compute Node as a UAN section of the HPE Cray User Access Node (UAN) Software Administration Guide (S-8033).</description>
    </item>
    
    <item>
      <title>Reset Credentials On Redfish Devices</title>
      <link>/docs-csm/en-13/operations/node_management/reset_credentials_on_redfish_devices_for_reinstallation/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:08 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/node_management/reset_credentials_on_redfish_devices_for_reinstallation/</guid>
      <description>Reset Credentials on Redfish Devices Before re-installing or upgrading the system the credentials need to be changed back to their defaults for any devices that had their credentials changed post-install. This is necessary for the installation process to properly discover and communicate with these devices.
Prerequisites Administrative privileges are required.
Procedure   Create an SCSD payload file with the default credentials for the Redfish devices that have been changed from the defaults.</description>
    </item>
    
    <item>
      <title>S3fs Usage And Guidelines For Shasta</title>
      <link>/docs-csm/en-13/operations/node_management/s3fs_usage_and_guidelines/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:08 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/node_management/s3fs_usage_and_guidelines/</guid>
      <description>S3FS Usage and Guidelines for Shasta Introduction S3FS is being deployed as tool to provide temporary relief of space usage as well as supporting SDU/NMD services as a near-posix file system to provide a landing point for dumps.
When to Use  If the need is a landing point for large files that may fill up the root volume. Short term storage of large files or rpms.  When NOT to Use  For long term storage of code, test images, test rpms, or tar files.</description>
    </item>
    
    <item>
      <title>Set Gigabyte Node BMC To Factory Defaults</title>
      <link>/docs-csm/en-13/operations/node_management/set_gigabyte_node_bmc_to_factory_defaults/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:08 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/node_management/set_gigabyte_node_bmc_to_factory_defaults/</guid>
      <description>Set Gigabyte Node BMC to Factory Defaults There are cases when a Gigabyte node BMC must be reset to its factory default settings. This page describes when this reset is appropriate, and how to use management scripts and text files to do the reset.
Set the BMC to the factory default settings in the following cases:
 There are problems using the ipmitool command and Redfish does not respond. There are problems using the ipmitool command and Redfish is running.</description>
    </item>
    
    <item>
      <title>Swap A Compute Blade With A Different System</title>
      <link>/docs-csm/en-13/operations/node_management/swap_a_compute_blade_with_a_different_system/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:08 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/node_management/swap_a_compute_blade_with_a_different_system/</guid>
      <description>Swap a Compute Blade with a Different System Swap an HPE Cray EX liquid-cooled compute blade between two systems.
  The two systems in this example are:
  Source system - Cray EX TDS cabinet x9000 with a healthy EX425 blade (Windom dual-injection) in chassis 3, slot 0
  Destination system - Cray EX cabinet x1005 with a defective EX425 blade (Windom dual-injection) in chassis 3, slot 0</description>
    </item>
    
    <item>
      <title>Swap A Compute Blade With A Different System Using SAT</title>
      <link>/docs-csm/en-13/operations/node_management/swap_a_compute_blade_with_a_different_system_using_sat/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:08 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/node_management/swap_a_compute_blade_with_a_different_system_using_sat/</guid>
      <description>Swap a Compute Blade with a Different System Using SAT Swap an HPE Cray EX liquid-cooled compute blade between two systems.
  The two systems in this example are:
  Source system - Cray EX TDS cabinet x9000 with a healthy EX425 blade (Windom dual-injection) in chassis 3, slot 0
  Destination system - Cray EX cabinet x1005 with a defective EX425 blade (Windom dual-injection) in chassis 3, slot 0</description>
    </item>
    
    <item>
      <title>Removing A Liquid-cooled Blade From A System</title>
      <link>/docs-csm/en-13/operations/node_management/removing_a_liquid-cooled_blade_from_a_system/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:07 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/node_management/removing_a_liquid-cooled_blade_from_a_system/</guid>
      <description>Removing a Liquid-cooled blade from a System This procedure will remove a liquid-cooled blades from an HPE Cray EX system.
Perquisites   The Cray command line interface (CLI) tool is initialized and configured on the system. See Configure the Cray CLI.
  Knowledge of whether Data Virtualization Service (DVS) is operating over the Node Management Network (NMN) or the High Speed Network (HSN).
  The Slingshot fabric must be configured with the desired topology for desired state of the blades in the system.</description>
    </item>
    
    <item>
      <title>Removing A Liquid-cooled Blade From A System Using SAT</title>
      <link>/docs-csm/en-13/operations/node_management/removing_a_liquid-cooled_blade_from_a_system_using_sat/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:07 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/node_management/removing_a_liquid-cooled_blade_from_a_system_using_sat/</guid>
      <description>Removing a Liquid-cooled blade from a System Using SAT This procedure will remove a liquid-cooled blade from an HPE Cray EX system.
Prerequisites   The Cray command line interface (CLI) tool is initialized and configured on the system. See Configure the Cray Command Line Interface.
  Knowledge of whether DVS is operating over the Node Management Network (NMN) or the High Speed Network (HSN).
  The Slingshot fabric must be configured with the desired topology for desired state of the blades in the system.</description>
    </item>
    
    <item>
      <title>Removing A Standard Rack Node From A System</title>
      <link>/docs-csm/en-13/operations/node_management/removing_a_standard_node_from_a_system/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:07 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/node_management/removing_a_standard_node_from_a_system/</guid>
      <description>Removing a Standard rack node from a System This procedure will remove one or more air-cooled standard node from an HPE Cray EX system.
This procedure is applicable for the following types of standard rack nodes:
 Single node chassis (DL325, DL385, etc&amp;hellip;) Dual node chassis (Apollo 6500 XL645d, etc&amp;hellip;) Quad dense node chassis (Gigabyte compute node chassis)  Prerequisites  The Cray command line interface (CLI) tool is initialized and configured on the system.</description>
    </item>
    
    <item>
      <title>Replace A Compute Blade</title>
      <link>/docs-csm/en-13/operations/node_management/replace_a_compute_blade/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:07 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/node_management/replace_a_compute_blade/</guid>
      <description>Replace a Compute Blade Replace an HPE Cray EX liquid-cooled compute blade.
Prerequisites   The Cray command line interface (CLI) tool is initialized and configured on the system. See Configure the Cray Command Line Interface.
  The Slingshot fabric must be configured with the desired topology.
  The System Layout Service (SLS) must have the desired HSN configuration.
  Check the status of the high-speed network (HSN) and record link status before the procedure.</description>
    </item>
    
    <item>
      <title>Replace A Compute Blade Using SAT</title>
      <link>/docs-csm/en-13/operations/node_management/replace_a_compute_blade_using_sat/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:07 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/node_management/replace_a_compute_blade_using_sat/</guid>
      <description>Replace a Compute Blade Using SAT Replace an HPE Cray EX liquid-cooled compute blade.
Prerequisites   The Cray command line interface (CLI) tool is initialized and configured on the system. See Configure the Cray Command Line Interface.
  The Slingshot fabric must be configured with the desired topology.
  The System Layout Service (SLS) must have the desired HSN configuration.
  Check the status of the high-speed network (HSN) and record link status before the procedure.</description>
    </item>
    
    <item>
      <title>Validate Boot Loader</title>
      <link>/docs-csm/en-13/operations/node_management/rebuild_ncns/validate_boot_loader/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:07 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/node_management/rebuild_ncns/validate_boot_loader/</guid>
      <description>Validate Boot Loader Perform the following steps on ncn-m001.
  (ncn-m001#) Run the script to ensure the local BOOTRAID has a valid kernel, initrd, and grub.cfg.
pdsh -b -w $(grep -oP &amp;#39;ncn-\w\d+&amp;#39; /etc/hosts | sort -u | tr -t &amp;#39;\n&amp;#39; &amp;#39;,&amp;#39;) &amp;#39; /opt/cray/tests/install/ncn/scripts/check_bootloader.sh &amp;#39; If the script fails because of &amp;lsquo;Host key verification&amp;rsquo; failures, then follow the documentation to Apply root SSH keys to NCNs.
  Next Step If this is a storage node rebuild, proceed to the next step to Re-add Storage Node to Ceph.</description>
    </item>
    
    <item>
      <title>Final Validation Steps</title>
      <link>/docs-csm/en-13/operations/node_management/rebuild_ncns/final_validation_steps/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:06 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/node_management/rebuild_ncns/final_validation_steps/</guid>
      <description>Final Validation Steps Use this procedure to finish validating the success of a rebuilt NCN(s).
Procedure   Confirm what the Configuration Framework Service (CFS) configurationStatus is for the desiredConfig after rebooting the node.
NOTE The following command will indicate if a CFS job is currently in progress for this node.
IMPORTANT: The following command assumes that the variables from the prerequisites section have been set.
cray cfs components describe $XNAME --format json Example output:</description>
    </item>
    
    <item>
      <title>Identify Nodes And Update Metadata</title>
      <link>/docs-csm/en-13/operations/node_management/rebuild_ncns/identify_nodes_and_update_metadata/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:06 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/node_management/rebuild_ncns/identify_nodes_and_update_metadata/</guid>
      <description>Identify Nodes and Update Metadata Use the following procedure to inspect and modify the Boot Script Service (BSS) boot parameters JSON file.
This section applies to all node types. The commands in this section assume the variables from the prerequisites section have been set.
Procedure (ncn-m001#) The following steps can all be done on ncn-m001.
  Generate the BSS boot parameters JSON file.
Run the following commands from a node that has cray CLI initialized:</description>
    </item>
    
    <item>
      <title>Post Rebuild Storage Node Validation</title>
      <link>/docs-csm/en-13/operations/node_management/rebuild_ncns/post_rebuild_storage_node_validation/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:06 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/node_management/rebuild_ncns/post_rebuild_storage_node_validation/</guid>
      <description>Post Rebuild Storage Node Validation Validate the storage node rebuilt successfully.
Skip this section if a master or worker node was rebuilt.
Procedure   Verify there are 3 mons, 3 mds, 3 mgr processes, and rgws.
ceph -s Example output:
cluster: id: 4c9e9d74-a208-11ed-b008-98039bb427f6 health: HEALTH_OK services: mon: 3 daemons, quorum ncn-s001,ncn-s002,ncn-s003 (age 19m) mgr: ncn-s002.mgvtbe(active, since 18m), standbys: ncn-s001.gvuyjf, ncn-s003.ndzqsk mds: 1/1 daemons up, 1 standby, 1 hot standby osd: 18 osds: 18 up (since 17m), 18 in (since 18m) rgw: 3 daemons active (3 hosts, 1 zones) data: volumes: 1/1 healthy pools: 13 pools, 553 pgs objects: 38.</description>
    </item>
    
    <item>
      <title>Power Cycle And Rebuild Nodes</title>
      <link>/docs-csm/en-13/operations/node_management/rebuild_ncns/power_cycle_and_rebuild_nodes/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:06 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/node_management/rebuild_ncns/power_cycle_and_rebuild_nodes/</guid>
      <description>Power Cycle and Rebuild Nodes This section applies to all node types. The commands in this section assume the variables from the prerequisites section have been set.
Procedure   Open and watch the console for the node being rebuilt.
  Log in to a second session to use it to watch the console using the instructions at the link below:
Open this link in a new tab or page Log in to a Node Using ConMan</description>
    </item>
    
    <item>
      <title>Prepare Storage Nodes</title>
      <link>/docs-csm/en-13/operations/node_management/rebuild_ncns/prepare_storage_nodes/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:06 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/node_management/rebuild_ncns/prepare_storage_nodes/</guid>
      <description>Prepare Storage Nodes Prepare a storage node before rebuilding it.
IMPORTANT: All of the output examples may not reflect the cluster status where this operation is being performed. For example, if this is a rebuild in place, then Ceph components will not be reporting down, in contrast to a failed node rebuild.
 Prerequisites Procedure Next step  Prerequisites   Ensure that the latest CSM documentation RPM is installed on ncn-m001.</description>
    </item>
    
    <item>
      <title>Re-add A Storage Node To Ceph</title>
      <link>/docs-csm/en-13/operations/node_management/rebuild_ncns/re-add_storage_node_to_ceph/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:06 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/node_management/rebuild_ncns/re-add_storage_node_to_ceph/</guid>
      <description>Re-Add a Storage Node to Ceph Use the following procedure to re-add a Ceph node to the Ceph cluster.
NOTE This operation can be done to add more than one node at the same time.
Run the Ceph Join Script   In a separate window, log into one of the first three storage nodes (ncn-s001, ncn-s002, or ncn-s003) and execute the following:
watch ceph -s   (ncn-m001#) Copy /usr/share/doc/csm/scripts/join_ceph_cluster.</description>
    </item>
    
    <item>
      <title>Rebuild NCNs</title>
      <link>/docs-csm/en-13/operations/node_management/rebuild_ncns/rebuild_ncns/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:06 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/node_management/rebuild_ncns/rebuild_ncns/</guid>
      <description>Rebuild NCNs Rebuild a master, worker, or storage non-compute node (NCN). Use this procedure in the event that a node has a hardware failure, or some other issue with the node has occurred that warrants rebuilding the node.
 Prerequisites Procedure Validation  Prerequisites The system is fully installed and has transitioned off of the LiveCD.
(ncn#) Variables set with the name of the node being rebuilt and its component name (xname) are required.</description>
    </item>
    
    <item>
      <title>Move A Liquid-cooled Blade Within A System</title>
      <link>/docs-csm/en-13/operations/node_management/move_a_liquid-cooled_blade_within_a_system/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:05 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/node_management/move_a_liquid-cooled_blade_within_a_system/</guid>
      <description>Move a liquid-cooled blade within a System This top level procedure outlines common scenarios for moving blades around within an HPE Cray EX system.
Blade movement scenarios:
 Scenario 1: Swap locations of two blades Scenario 2: Move blade into a populated slot Scenario 3: Move blade into an unpopulated slot  Prerequisites  Knowledge of whether DVS is operating over the Node Management Network (NMN) or the High Speed Network (HSN).</description>
    </item>
    
    <item>
      <title>NCN Drive Identification</title>
      <link>/docs-csm/en-13/operations/node_management/ncn_identify_drives_using_ledctl/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:05 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/node_management/ncn_identify_drives_using_ledctl/</guid>
      <description>NCN Drive Identification Basic usage for the ledmon/ledctl software for drive identification using the drive LEDs.
Usage Turn on led locator beacon
ledctl locate=/dev/&amp;lt;drive&amp;gt; Turn off led locator beacon
ledctl locate_off=/dev/&amp;lt;drive&amp;gt; </description>
    </item>
    
    <item>
      <title>NCN Network Troubleshooting</title>
      <link>/docs-csm/en-13/operations/node_management/ncn_network_troubleshooting/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:05 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/node_management/ncn_network_troubleshooting/</guid>
      <description>NCN Network Troubleshooting Interfaces within the network stack can be reloaded or reset to fix wedged interfaces. The NCNs have network device names set during first boot. The names vary based on the available hardware. For more information, see NCN Networking. Any process covered on this page will be covered by the installer.
The use cases for resetting services:
 Interfaces not showing up IP Addresses not applying Member/children interfaces not being included  Topics  Restart Network Services and Interfaces Command Reference  Check interface status (up/down/broken) Show routing and status for all devices Print real devices ( ignore no-device ) Show the currently enabled network service (Wicked or Network Manager)    Restart Network Services and Interfaces There are a few daemons that make up the SUSE network stack.</description>
    </item>
    
    <item>
      <title>Node Management</title>
      <link>/docs-csm/en-13/operations/node_management/node_management/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:05 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/node_management/node_management/</guid>
      <description>Node Management The HPE Cray EX systems include two node types:
 Compute Nodes that run high-performance computing applications and are named nidXXXXXX. Every system must contain four or more compute nodes, starting at nid000001. Non-Compute Nodes (NCNs) that carry out system management functions as part of the management Kubernetes cluster. NCNs outside of the Kubernetes cluster function as application nodes (AN).  Nine or more management NCNs host system services:</description>
    </item>
    
    <item>
      <title>Node Management Workflows</title>
      <link>/docs-csm/en-13/operations/node_management/node_management_workflows/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:05 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/node_management/node_management_workflows/</guid>
      <description>Node Management Workflows The following workflows are intended to be high-level overviews of node management tasks. These workflows depict how services interact with each other during node management and help to provide a quicker and deeper understanding of how the system functions.
The workflows and procedures in this section include:
 Add Nodes Remove Nodes Replace Nodes Move Nodes  Add Nodes  Add a Standard Rack Node  Use Cases: Administrator permanently adds select compute nodes to expand the system.</description>
    </item>
    
    <item>
      <title>Reboot NCNs</title>
      <link>/docs-csm/en-13/operations/node_management/reboot_ncns/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:05 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/node_management/reboot_ncns/</guid>
      <description>Reboot NCNs The following is a high-level overview of the non-compute node (NCN) reboot workflow:
  Run the NCN pre-reboot checks and procedures.
 Ensure that ncn-m001 is not booted to the LiveCD / PIT node. Check the metal.no-wipe settings for all NCNs. Run all platform health checks, including checks on the Border Gateway Protocol (BGP) peering sessions. Validate the current boot order.    Run the rolling NCN reboot procedure.</description>
    </item>
    
    <item>
      <title>Enable Ipmi Access On HPE Ilo BMCs</title>
      <link>/docs-csm/en-13/operations/node_management/enable_ipmi_access_on_hpe_ilo_bmcs/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:04 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/node_management/enable_ipmi_access_on_hpe_ilo_bmcs/</guid>
      <description>Enable IPMI access on HPE iLO BMCs New HPE nodes ship with with IPMI access disabled by default. In order for CSM to fully manage HPE nodes, IPMI access must be enabled on HPE node BMCs.
Prerequisites  The BMC or CMC is accessible over the network via hostname or IP address.  Procedure   (ncn#) Set up an environment variable with the hostname or IP address of the BMC where IPMI needs to be enabled.</description>
    </item>
    
    <item>
      <title>Enable Passwordless Connections To Liquid Cooled Node BMCs</title>
      <link>/docs-csm/en-13/operations/node_management/enable_passwordless_connections_to_liquid_cooled_node_bmcs/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:04 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/node_management/enable_passwordless_connections_to_liquid_cooled_node_bmcs/</guid>
      <description>Enable Passwordless Connections to Liquid Cooled Node BMCs Set the passwordless SSH keys for the root account and/or console of all liquid-cooled Baseboard Management Controllers (BMCs) on the system. This procedure will not work on BMCs for air-cooled hardware.
Warning: If admin uses SCSD to update the SSHConsoleKey value outside of ConMan, it will disrupt the ConMan connection to the console and collection of console logs. Refer to ConMan for more information.</description>
    </item>
    
    <item>
      <title>Find Node Type And Manufacturer</title>
      <link>/docs-csm/en-13/operations/node_management/find_node_type_and_manufacturer/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:04 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/node_management/find_node_type_and_manufacturer/</guid>
      <description>Find Node Type and Manufacturer There are three different vendors providing nodes for air-cooled cabinets, which are Gigabyte, Intel, and HPE. The Hardware State Manager (HSM) contains the information required to determine which type of air-cooled node is installed. The endpoint returned in the HSM command can be used to determine the manufacturer.
HPE nodes contain the /redfish/v1/Systems/1 endpoint:
cray hsm inventory componentEndpoints describe XNAME --format json | jq &#39;.RedfishURL&#39; &amp;quot;x3000c0s18b0/redfish/v1/Systems/1&amp;quot; Gigabyte nodes contain the /redfish/v1/Systems/Self endpoint:</description>
    </item>
    
    <item>
      <title>Launch A Virtual KVM On Gigabyte Servers</title>
      <link>/docs-csm/en-13/operations/node_management/launch_a_virtual_kvm_on_gigabyte_nodes/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:04 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/node_management/launch_a_virtual_kvm_on_gigabyte_nodes/</guid>
      <description>Launch a Virtual KVM on Gigabyte Servers This procedure shows how to launch a virtual KVM to connect to Gigabyte node. The virtual KVM can be launched on any host that is on the same network as the node&amp;rsquo;s BMC. This method of connecting to a node is frequently used during system installation.
Prerequisites  A laptop or workstation with a browser and access to the Internet The externally visible BMC IP address of the node&amp;rsquo;s integrated BMC  Procedure   Connect to the node&amp;rsquo;s BMC by entering the externally visible BMC IP address in the address bar of a web browser.</description>
    </item>
    
    <item>
      <title>Launch A Virtual KVM On Intel Servers</title>
      <link>/docs-csm/en-13/operations/node_management/launch_a_virtual_kvm_on_intel_nodes/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:04 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/node_management/launch_a_virtual_kvm_on_intel_nodes/</guid>
      <description>Launch a Virtual KVM on Intel Servers This procedure shows how to launch a virtual KVM to connect to an Intel node. The virtual KVM can be launched on any host that is on the same network as the node&amp;rsquo;s BMC. This method of connecting to a node is frequently used during system installation.
Prerequisites  A laptop or workstation with a browser and access to the Internet The externally visible BMC IP address of the node&amp;rsquo;s integrated BMC  Procedure   Connect to the node&amp;rsquo;s BMC by entering the externally visible BMC IP address in the address bar of a web browser.</description>
    </item>
    
    <item>
      <title>Move A Standard Rack Node</title>
      <link>/docs-csm/en-13/operations/node_management/move_a_standard_rack_node/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:04 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/node_management/move_a_standard_rack_node/</guid>
      <description>Move a Standard Rack Node Update the location-based component name (xname) for a standard rack node within the system.
Prerequisites   An authentication token has been retrieved.
function get_token () { curl -s -S -d grant_type=client_credentials \ -d client_id=admin-client \ -d client_secret=`kubectl get secrets admin-client-auth -o jsonpath=&#39;{.data.client-secret}&#39; | base64 -d` \ https://api-gw-service-nmn.local/keycloak/realms/shasta/protocol/openid-connect/token | jq -r &#39;.access_token&#39; }   The Cray command line interface (CLI) tool is initialized and configured on the system.</description>
    </item>
    
    <item>
      <title>Move A Standard Rack Node (same Rack/same HSN Ports)</title>
      <link>/docs-csm/en-13/operations/node_management/move_a_standard_rack_node_samerack_samehsnports/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:04 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/node_management/move_a_standard_rack_node_samerack_samehsnports/</guid>
      <description>Move a Standard Rack Node (Same Rack/Same HSN Ports) This procedure move standard rack UAN or compute node to a different location and uses the same Slingshot switch ports and management network ports.
Update the location-based component name (xname) for a standard rack node within the system.
If a node has an incorrect component name (xname) based on its physical location, then this procedure can be used to correct the component name (xname) of the node without the need to physically move the node.</description>
    </item>
    
    <item>
      <title>Customize Pcie Hardware</title>
      <link>/docs-csm/en-13/operations/node_management/customize_disk_hardware/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:03 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/node_management/customize_disk_hardware/</guid>
      <description>Customize PCIe Hardware This page will assist an admin with changing the kernel parameters for NCNs that have extra disks.
 NOTE: If a system&amp;rsquo;s hardware is Plan of Record (PoR), then this page is not needed.
 For any procedure below, it is assumed that the extra disks are going to be utilized. If they are undesired, then the only action item to do is to yank/remove/pull the disks from the NCN.</description>
    </item>
    
    <item>
      <title>Customize Pcie Hardware</title>
      <link>/docs-csm/en-13/operations/node_management/customize_pcie_hardware/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:03 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/node_management/customize_pcie_hardware/</guid>
      <description>Customize PCIe Hardware This page will assist an administrator with changing the NCN udev rules for varying PCIe hardware.
 NOTE: If a system&amp;rsquo;s hardware is Plan of Record (PoR), then this page is not needed.
 Procedure Identify the hardware configuration by PXE booting a node.
  (pit#) Prevent the network boots from completing by removing the links generated by set-sqfs-links.sh.
rm /var/www/ncn-*/{initrd.img.xz,kernel,filesystem.squashfs} The NCNs will fetch the iPXE binary and then pause; this pause prevents the NCN from continuing to boot, providing an opportunity to collect information from it.</description>
    </item>
    
    <item>
      <title>Defragment Nid Numbering</title>
      <link>/docs-csm/en-13/operations/node_management/defragment_nid_numbering/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:03 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/node_management/defragment_nid_numbering/</guid>
      <description>Defragment NID Numbering This procedure will rearrange NIDs for specified compute nodes to create a numerically (NID) and lexicographically (xname) contiguous block of NIDs at the specified start point.
It is recommended that the system be taken down for maintenance while performing this procedure.
This procedure should only be performed if absolutely required. Some reasons for needing to perform this procedure include:
 Compute nodes were added to SLS with incorrect NID numbering, missing node entries, and/or extra node entries.</description>
    </item>
    
    <item>
      <title>Disable Nodes</title>
      <link>/docs-csm/en-13/operations/node_management/disable_nodes/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:03 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/node_management/disable_nodes/</guid>
      <description>Disable Nodes Use the Hardware State Manager (HSM) Cray CLI commands to disable nodes on the system.
Disabling nodes that are not configured correctly allows the system to successfully boot.
Prerequisites  The Cray command line interface (CLI) tool is initialized and configured on the system.  Procedure   Disable one or more nodes with HSM.
cray hsm state components bulkEnabled update --enabled false --component-ids XNAME_LIST   Verify the desired nodes are disabled.</description>
    </item>
    
    <item>
      <title>Dump A Non-compute Node</title>
      <link>/docs-csm/en-13/operations/node_management/dump_a_non-compute_node/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:03 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/node_management/dump_a_non-compute_node/</guid>
      <description>Dump a Non-Compute Node Trigger an NCN memory dump and send the dump for analysis. This procedure is helpful for debugging NCN crashes.
Prerequisites A non-compute node (NCN) has crashed or an admin has triggered a node crash.
Procedure   Force a dump on an NCN.
echo c &amp;gt; /proc/sysrq-trigger   Wait for the node to reboot.
The NCN dump is stored in /var/crash is on local disk after the node is rebooted.</description>
    </item>
    
    <item>
      <title>Enable Nodes</title>
      <link>/docs-csm/en-13/operations/node_management/enable_nodes/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:03 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/node_management/enable_nodes/</guid>
      <description>Enable Nodes Use the Hardware State Manager (HSM) Cray CLI commands to enable nodes on the system.
Enabling nodes that are available provides an accurate system configuration and node map.
Prerequisites  The Cray command line interface (CLI) tool is initialized and configured on the system.  Procedure   Enable one or more nodes with HSM.
cray hsm state components bulkEnabled update --enabled true --component-ids XNAME_LIST   Verify the desired nodes are enabled.</description>
    </item>
    
    <item>
      <title>Change Settings For Hms Collector Polling Of Air-cooled Nodes</title>
      <link>/docs-csm/en-13/operations/node_management/change_settings_for_hms_collector_polling_of_air_cooled_nodes/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:02 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/node_management/change_settings_for_hms_collector_polling_of_air_cooled_nodes/</guid>
      <description>Change Settings for HMS Collector Polling of Air-Cooled Nodes The cray-hms-hmcollector service polls all air-cooled hardware to gather the necessary telemetry information for use by other services, such as the Cray Advanced Platform Monitoring and Control (CAPMC) service. This polling occurs every 10 seconds on a continual basis. Instabilities with the AMI Redfish implementation in the Gigabyte BMCs require a less significant approach when gathering power and temperature telemetry data. If the BMCs are overloaded, they can become unresponsive, return incorrect data, or encounter other errors.</description>
    </item>
    
    <item>
      <title>Check And Set The Metal.no-wipe Setting On NCNs</title>
      <link>/docs-csm/en-13/operations/node_management/check_and_set_the_metalno-wipe_setting_on_ncns/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:02 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/node_management/check_and_set_the_metalno-wipe_setting_on_ncns/</guid>
      <description>Check and Set the metal.no-wipe Setting on NCNs Configure the metal.no-wipe setting on non-compute nodes (NCNs) to preserve data on the nodes before doing an NCN reboot.
Run the ./ncnGetXnames.shscript to view the metal.no-wipe settings for each NCN. The component name (xname) and metal.no-wipe settings are also dumped out when executing the /opt/cray/platform-utils/ncnHealthChecks.sh script.
Prerequisites This procedure requires administrative privileges.
Procedure   Change to the /opt/cray/platform-utils directory on any master or worker NCN.</description>
    </item>
    
    <item>
      <title>Check The BMC Failover Mode</title>
      <link>/docs-csm/en-13/operations/node_management/check_the_bmc_failover_mode/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:02 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/node_management/check_the_bmc_failover_mode/</guid>
      <description>Check the BMC Failover Mode Gigabyte BMCs must have their failover mode disabled to prevent incorrect network assignment.
If Gigabyte BMC failover mode is not disabled, then some BMCs may receive incorrect IP addresses. Specifically, a BMC may request an IP address on the wrong subnet and be unable to re-acquire a new IP address on the correct subnet. If this occurs, administrators should ensure that the impacted BMC has its failover feature disabled.</description>
    </item>
    
    <item>
      <title>Clear Space In Root File System On Worker Nodes</title>
      <link>/docs-csm/en-13/operations/node_management/clear_space_in_root_file_system_on_worker_nodes/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:02 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/node_management/clear_space_in_root_file_system_on_worker_nodes/</guid>
      <description>Clear Space in Root File System on Worker Nodes The disk space on an NCN worker node can fill up if any services are consuming a large portion of the root file system on the node. This procedure shows how to safely clear some space on worker nodes to return them to an appropriate storage threshold.
Prerequisites An NCN worker node has a full disk.
Procedure   Check to see if Docker is running.</description>
    </item>
    
    <item>
      <title>Configuration Of NCN Bonding</title>
      <link>/docs-csm/en-13/operations/node_management/configuration_of_ncn_bonding/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:02 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/node_management/configuration_of_ncn_bonding/</guid>
      <description>Configuration of NCN Bonding Non-compute nodes (NCNs) have network interface controllers (NICs) connected to the management network that are configured in a redundant manner via Link Aggregation Control Protocol (LACP) link aggregation. The link aggregation configuration can be modified by editing and applying various configuration files either through Ansible or the interfaces directly.
The bond configuration exists across three files on an NCN. These files may vary depending on the NCN in use:</description>
    </item>
    
    <item>
      <title>Configure NTP On NCNs</title>
      <link>/docs-csm/en-13/operations/node_management/configure_ntp_on_ncns/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:02 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/node_management/configure_ntp_on_ncns/</guid>
      <description>Configure NTP on NCNs The management nodes serve Network Time Protocol (NTP) at stratum 10, except for ncn-m001, which serves at stratum 8 (or lower if an upstream NTP server is set). All management nodes peer with each other.
Until an upstream NTP server is configured, the time on the NCNs may not match the current time at the site, but they will stay in sync with each other.
Topics  Fix BSS metadata Fix broken configurations  Fix BSS metadata If nodes are missing metadata for NTP, then the data must be generated using csi and the system&amp;rsquo;s system_config.</description>
    </item>
    
    <item>
      <title>Add A Standard Rack Node</title>
      <link>/docs-csm/en-13/operations/node_management/add_a_standard_rack_node/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:01 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/node_management/add_a_standard_rack_node/</guid>
      <description>Add a Standard Rack Node These procedures are intended for trained technicians and support personnel only. Always follow ESD precautions when handling this equipment.
The example is this procedure adds a User Access Node (UAN) or compute node to an HPE Cray standard rack system. This example adds a node to rack number 3000 at U27.
Procedures for updating the Hardware State Manager (HSM) or System Layout Service (SLS) are similar when adding additional compute nodes or User Application Nodes (UANs).</description>
    </item>
    
    <item>
      <title>Add Additional Air-cooled Cabinets To A System</title>
      <link>/docs-csm/en-13/operations/node_management/add_additional_air-cooled_cabinets_to_a_system/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:01 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/node_management/add_additional_air-cooled_cabinets_to_a_system/</guid>
      <description>Add Additional Air-Cooled Cabinets to a System This procedure adds one or more air-cooled cabinets and all associated hardware within the cabinet except for management NCNs.
Prerequisites  The system&amp;rsquo;s SHCD file has been updated with the new cabinets and cabling changes. The new cabinets have been cabled to the system, and the system&amp;rsquo;s cabling has been validated to be correct. The following procedure has been completed: Create a Backup of the SLS Postgres Database.</description>
    </item>
    
    <item>
      <title>Add Additional Liquid-cooled Cabinets To A System</title>
      <link>/docs-csm/en-13/operations/node_management/add_additional_liquid-cooled_cabinets_to_a_system/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:01 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/node_management/add_additional_liquid-cooled_cabinets_to_a_system/</guid>
      <description>Add Additional Liquid-Cooled Cabinets to a System This top level procedure outlines the process for adding additional liquid-cooled cabinets to a currently installed system.
Prerequisites  The system&amp;rsquo;s SHCD file has been updated with the new cabinets and cabling changes. The new cabinets have been cabled to the system, and the system&amp;rsquo;s cabling has been validated to be correct. Follow the procedure Create a Backup of the SLS Postgres Database. Follow the procedure Create a Backup of the HSM Postgres Database.</description>
    </item>
    
    <item>
      <title>Adding A Liquid-cooled Blade To A System</title>
      <link>/docs-csm/en-13/operations/node_management/adding_a_liquid-cooled_blade_to_a_system/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:01 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/node_management/adding_a_liquid-cooled_blade_to_a_system/</guid>
      <description>Adding a Liquid-cooled Blade to a System This procedure will add a liquid-cooled blades from an HPE Cray EX system.
Prerequisites   The Cray command line interface (CLI) tool is initialized and configured on the system. See Configure the Cray CLI.
  Knowledge of whether DVS is operating over the Node Management Network (NMN) or the High Speed Network (HSN).
  Blade is being added to an existing liquid-cooled cabinet in the system.</description>
    </item>
    
    <item>
      <title>Adding A Liquid-cooled Blade To A System Using SAT</title>
      <link>/docs-csm/en-13/operations/node_management/adding_a_liquid-cooled_blade_to_a_system_using_sat/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:01 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/node_management/adding_a_liquid-cooled_blade_to_a_system_using_sat/</guid>
      <description>Adding a Liquid-cooled blade to a System Using SAT This procedure will add a liquid-cooled blade to an HPE Cray EX system.
Prerequisites   The Cray command line interface (CLI) tool is initialized and configured on the system. See Configure the Cray Command Line Interface.
  Knowledge of whether DVS is operating over the Node Management Network (NMN) or the High Speed Network (HSN).
  Blade is being added to an existing liquid-cooled cabinet in the system.</description>
    </item>
    
    <item>
      <title>Build NCN Images Locally</title>
      <link>/docs-csm/en-13/operations/node_management/build_ncn_images_locally/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:01 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/node_management/build_ncn_images_locally/</guid>
      <description>Build NCN Images Locally Build and test NCN images locally by using the following procedure. This procedure can be done on any x86 machine with the following prerequisites.
Necessary software The listed software below will equip a local machine or build server to build for any medium (.squashfs, .vbox, .qcow2, .iso).
 media (.iso, .ovf, or .qcow2) (depending on the layer) packer qemu envsubst  Media packer can intake any ISO, the sections below detail utilized base ISOs in CRAY HPCaaS.</description>
    </item>
    
    <item>
      <title>Change Java Security Settings</title>
      <link>/docs-csm/en-13/operations/node_management/change_java_security_settings/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:01 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/node_management/change_java_security_settings/</guid>
      <description>Change Java Security Settings If Java will not allow a connection to an Intel node via SOL or iKVM, change Java security settings to add an exception for the node&amp;rsquo;s BMC IP address.
The Intel nodes ship with an insecure certificate, which causes an exception for Java when trying to connect via SOL or iKVM to these nodes. The workaround is to add the node&amp;rsquo;s BMC IP address to the Exception Site List in the Java Control Panel of the machine attempting to connect to the Intel node.</description>
    </item>
    
    <item>
      <title>Add TLS Certificates To BMCs</title>
      <link>/docs-csm/en-13/operations/node_management/add_tls_certificates_to_bmcs/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:00 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/node_management/add_tls_certificates_to_bmcs/</guid>
      <description>Add TLS Certificates to BMCs Use the System Configuration Service (SCSD) tool to create TLS certificates and store them in Vault secure storage. Once certificates are created, they are placed on to the target BMCs.
 Prerequisites Limitations Generate TLS certificates Regenerate TLS certificates  Prerequisites  The Cray command line interface (CLI) tool is initialized and configured on the system. See Configure the Cray CLI.  Limitations TLS certificates can only be set for liquid-cooled BMCs.</description>
    </item>
    
    <item>
      <title>Remove Switch Configuration For NCN</title>
      <link>/docs-csm/en-13/operations/node_management/add_remove_replace_ncns/remove_switch_config/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:00 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/node_management/add_remove_replace_ncns/remove_switch_config/</guid>
      <description>Remove Switch Configuration for NCN Description Update the network switches for the NCN that was removed.
Procedure Update Networking to Remove NCN Details coming soon.
Next Step Proceed to the next step to Redeploy Services or return to the main Add, Remove, Replace, or Move NCNs page.</description>
    </item>
    
    <item>
      <title>Update Firmware</title>
      <link>/docs-csm/en-13/operations/node_management/add_remove_replace_ncns/update_firmware/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:00 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/node_management/add_remove_replace_ncns/update_firmware/</guid>
      <description>Update Firmware Description Use FAS to update the firmware and set the BMC password.
Procedure See Update Firmware.
Proceed to the next step to Update NCN BIOS TPM State or return to the main Add, Remove, Replace, or Move NCNs page.</description>
    </item>
    
    <item>
      <title>Update NCN Bios Tpm State</title>
      <link>/docs-csm/en-13/operations/node_management/add_remove_replace_ncns/update_ncn_bios_tpm_state/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:00 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/node_management/add_remove_replace_ncns/update_ncn_bios_tpm_state/</guid>
      <description>Update NCN BIOS TPM State Description Enable Trusted Platform Module (TPM) in the BIOS on the new NCN if TPM is being used on the other NCNs. Skip this step if TPM is not being used in the system and proceed to the next step to Boot NCN and Configure.
Using TPM involves both enabling the hardware and configuring TPM. This document only details how to enable TPM on the hardware.</description>
    </item>
    
    <item>
      <title>Validate Added NCN</title>
      <link>/docs-csm/en-13/operations/node_management/add_remove_replace_ncns/validate_ncn/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:00 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/node_management/add_remove_replace_ncns/validate_ncn/</guid>
      <description>Validate Added NCN Only follow the steps in the section for the node type that was added:
 Master Node Worker Node Storage Node  Validate: Master Node Validate that the master node added successfully.
  Verify that the new node is in the cluster.
Run the following command from any master or worker node that is already in the cluster. It is helpful to run this command several times to watch for the newly rebuilt node to join the cluster.</description>
    </item>
    
    <item>
      <title>Validate Health</title>
      <link>/docs-csm/en-13/operations/node_management/add_remove_replace_ncns/validate_health/</link>
      <pubDate>Mon, 17 Jul 2023 03:18:00 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/node_management/add_remove_replace_ncns/validate_health/</guid>
      <description>Validate Health Description Validate that the system is healthy.
Procedure The following procedures can be run from any master or worker node.
  Collect data about the system management platform health.
/opt/cray/platform-utils/ncnHealthChecks.sh /opt/cray/platform-utils/ncnPostgresHealthChecks.sh NOTE If workers have been removed and the worker count is currently at two, the following failures can be ignored. A re-check will be needed once workers are added and the count returns to three or above.</description>
    </item>
    
    <item>
      <title>Add Switch Configuration For NCN</title>
      <link>/docs-csm/en-13/operations/node_management/add_remove_replace_ncns/add_switch_config/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:59 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/node_management/add_remove_replace_ncns/add_switch_config/</guid>
      <description>Add Switch Configuration for NCN Description Update the network switches for the NCN that is being added.
Procedure Update Networking to Add NCN Details coming soon.
Next Step Proceed to the next step to Add NCN Data or return to the main Add, Remove, Replace, or Move NCNs page.</description>
    </item>
    
    <item>
      <title>Allocate NCN Ip Addresses</title>
      <link>/docs-csm/en-13/operations/node_management/add_remove_replace_ncns/allocate_ncn_ip_addresses/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:59 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/node_management/add_remove_replace_ncns/allocate_ncn_ip_addresses/</guid>
      <description>Allocate NCN IP Addresses Description This procedure allocates IP addresses for an NCN being added to a system. The addresses are allocated on the applicable networks (HMN, NMN, MTL, CMN, etc.), and added to both the System Layout Service (SLS) and the Boot Script Service (BSS).
This procedure will perform and verify the following:
 If the NCN being added is one of the first three master, storage, or worker NCNs, then its IP address is expected to already be present and consistent between SLS and BSS.</description>
    </item>
    
    <item>
      <title>Boot NCN</title>
      <link>/docs-csm/en-13/operations/node_management/add_remove_replace_ncns/boot_ncn/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:59 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/node_management/add_remove_replace_ncns/boot_ncn/</guid>
      <description>Boot NCN Description Boot a master, worker, or storage non-compute node (NCN) that is to be added to the cluster.
Procedure Open and watch the console for the node being rebuilt   Log in to a second session in order to watch the console.
Open this link in a new tab or page: Log in to a Node Using ConMan
The first session will be needed to run the commands in the following Rebuild Node steps.</description>
    </item>
    
    <item>
      <title>Collect NCN Mac Addresses</title>
      <link>/docs-csm/en-13/operations/node_management/add_remove_replace_ncns/collect_ncn_mac_addresses/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:59 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/node_management/add_remove_replace_ncns/collect_ncn_mac_addresses/</guid>
      <description>Collect NCN MAC Addresses This procedure can be used to to collect MAC addresses from the NCNs along with their assigned interface names for use with the Add NCN Data procedure. A temporary MAC address collection iPXE bootscript is put into place on the system to discover the MAC addresses of the NCNs, along with their associated interface names (such as mgmt0).
WARNING This procedure will temporarily break the system&amp;rsquo;s ability to properly boot nodes in the system.</description>
    </item>
    
    <item>
      <title>Redeploy Services Impacted By Adding Or Permanently Removing Storage Nodes</title>
      <link>/docs-csm/en-13/operations/node_management/add_remove_replace_ncns/redeploy_services/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:59 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/node_management/add_remove_replace_ncns/redeploy_services/</guid>
      <description>Redeploy Services Impacted by Adding or Permanently Removing Storage Nodes This procedure redeploys S3 and sysmgmt-health services to add or remove storage node endpoints.
This procedure can be skipped if a worker or master node has been added. In that case, proceed to the next step to Validate NCN or return to the main Add, Remove, Replace, or Move NCNs page.
This procedure can be skipped if a worker or master node have been removed.</description>
    </item>
    
    <item>
      <title>Remove NCN Data</title>
      <link>/docs-csm/en-13/operations/node_management/add_remove_replace_ncns/remove_ncn_data/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:59 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/node_management/add_remove_replace_ncns/remove_ncn_data/</guid>
      <description>Remove NCN Data Description Remove NCN data to System Layout Service (SLS), Boot Script Service (BSS), and Hardware State Manager (HSM) as needed to remove an NCN.
Procedure IMPORTANT: The following procedures assume that you have set the variables from the prerequisites section.
  (ncn-mw#) Prepare for the procedure.
  Obtain an API token.
cd /usr/share/docs/csm/scripts/operations/node_management/Add_Remove_Replace_NCNs export TOKEN=$(curl -s -S -d grant_type=client_credentials -d client_id=admin-client \  -d client_secret=`kubectl get secrets admin-client-auth -o jsonpath=&amp;#39;{.</description>
    </item>
    
    <item>
      <title>Remove NCN From Role</title>
      <link>/docs-csm/en-13/operations/node_management/add_remove_replace_ncns/remove_ncn_from_role/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:59 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/node_management/add_remove_replace_ncns/remove_ncn_from_role/</guid>
      <description>Remove NCN from Role Description Remove a master, worker, or storage NCN from current roles. Select the procedure below based on the node type, complete the remaining steps to wipe the drives, and then power off the node.
Procedure IMPORTANT: The following procedures assume that the variables from the prerequisites section have been set.
 Remove roles  Master node Worker node Storage node   Disable disk boots Power off the node Next step  1.</description>
    </item>
    
    <item>
      <title>Access And Update Settings For Replacement NCNs</title>
      <link>/docs-csm/en-13/operations/node_management/access_and_update_the_settings_for_replacement_ncns/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:58 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/node_management/access_and_update_the_settings_for_replacement_ncns/</guid>
      <description>Access and Update Settings for Replacement NCNs When a new NCN is added to the system as a hardware replacement, it might use the default credentials. Contact HPE Cray service to learn what these are.
Use this procedure to verify that the default BMC credentials are set correctly after a replacement NCN is installed, cabled, and powered on.
All NCN BMCs must have credentials set up for ipmitool access.
Prerequisites A new non-compute node (NCN) has been added to the system as a hardware replacement.</description>
    </item>
    
    <item>
      <title>Add NCN Data</title>
      <link>/docs-csm/en-13/operations/node_management/add_remove_replace_ncns/add_ncn_data/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:58 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/node_management/add_remove_replace_ncns/add_ncn_data/</guid>
      <description>Add NCN Data Topics  Description Prerequisites Procedure  Collect information from the NCN  Saving/reloading   Collect MAC addresses from the NCN  Swapping/moving an NCN Adding a new NCN     Next step  Description Add NCN data to the System Layout Service (SLS), Boot Script Service (BSS), and Hardware State Manager (HSM) as needed, in order to add an NCN to the system.
Scenarios where this procedure is applicable:</description>
    </item>
    
    <item>
      <title>Alpha Framework To Add, Remove, Replace, Or Move NCNs</title>
      <link>/docs-csm/en-13/operations/node_management/add_remove_replace_ncns/add_remove_replace_ncns/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:58 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/node_management/add_remove_replace_ncns/add_remove_replace_ncns/</guid>
      <description>Alpha Framework to Add, Remove, Replace, or Move NCNs Add, remove, replace, or move non-compute nodes (NCNs). This applies to worker, storage, or master nodes. Use this procedure in the event that:
 Worker, storage, or master nodes are being replaced and the MAC address is changing. Worker or storage nodes are being added. Worker, storage, or master nodes are being moved to a different cabinet.  IMPORTANT: Always maintain at least two of the first three worker, storage, and master nodes when adding, removing, replacing, or moving NCNs.</description>
    </item>
    
    <item>
      <title>Metallb In BGP-mode</title>
      <link>/docs-csm/en-13/operations/network/metallb_bgp/metallb_in_bgp-mode/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:58 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/metallb_bgp/metallb_in_bgp-mode/</guid>
      <description>MetalLB in BGP-Mode MetalLB is a component in Kubernetes that manages access to LoadBalancer services from outside the Kubernetes cluster. There are LoadBalancer services on the Node Management Network (NMNLB), Hardware Management Network (HMNLB), Customer Management Network (CMN), Customer High-Speed Network (CHN), and Customer Access Network (CAN).
MetalLB can run in either Layer 2 mode or BGP mode for each address pool it manages. BGP mode is used for the NMNLB, HMNLB, and CAN.</description>
    </item>
    
    <item>
      <title>Troubleshoot BGP Not Accepting Routes From Metallb</title>
      <link>/docs-csm/en-13/operations/network/metallb_bgp/troubleshoot_bgp_not_accepting_routes_from_metallb/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:58 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/metallb_bgp/troubleshoot_bgp_not_accepting_routes_from_metallb/</guid>
      <description>Troubleshoot BGP not Accepting Routes from MetalLB Check the number of routes that the Border Gateway Protocol (BGP) Router is accepting in the peering session. This procedure is useful if Kubernetes LoadBalancer services in the NMNLB, HMNLB, CMN, CHN or CAN address pools are not accessible from outside the cluster.
Regain access to Kubernetes LoadBalancer services from outside the cluster.
Prerequisites This procedure requires administrative privileges.
Procedure   Log into the spine or aggregate switch.</description>
    </item>
    
    <item>
      <title>Troubleshoot Services Without An Allocated Ip Address</title>
      <link>/docs-csm/en-13/operations/network/metallb_bgp/troubleshoot_services_without_an_allocated_ip_address/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:58 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/metallb_bgp/troubleshoot_services_without_an_allocated_ip_address/</guid>
      <description>Troubleshoot Services without an Allocated IP Address Check if a given service has an IP address allocated for it if the Kubernetes LoadBalancer services in the NMNLB, HMNLB, CMN, CHN, or CAN address pools are not accessible from outside the cluster.
Regain access to Kubernetes LoadBalancer services from outside the cluster.
Prerequisites This procedure requires administrative privileges.
Procedure   Check the status of the services with the kubectl command to see the External-IP of the service.</description>
    </item>
    
    <item>
      <title>Check BGP Status And Reset Sessions</title>
      <link>/docs-csm/en-13/operations/network/metallb_bgp/check_bgp_status_and_reset_sessions/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:57 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/metallb_bgp/check_bgp_status_and_reset_sessions/</guid>
      <description>Check BGP Status and Reset Sessions Check the Border Gateway Protocol (BGP) status on the Aruba and Mellanox switches and verify that all sessions are in an Established state. If the state of any session in the table is Idle, then the BGP sessions must be reset.
 Prerequisites Procedure  Mellanox Aruba    Prerequisites This procedure requires administrative privileges.
Procedure The following procedures may not resolve the problem after just one attempt.</description>
    </item>
    
    <item>
      <title>Metallb Configuration</title>
      <link>/docs-csm/en-13/operations/network/metallb_bgp/metallb_configuration/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:57 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/metallb_bgp/metallb_configuration/</guid>
      <description>MetalLB Configuration MetalLB provides a more robust configuration for the Node Management Network (NMNLB), Hardware Management Network (HMNLB), Customer Management Network (CMN), Customer High-Speed Network (CHN), and Customer Access Network (CAN). This configuration is generated from the csi config init input values.
MetalLB Peer Configuration The content for metallb_bgp_peers is generated by the csi config init command. In addition to the MetalLB configuration, there is configuration needed on the spine switches to set up the BGP router on these switches.</description>
    </item>
    
    <item>
      <title>Upgrade Switches From 1.2 To 1.3 Preconfig</title>
      <link>/docs-csm/en-13/operations/network/management_network/upgrade/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:57 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/management_network/upgrade/</guid>
      <description>Upgrade Switches From 1.2 to 1.3 Preconfig Use the following procedure to upgrade switches from 1.2 to 1.3 preconfig.
To check if the management network is using generated switch configurations, log onto a management switch and check for a banner with a CANU version. This indicates the switch configuration has been generated.
############################################################################### # CSM version: 1.2 # CANU version: 1.6.13 ############################################################################### To upgrade from CSM 1.3 preconfig (switch configurations generated) to CSM 1.</description>
    </item>
    
    <item>
      <title>Validate Cabling</title>
      <link>/docs-csm/en-13/operations/network/management_network/validate_cabling/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:57 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/management_network/validate_cabling/</guid>
      <description>Validate Cabling  Warning: If this step is completed when NCNs are offline or shutdown, the information compared here will not match the actual connections. Therefore, this step should be re-run again once the whole system is up.
   To validate the cabling you can run command similar to below:
canu validate shcd-cabling --ips 10.252.0.2,10.252.0.3 --tabs 40G_10G --corners J12,T36 --shcd ./SHCD.xlsx  NOTE Modify the command to use the correct SHCD file and correct --tabs, --corners, and IP addresses.</description>
    </item>
    
    <item>
      <title>Validate Switch Configurations</title>
      <link>/docs-csm/en-13/operations/network/management_network/validate_switch_configs/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:57 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/management_network/validate_switch_configs/</guid>
      <description>Validate Switch Configurations Prerequisites  SSH access to the switches or the running configuration file. Generated switch configurations.  Generate Switch Configurations   CANU installed with version 1.1.11 or greater.  Run canu --version to see version. If doing a CSM install or upgrade, a CANU RPM is located in the release tarball. For more information, see this procedure: Update CANU From CSM Tarball    Compare CSM 1.</description>
    </item>
    
    <item>
      <title>Validate The Shcd &amp; Create Json Topology File</title>
      <link>/docs-csm/en-13/operations/network/management_network/validate_shcd/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:57 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/management_network/validate_shcd/</guid>
      <description>Validate the SHCD &amp;amp; Create JSON topology file Use the CSM Automated Network Utility (CANU) to validate the SHCD. SHCD validation is required to ensure Plan-of-Record network configurations are generated. This is an iterative process to create a model of the entire network topology connection by connection. Once the validation is complete, a machine readable JSON file should be produced.
Topics  Prerequisites Validation steps Retrieve JSON paddle file Under the hood  Check warnings Check SHCD port usage   Logging and updates  Prerequisites  Up-to-date SHCD.</description>
    </item>
    
    <item>
      <title>Wipe Management Switch Configuration</title>
      <link>/docs-csm/en-13/operations/network/management_network/wipe_mgmt_switches/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:57 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/management_network/wipe_mgmt_switches/</guid>
      <description>Wipe Management Switch Configuration This procedure describes how to wipe Aruba, Dell, and Mellanox switch configurations.
Prerequisites Out-of-band access to the switches (console)
Aruba   (sw-spine-001#) Create a checkpoint before erasing the switch configuration.
More information related to backing up configuration can be found on the Configuration Management procedure.
copy running-config checkpoint CSM1_0   (sw-spine-001#)Verify the checkpoint was created.
show checkpoint Example output:
NAME TYPE WRITER DATE(YYYY/MM/DD) IMAGE VERSION CSM1_0 latest User 2022-01-27T18:52:31Z GL.</description>
    </item>
    
    <item>
      <title>Network Tests</title>
      <link>/docs-csm/en-13/operations/network/management_network/network_tests/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:56 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/management_network/network_tests/</guid>
      <description>Network Tests The CSM Automatic Network Utility (CANU) has the ability to run tests against the management network.
If doing a CSM install or upgrade, a CANU RPM is located in the release tarball. For more information, refer to the Update CANU From CSM Tarball procedure.
The switch inventory is dynamically created from either a System Layout Service (SLS) file --sls-file, or it will automatically query the SLS API if an SLS file is not specified.</description>
    </item>
    
    <item>
      <title>Prometheus Snmp Exporter</title>
      <link>/docs-csm/en-13/operations/network/management_network/snmp_exporter_configs/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:56 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/management_network/snmp_exporter_configs/</guid>
      <description>Prometheus SNMP Exporter The Prometheus SNMP Exporter is deployed by the cray-sysmgmt-health chart to the sysmgmt-health namespace as part of the Cray System Management (CSM) release.
Configuration In order to provide data to the Grafana SNMP dashboards, the SNMP Exporter must be configured with a list of management network switches to scrape metrics from.
 NOTE All variables used within this page depend on the /etc/environment setup done in Pre-installation.</description>
    </item>
    
    <item>
      <title>Reinstall</title>
      <link>/docs-csm/en-13/operations/network/management_network/reinstall/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:56 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/management_network/reinstall/</guid>
      <description>Reinstall Reinstall the same CSM version.
Before continuing with install, make sure that CANU is running the most current version:
Install/Upgrade CANU
 CAUTION: All of these steps should be done using an out-of-band connection. This process is disruptive and will require downtime.
 Procedure   If the switches being reinstalled are already in the right CSM version, no configuration changes should be required.
  Check the differences between generated configurations and the configurations on the system.</description>
    </item>
    
    <item>
      <title>Replace Switch</title>
      <link>/docs-csm/en-13/operations/network/management_network/replace_switch/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:56 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/management_network/replace_switch/</guid>
      <description>Replace Switch  CAUTION: Do not plug in a switch that is not configured. This can cause unpredictable behavior and network outages.
 Prerequisites  Out-of-band access to the switches (console). GA generated switch configuration or backed-up switch configuration exists.  Generate Switch Configurations Configuration Management    Procedure The following steps are required to replace a switch.
  Update firmware on new switch.
See Update Management Network Firmware.</description>
    </item>
    
    <item>
      <title>Save A Configuration</title>
      <link>/docs-csm/en-13/operations/network/management_network/saving_config/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:56 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/management_network/saving_config/</guid>
      <description>Save a Configuration Write Memory To keep track of what configuration version is running on the switch, create a new configuration file using the CSN version and the CANU version from the MOTD banner from the running config.
Mellanox   Get the CSM and CANU versions from the MOTD banner.
sw-spine-001 [mlag-domain: master] (config) # show banner Example output:
Banners: Message of the Day (MOTD): ############################################################################### # CSM version: 1.</description>
    </item>
    
    <item>
      <title>Web User Interface (webui)</title>
      <link>/docs-csm/en-13/operations/network/management_network/mellanox/web-ui/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:56 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/management_network/mellanox/web-ui/</guid>
      <description>Web user interface (WebUI) A web-based management user interface provides a visual representation of a subset of the current switch configuration and states. The Web-UI allows for easy access from modern browsers to modify some aspects of the configuration.
Relevant Configuration
Enable the WebUI
switch(config)# web enable Configure REST API
switch(config)# web enable http|https Show Commands to Validate Functionality
show web Expected Results
 Step 1: You can connect the management interface to a private network Step 2: You can enable web-management Step 3: You can connect to the IP address from a browser login to the management menu  Back to Index</description>
    </item>
    
    <item>
      <title>Verify Route To Tftp</title>
      <link>/docs-csm/en-13/operations/network/management_network/mellanox/verify_route_to_tftp/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:55 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/management_network/mellanox/verify_route_to_tftp/</guid>
      <description>Verify route to TFTP On BOTH Aruba switches we need a single route to the TFTP server 10.92.100.60 (your configuration may differ).
This is needed because there are issues with Aruba ECMP hashing and TFTP traffic.
show ip route 10.92.100.60 Displaying ipv4 routes selected for forwarding &#39;[x/y]&#39; denotes [distance/metric] 10.92.100.60/32, vrf default, tag 0 via 10.252.1.9, [70/0], bgp  This route can be a static route or a BGP route that is pinned to a single worker.</description>
    </item>
    
    <item>
      <title>Verify The DHCP Traffic On The Workers</title>
      <link>/docs-csm/en-13/operations/network/management_network/mellanox/verify_dhcp_traffic_on_workers/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:55 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/management_network/mellanox/verify_dhcp_traffic_on_workers/</guid>
      <description>Verify the DHCP traffic on the workers Example issue: Source address of the DHCP Offer is the MetalLB address of KEA &amp;ldquo;10.92.100.222&amp;rdquo;.
The source address of the DHCP Reply/Offer NEEDS to be the address of the vlan interface on the Worker.
Here is how to look at DHCP traffic on the workers:
tcpdump -envli bond0 port 67 or 68 You are looking for the source IP address of the DHCP Reply/Offer, this is an example of working offer:</description>
    </item>
    
    <item>
      <title>Very Large (exascale)</title>
      <link>/docs-csm/en-13/operations/network/management_network/mellanox/very_large/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:55 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/management_network/mellanox/very_large/</guid>
      <description>Very Large (Exascale) Back to index.</description>
    </item>
    
    <item>
      <title>Virtual Local Access Networks (vlans)</title>
      <link>/docs-csm/en-13/operations/network/management_network/mellanox/vlan/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:55 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/management_network/mellanox/vlan/</guid>
      <description>Virtual local access networks (VLANs) VLANs allow for the logical grouping of switch interfaces, enabling communication as if all connected devices were on the same isolated network.
Relevant Configuration
Create VLAN
switch(config)# vlan &amp;lt;VLAN&amp;gt; Configure an interface to associate it with a VLAN
switch (config) # interface ethernet 1/22 switch (config interface ethernet 1/22) # From within the interface context, configure the interface mode to Access.
switch (config interface ethernet 1/22) # switchport mode access From within the interface context, configure the Access VLAN membership.</description>
    </item>
    
    <item>
      <title>Vlan Interface</title>
      <link>/docs-csm/en-13/operations/network/management_network/mellanox/vlan_interface/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:55 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/management_network/mellanox/vlan_interface/</guid>
      <description>VLAN interface The switch also supports classic L3 VLAN interfaces.
Relevant Configuration
Configure the VLAN
switch (config) # vlan 6 switch (config vlan 6) # Create and enable the VLAN interface, and assign it an IP address
switch(config vlan 6)# ip address 10.1.0.2/16 Show Commands to Validate Functionality
show vlan Expected Results
 Step 1: You can configure the VLAN Step 2: You can enable the interface and associate it with the VLAN Step 3: You can create an IP-enabled VLAN interface, and it is up Step 4: You validate the configuration is correct Step 5: You can ping from the switch to the client and from the client to the switch  Back to Index</description>
    </item>
    
    <item>
      <title>Vlan Trunking 802.1q</title>
      <link>/docs-csm/en-13/operations/network/management_network/mellanox/vlan_trunking_8021q/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:55 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/management_network/mellanox/vlan_trunking_8021q/</guid>
      <description>VLAN trunking 802.1Q A trunk port carries packets on one or more VLANs specified. Packet that ingress on a trunk port are in the VLAN specified in its 802.1Q header, or native VLAN if the packet has no 802.1Q header. A packet that egresses through a trunk port will have an 802.1Q header if it has a nonzero VLAN ID. Any packet that ingresses on a trunk port tagged with a VLAN that the port does not trunk is dropped.</description>
    </item>
    
    <item>
      <title>Performing Upgrade On Mellanox Switches</title>
      <link>/docs-csm/en-13/operations/network/management_network/mellanox/upgrade/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:54 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/management_network/mellanox/upgrade/</guid>
      <description>Performing Upgrade On Mellanox Switches Supported Software Upgrades
   Target Version Verified Starting Versions     3.9.3xxx 3.9.2xxx, 3.9.1xxx   3.9.2xxx 3.9.1xxx, 3.9.0xxx   3.9.1xxx 3.9.0xxx, 3.8.2xxx   3.9.0xxx 3.8.2xxx, 3.8.1xxx    Repeated the following procedure for each &amp;ldquo;upgrade hop&amp;rdquo;.
Upgrading the Switch Using the CLI The Switch OS software packages include the switch firmware and the CPU software for the specific switch board CPU (x86).</description>
    </item>
    
    <item>
      <title>System Images</title>
      <link>/docs-csm/en-13/operations/network/management_network/mellanox/system_images/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:54 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/management_network/mellanox/system_images/</guid>
      <description>System images Mellanox switches can hold two firmware images. These images, once uploaded, are called the Running and Image available for install.
Relevant Configuration
Copy an image from a local server using sftp
switch (config)#image delete XXX // --&amp;gt; delete old images, if exist switch (config)#image fetch scp://root:password@server/path-to-image/image-X86_64-3.4.2002.img switch (config)#image install image-X86_64-3.4.2002.img Boot the switch into the new firmware
switch (config)#image boot next switch (config)#configuration write switch (config)#reload Show Commands to Validate Functionality</description>
    </item>
    
    <item>
      <title>Test Tftp Traffic (aruba Only)</title>
      <link>/docs-csm/en-13/operations/network/management_network/mellanox/test_tftp_traffic/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:54 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/management_network/mellanox/test_tftp_traffic/</guid>
      <description>Test TFTP traffic (Aruba Only)  You can test the TFTP traffic by trying to download the ipxe.efi binary. Log into the leaf switch and try to download the iPXE binary. This requires that the leaf switch can talk to the TFTP server &amp;ldquo;10.92.100.60&amp;rdquo;  start-shell sw-leaf-001:~$ sudo su sw-leaf-001:/home/tftp 10.92.100.60 tftp&amp;gt; get ipxe.efi Received 1007200 bytes in 2.2 seconds tftp&amp;gt; get ipxe.efi Received 1007200 bytes in 2.2 seconds tftp&amp;gt; get ipxe.</description>
    </item>
    
    <item>
      <title>Typical Configuration Of Mlag Between Switches</title>
      <link>/docs-csm/en-13/operations/network/management_network/mellanox/typical_mlag_switch_configuration/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:54 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/management_network/mellanox/typical_mlag_switch_configuration/</guid>
      <description>Typical configuration of MLAG between switches The intent here is to show case very basic mlag configuration between two spine switches.
   mlag-vip cray-mlag-domain ip 192.168.255.242 /29 force no mlag shutdown mlag system-mac 00:00:5E:00:01:01 interface port-channel 100 ipl 1 interface vlan 4000 ipl 1 peer-address 192.168.255.253     mlag-vip cray-mlag-domain ip 192.168.255.242 /29 force no mlag shutdown mlag system-mac 00:00:5E:00:01:5D interface port-channel 100 ipl 1 interface vlan 4000 ipl 1 peer-address 192.</description>
    </item>
    
    <item>
      <title>Typical Configuration Of Mlag Link Connecting To NCN</title>
      <link>/docs-csm/en-13/operations/network/management_network/mellanox/typical_mlag_port_configuration/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:54 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/management_network/mellanox/typical_mlag_port_configuration/</guid>
      <description>Typical configuration of MLAG link connecting to NCN The intent here is to show case very basic MLAG link configuration and your configuration may differ. This is what defines the LAG to be able to peer both to Spine-01 and Spine-02.
   interface mlag-port-channel 1 interface mlag-port-channel 1 mtu 9216 force interface ethernet 1/1 mlag-channel-group 1 mode active interface mlag-port-channel 1 switchport mode hybrid interface mlag-port-channel 1 interface mlag-port-channel 1-11 lacp-individual enable force interface mlag-port-channel 1 switchport hybrid allowed-vlan add 2 interface mlag-port-channel 1 switchport hybrid allowed-vlan add 4 interface mlag-port-channel 1 switchport hybrid allowed-vlan add 7 interface mlag-port-channel 1 switchport hybrid allowed-vlan add 10     interface mlag-port-channel 1 interface mlag-port-channel 1 mtu 9216 force interface ethernet 1/1 mlag-channel-group 1 mode active interface mlag-port-channel 1 switchport mode hybrid interface mlag-port-channel 1 interface mlag-port-channel 1-11 lacp-individual enable force interface mlag-port-channel 1 switchport hybrid allowed-vlan add 2 interface mlag-port-channel 1 switchport hybrid allowed-vlan add 4 interface mlag-port-channel 1 switchport hybrid allowed-vlan add 7 interface mlag-port-channel 1 switchport hybrid allowed-vlan add 10   Back to Index</description>
    </item>
    
    <item>
      <title>Verify BGP</title>
      <link>/docs-csm/en-13/operations/network/management_network/mellanox/verify_bgp/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:54 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/management_network/mellanox/verify_bgp/</guid>
      <description>Verify BGP Verify the BGP neighbors are in the established state on BOTH the switches.
How to check Aruba BGP status:
show bgp ipv4 u s VRF : default BGP Summary ----------- Local AS : 65533 BGP Router Identifier : 10.252.0.3 Peers : 4 Log Neighbor Changes : No Cfg. Hold Time : 180 Cfg. Keep Alive : 60 Confederation Id : 0 Neighbor Remote-AS MsgRcvd MsgSent Up/Down Time State AdminStatus 10.</description>
    </item>
    
    <item>
      <title>Verify The Switches Are Forwarding DHCP Traffic</title>
      <link>/docs-csm/en-13/operations/network/management_network/mellanox/verify-switches_are_forwarding_dhcp_traffic/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:54 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/management_network/mellanox/verify-switches_are_forwarding_dhcp_traffic/</guid>
      <description>Verify the switches are forwarding DHCP traffic If you made it this far and still cannot pxe boot, you may have run into the IP-Helper breaking on the switch.
Back to Index</description>
    </item>
    
    <item>
      <title>Confirm The Status Of The Cray-DHCP-kea Pods/services</title>
      <link>/docs-csm/en-13/operations/network/management_network/mellanox/status_of_cray-dhcp-kea_pods/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:53 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/management_network/mellanox/status_of_cray-dhcp-kea_pods/</guid>
      <description>Confirm the status of the cray-dhcp-kea pods/services Check if the kea DHCP services are running. On ncn-w001 or a worker/manager with kubectl, run:
kubectl get -n services pods | grep kea You should see the following services as output:
kubectl get services -n services | grep kea cray-dhcp-kea-api Cluster IP 10.31.247.201 &amp;lt;none&amp;gt; 8000/TCP 3h36m cray-dhcp-kea-tcp-hmn LoadBalancer 10.25.109.178 10.94.100.222 67:30833/TCP 3h36m cray-dhcp-kea-tcp-nmn LoadBalancer 10.21.240.208 10.92.100.222 67:31915/TCP 3h36m cray-dhcp-kea-udp-hmn LoadBalancer 10.20.37.60 10.94.100.222 67:30357/UDP 3h36m cray-dhcp-kea-udp-nmn LoadBalancer 10.</description>
    </item>
    
    <item>
      <title>Mac Address Table</title>
      <link>/docs-csm/en-13/operations/network/management_network/mellanox/static_mac/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:53 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/management_network/mellanox/static_mac/</guid>
      <description>Mac address Table You can configure static MAC addresses for unicast traffic. This feature improves security and reduces unknown unicast flooding.
To configure Unicast Static MAC address:
Switch (config) # mac-address-table static unicast &amp;lt;destination mac address&amp;gt; vlan &amp;lt;vlan identifier(1-4094)&amp;gt; interface ethernet &amp;lt;slot&amp;gt;/&amp;lt;port&amp;gt; For example:
switch (config) # mac-address-table static 00:11:22:33:44:55 vlan 1 interface ethernet 1/1 Back to Index</description>
    </item>
    
    <item>
      <title>Secure Shell (SSH)</title>
      <link>/docs-csm/en-13/operations/network/management_network/mellanox/ssh/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:53 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/management_network/mellanox/ssh/</guid>
      <description>Secure shell (SSH) SSH server enables an SSH client to make a secure and encrypted connection to a switch. Currently, switch supports SSH version 2.0 only. The user authentication mechanisms supported for SSH are public key authentication and password authentication (RADIUS, TACACS+ or locally stored password). Secure File Transfer Protocol (SFTP) provides file transfer. SSH Server and sftp-client via the copy command are supported for managing the router.
Relevant Configuration</description>
    </item>
    
    <item>
      <title>Spine-leaf Architecture</title>
      <link>/docs-csm/en-13/operations/network/management_network/mellanox/spine_leaf_architecture2/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:53 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/management_network/mellanox/spine_leaf_architecture2/</guid>
      <description>Spine-leaf architecture How does a spine-leaf architecture differ from traditional network designs? Traditionally, data center networks were based on a three-tier model:
 Access switches connect to servers Aggregation or distribution switches provide redundant connections to access switches Core switches provide fast transport between aggregation switches, typically connected in a redundant pair for high availability  At the most basic level, a spine-leaf architecture collapses one of these tiers, as depicted in these diagrams.</description>
    </item>
    
    <item>
      <title>Static Routing</title>
      <link>/docs-csm/en-13/operations/network/management_network/mellanox/static_routing/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:53 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/management_network/mellanox/static_routing/</guid>
      <description>Static routing &amp;ldquo;Static routing is manually performed by the network administrator. The administrator is responsible for discovering and propagating routes through the network. These definitions are manually programmed in every routing device in the environment. After a device has been configured, it simply forwards packets out the predetermined ports. There is no communication between routers regarding the current topology of the network.&amp;rdquo; –IBM Redbook, TCP/IP
Relevant Configuration
switch(config)# ip route vrf default 0.</description>
    </item>
    
    <item>
      <title>Why Are Spine-leaf Architectures Becoming More Popular?</title>
      <link>/docs-csm/en-13/operations/network/management_network/mellanox/spine_leaf_architecture3/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:53 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/management_network/mellanox/spine_leaf_architecture3/</guid>
      <description>Why are spine-leaf architectures becoming more popular? Given the prevalence of cloud and containerized infrastructure in modern data centers, east-west traffic continues to increase. East-west traffic moves laterally, from server to server. This shift is primarily explained by modern applications having components that are distributed across more servers or VMs.
With east-west traffic, having low-latency, optimized traffic flows is imperative for performance, especially for time-sensitive or data-intensive applications. A spine-leaf architecture aids this by ensuring traffic is always the same number of hops from its next destination, so latency is lower and predictable.</description>
    </item>
    
    <item>
      <title>Routed Interfaces</title>
      <link>/docs-csm/en-13/operations/network/management_network/mellanox/routed_interface/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:52 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/management_network/mellanox/routed_interface/</guid>
      <description>Routed interfaces By default Mellanox interfaces are set as &amp;ldquo;switchports&amp;rdquo; which is to allow L2 communication. To change to routed only port, you have to disable L2 functionality.
Relevant Configuration
Disable L2 functionality
Switch (config) # interface ethernet ¼ Switch (config-int) no switchport force Give an interface an IP address
switch (config) # interface ethernet 1/14 ip address 192.168.75.1/31 primary Show Commands to Validate Functionality
show ethernet interface IFACE Expected Results</description>
    </item>
    
    <item>
      <title>Scenario A Network Connection Via Management Network</title>
      <link>/docs-csm/en-13/operations/network/management_network/mellanox/scenario-a/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:52 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/management_network/mellanox/scenario-a/</guid>
      <description>Scenario A: network connection via management network Description The example here covers outside connections achieved via management network.
Summary  Create a new VRF Move interfaces to the new VRF Create a new BGP process for the new VRF Setup the edge router Configure MetalLB Verification step for BGP routes Configure default route for workers Verification of external communication  Topology Back to Index</description>
    </item>
    
    <item>
      <title>Scenario B Network Connection Via High Speed Network</title>
      <link>/docs-csm/en-13/operations/network/management_network/mellanox/scenario-b/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:52 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/management_network/mellanox/scenario-b/</guid>
      <description>Scenario B: network connection via high speed network Description The example here covers outside connections achieved via highspeed network.
Summary  Create a new VRF Move interfaces to the new VRF Create a new BGP process for the new VRF Setup the edge router Configure MetalLB Verification step for BGP routes Configure default route for workers Verification of external communication  Topology Back to Index</description>
    </item>
    
    <item>
      <title>Small</title>
      <link>/docs-csm/en-13/operations/network/management_network/mellanox/small/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:52 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/management_network/mellanox/small/</guid>
      <description>Small Back to index.</description>
    </item>
    
    <item>
      <title>Snmpv2c Community</title>
      <link>/docs-csm/en-13/operations/network/management_network/mellanox/snmp_community/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:52 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/management_network/mellanox/snmp_community/</guid>
      <description>SNMPv2c community The switch supports SNMPv2c community-based security for Read-Only access.
Relevant Configuration
Configure an SNMPv2c community name
Enable SNMP
switch(config)# snmp-server community private rw Configure a SNMPv2c trap receiver host
switch(config)# snmp-server host IP-ADDR &amp;lt;trap|inform&amp;gt; version v2c [community NAME] Show Commands to Validate Functionality
show snmp Expected Results
 Step 1: You can configure the community name Step 2: You can bind the SNMP server to the default VRF Step 3: You can connect from the workstation using the community name  Back to Index</description>
    </item>
    
    <item>
      <title>Snmpv3 Users</title>
      <link>/docs-csm/en-13/operations/network/management_network/mellanox/snmpv3_users/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:52 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/management_network/mellanox/snmpv3_users/</guid>
      <description>SNMPv3 users SNMPv3 supports cryptographic security by a combination of authenticating and encrypting the SNMP protocol packets over the network. Read-Only access is currently supported. The admin user can add or remove SNMPv3 users.
Relevant Configuration
Configure a new SNMPv3 user (Minimum 8 characters for passwords)
switch(config)# snmp-server user admin v3 enable Show Commands to Validate Functionality
show snmp users Back to Index</description>
    </item>
    
    <item>
      <title>Spine-leaf Architecture</title>
      <link>/docs-csm/en-13/operations/network/management_network/mellanox/spine_leaf_architecture/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:52 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/management_network/mellanox/spine_leaf_architecture/</guid>
      <description>Spine-leaf architecture Description
The network design used in majority of our supercomputer installations is spine leaf architecture. In more sizeable systems we also utilize super-spine to accommodate number of spines that connect the network to provide additional HA capabilities.
What is Spine-Leaf Architecture? A spine-leaf architecture is data center network topology that consists of two switching layers—a spine and leaf. The leaf layer consists of access switches that aggregate traffic from servers and connect directly into the spine or network core.</description>
    </item>
    
    <item>
      <title>How To Connect Management Network To Your Campus Network</title>
      <link>/docs-csm/en-13/operations/network/management_network/mellanox/requirements_and_optional_configuration/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:51 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/management_network/mellanox/requirements_and_optional_configuration/</guid>
      <description>How to connect management network to your campus network In the event that you want to connect the Supercomputer directly to your campus network. In this guide we will go over the two most typical ways of accomplishing this. Further explained in Scenario A and B that will cover the examples of adding connections through management network or highspeed network.
Requirements and optional configuration
 System needs to be completely installed and running.</description>
    </item>
    
    <item>
      <title>Open Shortest Path First (ospf) V2</title>
      <link>/docs-csm/en-13/operations/network/management_network/mellanox/ospfv2/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:51 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/management_network/mellanox/ospfv2/</guid>
      <description>Open shortest path first (OSPF) v2 &amp;ldquo;OSPF is a link-state based routing protocol. It is designed to be run internal to a single Autonomous System. Each OSPF router maintains an identical database describing the Autonomous System&amp;rsquo;s topology. From this database, a routing table is calculated by constructing a shortest-path tree. OSPF recalculates routes quickly in the face of topological changes, utilizing a minimum of routing protocol traffic. OSPF provides support for equal-cost multipath.</description>
    </item>
    
    <item>
      <title>Physical Interfaces</title>
      <link>/docs-csm/en-13/operations/network/management_network/mellanox/physical_interfaces/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:51 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/management_network/mellanox/physical_interfaces/</guid>
      <description>Physical interfaces Interfaces in Mellanox are enabled by default.
Relevant Configuration
Enter interface context
switch (config) # interface ethernet 1/1 Show Commands to Validate Functionality
show interfaces ethernet 1/1 Expected Results
 Step 1: You can enter the interface context for the port Step 2: You can establish a link with a partner Step 3: You can pass traffic as expected  Back to Index</description>
    </item>
    
    <item>
      <title>Pim-sm Bootstrap Router (bsr) And Rendezvous-point (rp)</title>
      <link>/docs-csm/en-13/operations/network/management_network/mellanox/pim/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:51 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/management_network/mellanox/pim/</guid>
      <description>PIM-SM bootstrap router (BSR) and rendezvous-point (RP) &amp;ldquo;Every PIM multicast group needs to be associated with the IP address of a Rendezvous Point (RP) [&amp;hellip;] For all senders to reach all receivers, it is crucial that all routers in the domain use the same mappings of group addresses to RP addresses. [&amp;hellip;] The BSR mechanism provides a way in which viable group-to-RP mappings can be created and rapidly distributed to all the PIM routers in a domain.</description>
    </item>
    
    <item>
      <title>Rebooting NCN And PXE Fails</title>
      <link>/docs-csm/en-13/operations/network/management_network/mellanox/reboot_pxe_fail/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:51 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/management_network/mellanox/reboot_pxe_fail/</guid>
      <description>Rebooting NCN and PXE fails Common Error messages.
2021-04-19 23:27:09 PXE-E18: Server response timeout. 2021-02-02 17:06:13 PXE-E99: Unexpected network error. Verify the ip helper-address on VLAN 1 on the switches.
This is the same configuration as above &amp;ldquo;Aruba Configuration&amp;rdquo;.
Verify DHCP packets can be forwarded from the workers to the MTL network (VLAN1)
 If the Worker nodes cannot reach the metal network DHCP will fail. ALL WORKERS need to be able to reach the MTL network!</description>
    </item>
    
    <item>
      <title>Remote Logging</title>
      <link>/docs-csm/en-13/operations/network/management_network/mellanox/remote_logging/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:51 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/management_network/mellanox/remote_logging/</guid>
      <description>Remote logging &amp;ldquo;In its most simplistic terms, the syslog protocol provides a transport to allow a machine to send event notification messages across IP networks to event message collectors - also known as syslog servers.&amp;rdquo; –rfc3164
Note: the default facility is 3(DAEMON)
Relevant Configuration
Configure logging
switch(config)# logging &amp;lt;syslog-ip-address&amp;gt; [trap {&amp;lt;log-level&amp;gt; | override class &amp;lt;class&amp;gt; priority &amp;lt;log-level&amp;gt;}] Expected Results
 Step 1: You can configure remote logging Step 2: You can see the log files from the switch on the remote server  Back to Index</description>
    </item>
    
    <item>
      <title>Native Vlan</title>
      <link>/docs-csm/en-13/operations/network/management_network/mellanox/native_vlan/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:50 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/management_network/mellanox/native_vlan/</guid>
      <description>Native VLAN Untagged ingress packets are destined to the native VLAN. An interface can be configured in one of 2 native modes - Native-Untagged or Native-Tagged.
A native-untagged port accepts any untagged or tagged (with native VLAN ID) traffic on ingress.
Packets that egress on a native-untagged port in the native VLAN will not have an 802.1Q header. A native-tagged port accepts only tagged traffic (with native VLAN ID) on ingress.</description>
    </item>
    
    <item>
      <title>NCNs On Install</title>
      <link>/docs-csm/en-13/operations/network/management_network/mellanox/ncns_on_install/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:50 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/management_network/mellanox/ncns_on_install/</guid>
      <description>NCNs on Install Verify the DNSMASQ configuration file matches what is configured on the switches.
Example DNSMASQ Configuration File Here is a DNSMASQ configuration file for the Metal network (VLAN1). As you can see the router is 10.1.0.1, this has to match what the IP address is on the switches doing the routing for the MTL network.
This is most commonly on the spines.
This configuration is commonly missed on the CSI input file.</description>
    </item>
    
    <item>
      <title>Network Time Protocol (NTP) Client</title>
      <link>/docs-csm/en-13/operations/network/management_network/mellanox/ntp/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:50 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/management_network/mellanox/ntp/</guid>
      <description>Network Time Protocol (NTP) Client Summary of NTP from RFC-1305 Network Time Protocol (Version 3):
 NTP is used to synchronize timekeeping among a set of distributed time servers and clients &amp;hellip; It provides the protocol mechanisms to synchronize time in principle to precisions in the order of nanoseconds while preserving a non-ambiguous date well into the next century.
 The Network Time Protocol (NTP) client is essential for syncing time on various clients in the system.</description>
    </item>
    
    <item>
      <title>Network Traffic Pattern Inside Of The System</title>
      <link>/docs-csm/en-13/operations/network/management_network/mellanox/network_traffic_pattern/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:50 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/management_network/mellanox/network_traffic_pattern/</guid>
      <description>Network traffic pattern inside of the system Internal Networks:
 Node Management Network (NMN) - Provides the internal control plane for systems management and jobs control. Hardware Management Network (HMN) - Provides internal access to system baseboard management controllers (BMC/iLO) and other lower-level hardware access.  External and Edge Networks:
 Customer Management Network (CMN) - Provides customer access from the Site to the System for administrators. Customer Access Network (CAN) or Customer High Speed Network (CHN) provide:  Customer access from the site to the System for job control and jobs data movement.</description>
    </item>
    
    <item>
      <title>Network Types – Naming And Segment Function</title>
      <link>/docs-csm/en-13/operations/network/management_network/mellanox/network_naming_function/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:50 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/management_network/mellanox/network_naming_function/</guid>
      <description>Network types – Naming and segment Function Description
In below you can find the overview of the different networks services defined inside of our spine and leaf architecture.
   *********** Administration: Hardware Administration: Cloud/Job Customer: Jobs Customer: Administration Storage     Full name Hardware Management Network Node Management Network Customer Access Network Customer Management Network Storage User Network   Short name / acronym HMN NMN CAN CMN SUN    Back to Index</description>
    </item>
    
    <item>
      <title>Tcpdump</title>
      <link>/docs-csm/en-13/operations/network/management_network/mellanox/ncn_tcpdump/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:50 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/management_network/mellanox/ncn_tcpdump/</guid>
      <description>TCPDUMP If your host is not getting an IP address you can run a packet capture to see if DHCP traffic is being transmitted.
On ncn-w001 or a worker/manager with kubectl, run:
tcpdump -w dhcp.pcap -envli bond0.nmn0 port 67 or port 68 This will make a .pcap file named dhcp in your current directory. It will collect all DHCP traffic on the port you specify, in this example we are looking for DHCP traffic on interface bond0.</description>
    </item>
    
    <item>
      <title>Example Of How To Configure Scenario A Or B</title>
      <link>/docs-csm/en-13/operations/network/management_network/mellanox/management_network_configuration_example/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:49 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/management_network/mellanox/management_network_configuration_example/</guid>
      <description>Example of how to configure Scenario A or B Create the CAN VRF
Aruba
 switch#config
switch(config)#vrf CAN
 Move interfaces into CAN VRF
 If you have existing CAN interface configuration it will be deleted once you move the interface into the new VRF. You will have to re-apply it. NOTE: These are example configs only, most implementations of Bi-CAN will be different.  Aruba
Aruba Primary Config
interface vlan 7 vsx-sync active-gateways vrf attach CAN description CAN ip mtu 9198 ip address 128.</description>
    </item>
    
    <item>
      <title>Medium</title>
      <link>/docs-csm/en-13/operations/network/management_network/mellanox/medium/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:49 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/management_network/mellanox/medium/</guid>
      <description>Medium Back to index.</description>
    </item>
    
    <item>
      <title>Mlag</title>
      <link>/docs-csm/en-13/operations/network/management_network/mellanox/mlag_switch/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:49 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/management_network/mellanox/mlag_switch/</guid>
      <description>MLAG A link aggregation group (LAG) is used for extending the bandwidth from a single link to multiple links and provide redundancy in case of link failure. Extending the implementation of the LAG to more than a single device provides yet another level of redundancy that extends from the link level to the node level. This extrapolation of the LAG from single to multiple switches is referred to as multi-chassis link aggregation (MLAG).</description>
    </item>
    
    <item>
      <title>Mlag (multi-chassis Lag)</title>
      <link>/docs-csm/en-13/operations/network/management_network/mellanox/mlag_architecture/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:49 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/management_network/mellanox/mlag_architecture/</guid>
      <description>MLAG (Multi-Chassis LAG) Is a type of Link Aggregation Group where ports from single device such as server terminate on two separate switches providing switch-level redundancy.
What are the benefits of MLAG
  Increased bandwidth achieved by dual connection to node.
  High availability (HA) for servers while allowing full use of the bandwidth of both links
  To achieve HA on a switch level without the using of STP</description>
    </item>
    
    <item>
      <title>Multi-chassis Interface</title>
      <link>/docs-csm/en-13/operations/network/management_network/mellanox/mlag/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:49 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/management_network/mellanox/mlag/</guid>
      <description>Multi-chassis interface Multi-Chassis Link Aggregation Group (MCLAG) is a link aggregation technique where two or more links across two switches are aggregated together to form a trunk.
Creating an MLAG interface:
Create an MLAG interface for the host. Run:
switch (config)# interface mlag-port-channel 1 switch (config interface mlag-port-channel 1)# The MPO interfaces should be configured in the same sequence on both switches of MLAG cluster.
Example:
On Switch 1:
interface mlag-port-channel 1-10 interface mlag-port-channel 30-40 On Switch 2:</description>
    </item>
    
    <item>
      <title>Multiple Spanning Tree Protocol (mstp)</title>
      <link>/docs-csm/en-13/operations/network/management_network/mellanox/mstp/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:49 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/management_network/mellanox/mstp/</guid>
      <description>Multiple spanning tree protocol (MSTP) MSTP (802.1s) ensures that only one active path exists between any two nodes in a spanning-tree instance. A spanning-tree instance comprises a unique set of VLANs. MSTP instances significantly improve network resource utilization while maintaining a loop-free environment.
Relevant Configuration
Enable MSTP (default mode for spanning-tree)
switch(config)# spanning-tree switch(config)# spanning-tree mode mstp switch(config)# spanning-tree mst revision 1 switch(config)# spanning-tree mst name mellanox Show Commands to Validate Functionality</description>
    </item>
    
    <item>
      <title>System Management Network Functions In More Detail</title>
      <link>/docs-csm/en-13/operations/network/management_network/mellanox/management_network_function_in_detail/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:49 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/management_network/mellanox/management_network_function_in_detail/</guid>
      <description>System management network functions in more detail Description
  Edge - Any interactions with the Customer network or Internet.
 Customer Jobs (Customer Access Network - CAN) User-facing cloud APIs.  UAI (User Access Instances).   Customer Administration (Customer Management Network - CMN).  Administrative Access to the system by Customer Admins. Access from the system to external services:  Customer/Internet DNS. LDAP authentication. System installation and upgrade media (e.</description>
    </item>
    
    <item>
      <title>Key Features Used In The Management Network Configuration</title>
      <link>/docs-csm/en-13/operations/network/management_network/mellanox/key_features/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:48 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/management_network/mellanox/key_features/</guid>
      <description>Key features used in the management network configuration This section is to list and define the key features used in the management network configuration.
Feature list:
   Feature Notes Description     MLAG  Layer 2 Redundancy, Allows the NCNs to be bonded so if one link fails they can continue to operate.   MLAG  Layer 3 Redundancy, Allows one Spine switch/default gateway to fail and continue to work   Vlan  Segregates layer 2 broadcast domains, need to separate NMN/HMN/compute traffic.</description>
    </item>
    
    <item>
      <title>Large</title>
      <link>/docs-csm/en-13/operations/network/management_network/mellanox/large/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:48 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/management_network/mellanox/large/</guid>
      <description>Large Back to index.</description>
    </item>
    
    <item>
      <title>Link Aggregation Group (lag)</title>
      <link>/docs-csm/en-13/operations/network/management_network/mellanox/lag/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:48 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/management_network/mellanox/lag/</guid>
      <description>Link aggregation group (LAG) Link Aggregation allows you to assign multiple physical links to one logical link that functions as a single, higher-speed link providing dramatically increased bandwidth.
Relevant Configuration
Create and configure the LAG interface
switch (config) # interface port-channel 1 switch (config interface port-channel 1) # Exit port-channel context
switch (config interface port-channel 1) # exit switch (config) # Associate member links with the LAG interface switch(config)# interface IFACE</description>
    </item>
    
    <item>
      <title>Link Layer Discovery Protocol (lldp)</title>
      <link>/docs-csm/en-13/operations/network/management_network/mellanox/lldp/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:48 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/management_network/mellanox/lldp/</guid>
      <description>Link layer discovery protocol (LLDP) LLDP is used to advertise the device&amp;rsquo;s identity and abilities and read other devices connected to the same network. Note: LLDP is enabled by default.
Relevant Configuration
Enable lldp
switch(config)# lldp Enable lldp on interface
switch (config interface ethernet 1/1) # lldp receive switch (config interface ethernet 1/1) # lldp transmit Show Commands to Validate Functionality
show lldp local Expected Results
 Step 1: Link status between the peer devices is UP Step 2: LLDP is enabled Step 3: Local device LLDP Information is displayed Step 4: Remote device LLDP information is displayed Step 5: LLDP statistics are displayed  Back to Index</description>
    </item>
    
    <item>
      <title>Loopback Interface</title>
      <link>/docs-csm/en-13/operations/network/management_network/mellanox/loopback/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:48 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/management_network/mellanox/loopback/</guid>
      <description>Loopback interface You can think of loopbacks as internal virtual interfaces. Loopback interfaces are not bound to a physical port and are used for device management and routing protocols.
Relevant Configuration
Create a loopback interface. Run:
switch (config)# interface loopback 2 switch (config interface loopback 2)# Configure an IP address on the loopback interface. Run:
switch (config interface loopback 2)# ip address 20.20.20.20 /32 Show Commands to Validate Functionality
show interfaces loopback 2 Expected Results</description>
    </item>
    
    <item>
      <title>Management Interface</title>
      <link>/docs-csm/en-13/operations/network/management_network/mellanox/management_interface/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:48 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/management_network/mellanox/management_interface/</guid>
      <description>Management interface The management interface can be used to gain remote management access to the switch. The management interface is accessible using the &amp;ldquo;mgmt&amp;rdquo; VRF and is separate from the data plane interfaces, which are in the &amp;ldquo;default&amp;rdquo; VRF. Mellanox switches support out-of-band (OOB) dedicated interfaces (e.g. mgmt0, mgmt1) and in-band dedicated interfaces.
Relevant Configuration
Enter Config configuration mode. Run:
switch &amp;gt; enable switch # configure terminal Disable setting IP addresses using the DHCP using the following command:</description>
    </item>
    
    <item>
      <title>Domain Name</title>
      <link>/docs-csm/en-13/operations/network/management_network/mellanox/domain_name/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:47 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/management_network/mellanox/domain_name/</guid>
      <description>Domain name A domain name is a name to identify the person, group, or organization that controls the devices within an area. An example of a domain name could be us.cray.com
Relevant Configuration
Creating a domain name
switch(config)# ip map-hostname Show Commands to Validate Functionality
show hosts Expected Results
 Step 1: You can configure the domain name Step 2: The output of all show commands is correct  Back to Index</description>
    </item>
    
    <item>
      <title>Exec Banners</title>
      <link>/docs-csm/en-13/operations/network/management_network/mellanox/exec_banner/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:47 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/management_network/mellanox/exec_banner/</guid>
      <description>Exec banners Banners are custom messages displayed to users attempting to connect to the management interfaces. MOTD banners are displayed pre-login while exec banners are displayed post-login. Multiple lines of text can be stored using a custom delimiter to mark the end of message.
Relevant Configuration
Create a banner.
switch(config)# banner motd Testing Show Commands to Validate Functionality.
show banner Example Output
ufmapl [ mgmt-sa ] (config) # show banner Banners: MOTD: Mellanox UFM Appliance Login: Mellanox MLNX-OS UFM Appliance Management Expected Results:</description>
    </item>
    
    <item>
      <title>Hostname</title>
      <link>/docs-csm/en-13/operations/network/management_network/mellanox/hostname/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:47 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/management_network/mellanox/hostname/</guid>
      <description>Hostname A hostname is a human-friendly name used to identify a device. An example of a hostname could be the name &amp;ldquo;Test.&amp;rdquo;
Relevant Configuration
Creating a hostname
switch(config)# hostname &amp;lt;NAME&amp;gt; Show Commands to Validate Functionality
show hosts Expected Results
 Step 1: You can configure the hostname Step 2: The output of all show commands is correct  Back to Index</description>
    </item>
    
    <item>
      <title>Igmp</title>
      <link>/docs-csm/en-13/operations/network/management_network/mellanox/igmp/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:47 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/management_network/mellanox/igmp/</guid>
      <description>IGMP The Internet Group Multicast Protocol (IGMP) is a communications protocol used by hosts and adjacent routers on IP networks to establish multicast group memberships. The host joins a multicast-group by sending a join request message towards the network router, and responds to queries sent from the network router by dispatching a join report.
Relevant Configuration
Enable IGMP snooping globally. Run:
switch (config) # ip igmp snooping Enable IGMP snooping on a VLAN.</description>
    </item>
    
    <item>
      <title>Ip Filter</title>
      <link>/docs-csm/en-13/operations/network/management_network/mellanox/ip_filter/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:47 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/management_network/mellanox/ip_filter/</guid>
      <description>Ip filter There are two types of malicious traffic that can be received from external sources to the data center:
 Traffic that target the switch&amp;rsquo;s CPU, either inband or out of band (e.g. via mgmt0) targeted one of the IP interfaces of the switch (loopback, router IP). To protect or filter those traffic threats use the ip filter set of commands. Traffic that target the data center servers transferred via the switch.</description>
    </item>
    
    <item>
      <title>You Are Getting An Ip Address, But Not The Correct One. Duplicate Ip Address Check</title>
      <link>/docs-csm/en-13/operations/network/management_network/mellanox/duplicate_ip/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:47 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/management_network/mellanox/duplicate_ip/</guid>
      <description>You are getting an IP address, but not the correct one. Duplicate IP address check A sign of a duplicate IP address is seeing a DECLINE message from the client to the server.
10.40.0.0.337 &amp;gt; 10.42.0.58.67: BOOTP/DHCP, Request from b4:2e:99:be:1a:d3, length 301, hops 1, xid 0x9d1210d, Flags [none] Gateway-IP 10.252.0.2 Client-Ethernet-Address b4:2e:99:be:1a:d3 Vendor-rfc1048 Extensions Magic Cookie 0x63825363 DHCP-Message Option 53, length 1: Decline Client-ID Option 61, length 19: hardware-type 255, 99:be:1a:d3:00:01:00:01:26:c8:55:c3:b4:2e:99:be:1a:d3 Server-ID Option 54, length 4: 10.</description>
    </item>
    
    <item>
      <title>Check DHCP Lease Is Getting Allocated</title>
      <link>/docs-csm/en-13/operations/network/management_network/mellanox/check_dhcp_lease_is_getting_allocated/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:46 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/management_network/mellanox/check_dhcp_lease_is_getting_allocated/</guid>
      <description>Check DHCP lease is getting allocated  Check the KEA logs and verify that the lease is getting allocated.  kubectl logs -n services pod/$(kubectl get -n services pods | grep kea | head -n1 | cut -f 1 -d &#39; &#39;) -c cray-dhcp-kea 2021-04-21 00:13:05.416 INFO [kea-dhcp4.leases/24.139710796402304] DHCP4_LEASE_ ***ALLOC*** [hwtype=1 02:23:28:01:30:10], cid=[00:78:39:30:30:30:63:31:73:30:62:31], tid=0x21f2433a: lease 10.104.0.23 has been allocated for 300 seconds
 Here we can see that KEA is allocating a lease to 10.</description>
    </item>
    
    <item>
      <title>Check HSM</title>
      <link>/docs-csm/en-13/operations/network/management_network/mellanox/check_hsm/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:46 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/management_network/mellanox/check_hsm/</guid>
      <description>Check HSM Hardware State Manager has two important parts:
 System Layout Service (SLS): This is the &amp;ldquo;expected&amp;rdquo; state of the system (as populated by networks.yaml and other sources). State Manager Daemon (SMD): This is the &amp;ldquo;discovered&amp;rdquo; or active state of the system during runtime.  Prerequisites  The API calls on this page require an authorization token to be set in the TOKEN variable. See Retrieve an Authentication Token. The cray CLI commands on this page require the Cray command line interface to be configured.</description>
    </item>
    
    <item>
      <title>Check Kea DHCP Logs</title>
      <link>/docs-csm/en-13/operations/network/management_network/mellanox/check_kea_dhcp_logs/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:46 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/management_network/mellanox/check_kea_dhcp_logs/</guid>
      <description>Check KEA DHCP logs In order to check the logs for the pod you will need to know the pod name, run this command to see the pod name:
kubectl logs -n services -l app.kubernetes.io/instance=cray-dhcp-kea -c cray-dhcp-kea Example:
kubectl logs -n services -l app.kubernetes.io/instance=cray-dhcp-kea -c cray-dhcp-kea 2020-08-03 21:47:50.580 INFO [kea-dhcp4.dhcpsrv/10] DHCPSRV_MEMFILE_LEASE_FILE_LOAD loading leases from file /cray-dhcp-kea-socket/dhcp4.leases 2020-08-03 21:47:50.580 INFO [kea-dhcp4.dhcpsrv/10] DHCPSRV_MEMFILE_LFC_SETUP setting up the Lease File Cleanup interval to 3600 sec 2020-08-03 21:47:50.</description>
    </item>
    
    <item>
      <title>Computes/UANs/application Nodes</title>
      <link>/docs-csm/en-13/operations/network/management_network/mellanox/compute_uan_application_nodes/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:46 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/management_network/mellanox/compute_uan_application_nodes/</guid>
      <description>Computes/UANs/Application Nodes If the Computes make it past PXE and go into the PXE shell you can verify DNS and connectivity.
iPXE&amp;gt; dhcp Configuring (net0 98:03:9b:a8:60:88).................. No configuration methods succeeded (http://ipxe.org/040ee186) Configuring (net1 b4:2e:99:be:1a:37)...... ok iPXE&amp;gt; show dns net1.dhcp/dns:ipv4 = 10.92.100.225 iPXE&amp;gt; nslookup address api-gw-service-nmn.local iPXE&amp;gt; echo ${address} 10.92.100.71 Back to Index</description>
    </item>
    
    <item>
      <title>Domain Name System (DNS) Client</title>
      <link>/docs-csm/en-13/operations/network/management_network/mellanox/dns-client/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:46 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/management_network/mellanox/dns-client/</guid>
      <description>Domain name system (DNS) client The Domain Name System (DNS) translates domain and host names to and from IP addresses. A DNS client resolves hostnames to IP addresses by querying assigned DNS servers for the appropriate IP address.
Relevant Configuration
Configure the switch to resolve queries via a DNS server
switch(config)# ip name-server &amp;lt;IPv4/IPv6 address&amp;gt; Configure a domain name
switch(config)# ip domain-list mydomain2.com Show Commands to Validate Functionality
show hosts Expected Results</description>
    </item>
    
    <item>
      <title>Large Number Of DHCP Declines During A Node Boot</title>
      <link>/docs-csm/en-13/operations/network/management_network/mellanox/dhcp_decline/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:46 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/management_network/mellanox/dhcp_decline/</guid>
      <description>Large Number of DHCP Declines During a Node Boot If something similar to the following is in the logs, then this indicates an issue that an IP address being allocated is already being used. It is not able to get the IP address assigned to the device.
dracut-initqueue[1902]: wicked: eth0: Declining DHCPv4 lease with address 10.252.0.56 dracut-initqueue[1902]: wicked: eth0: Declining DHCPv4 lease with address 10.252.0.57 dracut-initqueue[1902]: wicked: eth0: Declining DHCPv4 lease with address 10.</description>
    </item>
    
    <item>
      <title>Access Control Lists (ACLs)</title>
      <link>/docs-csm/en-13/operations/network/management_network/mellanox/acl/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:45 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/management_network/mellanox/acl/</guid>
      <description>Access control lists (ACLs) ACLs are used to help improve network performance and restrict network usage by creating policies to eliminate unwanted IP traffic by filtering packets where they enter the switch on layer 2 and layer 3 interfaces. An ACL is an ordered list of one or more access control list entries (ACEs) prioritized by sequence number. An incoming packet is matched sequentially against each entry in an ACL. When a match is made, the action of that ACE is taken and the packet is not compared against any other ACEs in the list.</description>
    </item>
    
    <item>
      <title>Address Resolution Protocol (arp)</title>
      <link>/docs-csm/en-13/operations/network/management_network/mellanox/arp/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:45 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/management_network/mellanox/arp/</guid>
      <description>Address resolution protocol (ARP) ARP is commonly used for mapping IPv4 addresses to MAC addresses. Static ARP addresses only supported in management interfaces;
Relevant Configuration
Configure static ARP on an interface
Switch (config) # interface mgmt0 switch(config interface mgmt0)# arp ipv4 IP-ADDR mac MAC-ADDR Show Commands to Validate Functionality
show ip arp Expected Results
 Step 1: You are able to ping the connected device Step 2: You can view the ARP entries  Back to Index</description>
    </item>
    
    <item>
      <title>Backing Up Switch Configuration</title>
      <link>/docs-csm/en-13/operations/network/management_network/mellanox/backup/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:45 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/management_network/mellanox/backup/</guid>
      <description>Backing up switch configuration Backing up current configuration of the switch in text format Example
To create a new text-based configuration file, complete the following steps:
Log in to the switch as Admin.
Type the following command:
switch (config) # configuration text generate active running save my-filename To upload a text-based configuration file from a switch to an external file server, complete the following steps:
switch (config) # configuration text file my-filename upload scp://root@my-server/root/tmp/my-filename Back to Index</description>
    </item>
    
    <item>
      <title>BGP Basics</title>
      <link>/docs-csm/en-13/operations/network/management_network/mellanox/bgp_basic/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:45 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/management_network/mellanox/bgp_basic/</guid>
      <description>BGP basics &amp;ldquo;The primary function of a Border Gateway Protocol (BGP) speaking system is to exchange network reachability information with other BGP systems. This network reachability information includes information on the list of Autonomous Systems (ASes) that reachability information traverses. This information is sufficient for constructing a graph of AS connectivity for this reachability, from which routing loops may be pruned and, at the AS level, some policy decisions may be enforced.</description>
    </item>
    
    <item>
      <title>Cable Diagnostics</title>
      <link>/docs-csm/en-13/operations/network/management_network/mellanox/cable_diagnostics/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:45 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/management_network/mellanox/cable_diagnostics/</guid>
      <description>Cable diagnostics Cable plugin collects various information from the cables attached to the fabric ports.
&amp;ndash;get_cable_info	Gets cable info from the fabric ports. &amp;ndash;cable_info_disconnected	Gets cable info on disconnected ports (the cable is attached only to the switch port). This option is applicable with the &amp;ldquo;get-cable-info&amp;rdquo; flag.
Relevant Configuration
Example:
ibdiagnet --get-cable-info --cable_info_disconnected The data is dumped to the ibdiagnet2.cables file in the following format: ------------------------------------------------------- Port=1 Lid=0x00a4 GUID=0xf45214030046a0a1 Port Name=coral-ufm-001/U1/P1 ------------------------------------------------------- Vendor: Mellanox OUI: 0x2c9 PN: MCP1600-E002 SN: MT1739VS02126 Rev: A3 Length: 2 m Type: Copper cable- unequalized SupportedSpeed: SDR/DDR/QDR/FDR/EDR Temperature: N/A PowerClass: 1 NominalBitrate: 0 Gb/s CDREnableTxRx: N/A N/A InputEq: N/A OutputAmp: N/A OutputEmp: N/A FW Version: N/A Attenuation(5,7,12): 7 8 13 RX power type: OMA RX1 Power: 0.</description>
    </item>
    
    <item>
      <title>Check BGP And Metallb</title>
      <link>/docs-csm/en-13/operations/network/management_network/mellanox/check_bgp_and_metallb/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:45 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/management_network/mellanox/check_bgp_and_metallb/</guid>
      <description>Check BGP and MetalLB Log in to the spine switches if you have access and check that MetalLB is peering to the spines via BGP.
Check both spines if they are available (powered up):
show ip bgp summary Example working state:
All the neighbors should be in the Established state.
sw-spine01 [standalone: master] # show ip bgp summary VRF name : default BGP router identifier : 10.252.0.1 local AS number : 65533 BGP table version : 6 Main routing table version: 6 IPV4 Prefixes : 84 IPV6 Prefixes : 0 L2VPN EVPN Prefixes : 0 ------------------------------------------------------------------------------------------------------------------ Neighbor V AS MsgRcvd MsgSent TblVer InQ OutQ Up/Down State/PfxRcd ------------------------------------------------------------------------------------------------------------------ 10.</description>
    </item>
    
    <item>
      <title>Check Current DHCP Leases</title>
      <link>/docs-csm/en-13/operations/network/management_network/mellanox/check_current_dhcp_leases/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:45 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/management_network/mellanox/check_current_dhcp_leases/</guid>
      <description>Check current DHCP leases We will use the Kea API to retrieve data from the DHCP lease database. First you need to get the auth token, On ncn-w001 or a worker/manager with kubectl, run:
export TOKEN=$(curl -s -k -S -d grant_type=client_credentials -d client_id=admin-client -d client_secret=`kubectl get secrets admin-client-auth -o jsonpath=&#39;{.data.client-secret}&#39; | base64 -d` https://api-gw-service-nmn.local/keycloak/realms/shasta/protocol/openid-connect/token | jq -r &#39;.access_token&#39;) Once you generate the auth token you can run these commands on a worker or manager node.</description>
    </item>
    
    <item>
      <title>Generate Switch Configurations</title>
      <link>/docs-csm/en-13/operations/network/management_network/generate_switch_configs/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:44 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/management_network/generate_switch_configs/</guid>
      <description>Generate Switch Configurations Generating configuration files can be done for a single switch or for the full system.
For example, if there is a suspected configuration issue on single switch, a configuration file can be generated for only that switch in order to simplify debugging.
Prerequisites  CANU installed with 1.6.13 or later versions.  Run canu --version to see version. If doing a CSM install or upgrade, a CANU RPM is located in the release tarball.</description>
    </item>
    
    <item>
      <title>Load Saved Switch Configuration</title>
      <link>/docs-csm/en-13/operations/network/management_network/load_saved_switch_config/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:44 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/management_network/load_saved_switch_config/</guid>
      <description>Load Saved Switch Configuration  This procedure is intended for internal use only.
 This procedure switches between already saved switch configurations. It is used to quickly switch between configurations that are already loaded on the switches.
To save switch configurations, refer to the Configuration Management procedure.
When switching between configurations, the procedure must be followed on all management switches.
  Spine switches have three total configuration files/checkpoints.
 1.</description>
    </item>
    
    <item>
      <title>Manual Switch Configuration</title>
      <link>/docs-csm/en-13/operations/network/management_network/manual_switch_config/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:44 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/management_network/manual_switch_config/</guid>
      <description>Manual Switch Configuration Some of the switch configuration is not generated by CANU and needs to be manually applied.
Prerequisites The custom switch configuration is backed up. See Manual Switch Config.
SNMP Configuration SNMP is currently only used on sw-leaf-bmc switches, these credentials can be retrieved from Vault. More information on SNMP credentials can be found in the Change SNMP Credentials on Leaf-BMC Switches procedure.
Once these credentials are retrieved from vault, fill in the xxxxxx fields below and paste the commands into the switch.</description>
    </item>
    
    <item>
      <title>Mellanox Installation And Configuration Guide</title>
      <link>/docs-csm/en-13/operations/network/management_network/mellanox/readme/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:44 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/management_network/mellanox/readme/</guid>
      <description>Mellanox Installation and Configuration Guide This documentation helps network administrators and support personnel install install and manage Mellanox network devices in a CSM install.
The HPE Cray recommended way of configuring the network is by using the CANU tool. Therefore this guide will not go into detail on how to configure each switch manually using the CLI. Instead, it will give helpful examples of how to configure/use features generated by CANU, in order to provide administrators easy ways to customize their installation.</description>
    </item>
    
    <item>
      <title>Configure Virtual Local Access Networks (vlans)</title>
      <link>/docs-csm/en-13/operations/network/management_network/dell/vlan/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:43 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/management_network/dell/vlan/</guid>
      <description>Configure Virtual Local Access Networks (VLANs) VLANs allow for the logical grouping of switch interfaces, enabling communication as if all connected devices were on the same isolated network.
Configuration Commands Create VLAN:
interface vlan &amp;lt;VLAN&amp;gt; Show commands to validate functionality:
show vlan [VLAN] Expected Results  Administrators can create a VLAN Administrators can assign a VLAN to the physical interface  Back to Index</description>
    </item>
    
    <item>
      <title>Configure Vlan Interface</title>
      <link>/docs-csm/en-13/operations/network/management_network/dell/vlan_interface/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:43 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/management_network/dell/vlan_interface/</guid>
      <description>Configure VLAN Interface The switch also supports classic L3 VLAN interfaces.
Configuration Commands Configure the VLAN:
vlan VLAN The default mode of any VLAN is L2 only. To enable L3 functionality, run no shutdown on the VLAN:
interface vlan 2 no shutdown Show commands to validate functionality:
show interface vlan Expected Results  Administrators can configure the VLAN Administrators can enable the interface and associate it with the VLAN Administrators can create an IP-enabled VLAN interface, and it is up Administrators validate the configuration is correct Administrators can ping from the switch to the client and from the client to the switch  Back to Index</description>
    </item>
    
    <item>
      <title>External User Guides</title>
      <link>/docs-csm/en-13/operations/network/management_network/external_user_guides/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:43 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/management_network/external_user_guides/</guid>
      <description>External User Guides Refer to the following vendor-specific user guides for more information on Aruba, Dell, and Mellanox switches.
Aruba  https://asp.arubanetworks.com/downloads  Dell  https://www.dell.com/support/manuals/en-us/force10-s4048-on/smartfabric-os-user-guide-10-5-1  Mellanox  https://docs.nvidia.com/networking/spaces/viewspace.action?key=Onyxv393210  </description>
    </item>
    
    <item>
      <title>Fresh Install</title>
      <link>/docs-csm/en-13/operations/network/management_network/fresh_install/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:43 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/management_network/fresh_install/</guid>
      <description>Fresh Install Use this procedure for either a first-time install or in the event a previous CSM was wiped and requires a new install.
Before continuing with install, make sure that CANU is running the most current version:
Install/Upgrade CANU
Procedure  CAUTION: All of the following steps should be done using an out-of-band connection. This process is disruptive and will require downtime.
   Upgrade switch firmware to specified firmware version.</description>
    </item>
    
    <item>
      <title>Update Management Network Firmware</title>
      <link>/docs-csm/en-13/operations/network/management_network/firmware/update_management_network_firmware/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:43 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/management_network/firmware/update_management_network_firmware/</guid>
      <description>Update Management Network Firmware This page describes how to update firmware on the management network switches. More details and other options to upgrade firmware can be found in the switch External User Guides.
Prerequisites  Access to the switches. Firmware in a location that the switches can reach.  All firmware can be found in the HFP package provided with the Shasta release.
Switch Firmware    Model software version     Aruba 8320 Switch Series 10.</description>
    </item>
    
    <item>
      <title>Vlan Trunking 802.1q</title>
      <link>/docs-csm/en-13/operations/network/management_network/dell/vlan_trunking_8021q/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:43 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/management_network/dell/vlan_trunking_8021q/</guid>
      <description>VLAN Trunking 802.1Q A trunk port carries packets on one or more specified VLANs. Packet that ingress on a trunk port are in the VLAN specified in its 802.1Q header, or native VLAN if the packet has no 802.1Q header. A packet that egresses through a trunk port will have an 802.1Q header if it has a nonzero VLAN ID. Any packet that ingresses on a trunk port tagged with a VLAN that the port does not trunk is dropped.</description>
    </item>
    
    <item>
      <title>Configure Remote Logging</title>
      <link>/docs-csm/en-13/operations/network/management_network/dell/remote_logging/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:42 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/management_network/dell/remote_logging/</guid>
      <description>Configure Remote Logging Configure remote logging to view log files from the switch on a remote server. This functionality is enabled by syslog.
Configuration Commands Configure logging:
logging server dell.com severity log-info Expected Results  Administrators can configure remote logging Administrators can see the log files from the switch on the remote server  Back to Index</description>
    </item>
    
    <item>
      <title>Configure Secure Shell (SSH)</title>
      <link>/docs-csm/en-13/operations/network/management_network/dell/ssh/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:42 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/management_network/dell/ssh/</guid>
      <description>Configure Secure Shell (SSH) SSH server enables an SSH client to make a secure and encrypted connection to a switch. Currently, switches support SSH version 2.0 only. The user authentication mechanisms supported for SSH are public key authentication and password authentication (RADIUS, TACACS+, or locally stored password). Secure File Transfer Protocol (SFTP) provides file transfer. SSH Server and sftp-client via the copy command are supported for managing the router.
Configuration Commands The SSH server is enabled by default.</description>
    </item>
    
    <item>
      <title>Configure Snmpv2c Community</title>
      <link>/docs-csm/en-13/operations/network/management_network/dell/snmp-community/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:42 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/management_network/dell/snmp-community/</guid>
      <description>Configure SNMPv2c Community The switch supports SNMPv2c community-based security for read-only access.
Configuration Commands Configure an SNMPv2c community name:
snmp-server community community-name Show commands to validate functionality:
show snmp community Expected Results  Administrators can configure the community name Administrators can bind the SNMP server to the default VRF Administrators can connect from the workstation using the community name  Back to Index</description>
    </item>
    
    <item>
      <title>Configure System Images</title>
      <link>/docs-csm/en-13/operations/network/management_network/dell/system_images/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:42 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/management_network/dell/system_images/</guid>
      <description>Configure System Images Dell switches support active and standby images.
Configuration Commands Copy an image from a local server:
image download ftp://admin@1.1.1.1:/image.bin Install image:
image install file-url Show commands to validate functionality:
show boot detail Expected Results  Administrators can upload an image to the switch Administrators can boot into the uploaded image Administrators can see they are running the uploaded image  Back to Index</description>
    </item>
    
    <item>
      <title>Dell Snmpv3 Users</title>
      <link>/docs-csm/en-13/operations/network/management_network/dell/snmpv3_users/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:42 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/management_network/dell/snmpv3_users/</guid>
      <description>Dell SNMPv3 Users SNMPv3 supports cryptographic security by a combination of authenticating and encrypting the SNMP protocol packets over the network. Read-only access is currently supported. The admin user can add or remove SNMPv3 users.
Configuration Commands Configure a new SNMPv3 user (minimum 8 characters for passwords):
snmp-server user &amp;lt;USER&amp;gt; cray-reds-group 3 auth md5 &amp;lt;A-PASS&amp;gt; priv des &amp;lt;P-PASS&amp;gt;  NOTE Removal of an SNMPv3 user is not possible on Dell equipment.</description>
    </item>
    
    <item>
      <title>Perform An Upgrade On Dell Switches</title>
      <link>/docs-csm/en-13/operations/network/management_network/dell/upgrade/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:42 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/management_network/dell/upgrade/</guid>
      <description>Perform an Upgrade on Dell Switches How to perform an upgrade on the Dell switches.
Configuration Commands Download the new software image:
image download file-url View the current software download status:
show image status Install the software image:
image install image-url View the status of the current software install:
show image status Change the next boot partition to the standby partition:
boot system standby Reload the new software image:
reload Back to Index</description>
    </item>
    
    <item>
      <title>Reset Dell Switch Configuration</title>
      <link>/docs-csm/en-13/operations/network/management_network/dell/reset/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:42 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/management_network/dell/reset/</guid>
      <description>Reset Dell Switch Configuration How to reset Dell switch configuration:
delete startup-config Back to Index</description>
    </item>
    
    <item>
      <title>Configure Loopback Interface</title>
      <link>/docs-csm/en-13/operations/network/management_network/dell/loopback/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:41 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/management_network/dell/loopback/</guid>
      <description>Configure Loopback Interface Loopbacks can be thought of as internal virtual interfaces. Loopback interfaces are not bound to a physical port and are used for device management and routing protocols.
Configuration Commands interface loopback LOOPBACK ip address IP-ADDR/&amp;lt;SUBNET|PREFIX&amp;gt; Expected Results  Create a loopback interface. Give a loopback interface an IP address. Validate the configuration using the show commands.  Back to Index</description>
    </item>
    
    <item>
      <title>Configure Management Interface</title>
      <link>/docs-csm/en-13/operations/network/management_network/dell/management_interface/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:41 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/management_network/dell/management_interface/</guid>
      <description>Configure Management Interface The management interface can be used to gain remote management access to the switch. The management interface is accessible using the mgmt VRF and is separate from the data plane interfaces, which are in the default VRF.
Alternatively, a loopback interface can be configured to be used as management interface.
Configuration Commands Configure the Management interface in CONFIGURATION mode:
interface mgmt 1/1/1 Configure an IP address and mask on the Management interface in INTERFACE mode:</description>
    </item>
    
    <item>
      <title>Configure Multiple Spanning Tree Protocol (mstp)</title>
      <link>/docs-csm/en-13/operations/network/management_network/dell/mstp/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:41 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/management_network/dell/mstp/</guid>
      <description>Configure Multiple Spanning Tree Protocol (MSTP) MSTP (802.1s) ensures that only one active path exists between any two nodes in a spanning-tree instance. A spanning-tree instance comprises a unique set of VLANs. MSTP instances significantly improve network resource utilization while maintaining a loop-free environment.
Configuration Commands Enable MSTP (default mode for spanning-tree):
spanning-tree mode mst name my-mstp-region revision 0 Show commands to validate functionality:
show spanning-tree mst Expected Results  Spanning-tree mode is configured Spanning-tree is enabled, if loops are detected ports should go blocked state Spanning-tree splits traffic domain between two DUTs  Back to Index</description>
    </item>
    
    <item>
      <title>Configure Physical Interfaces</title>
      <link>/docs-csm/en-13/operations/network/management_network/dell/physical_interfaces/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:41 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/management_network/dell/physical_interfaces/</guid>
      <description>Configure Physical Interfaces Ethernet port interfaces are enabled by default.
Configuration Commands Enable the interface:
interface ethernet 1/1/1 no shutdown Disable the interface:
interface ethernet 1/1/1 shutdown Show commands to validate functionality:
show configuration Expected Results  The switch recognizes the transceiver without errors Administrators can enter the interface context for the port and enable it Administrators can establish a link with a partner Administrators can pass traffic as expected  Back to Index</description>
    </item>
    
    <item>
      <title>Configure Qos</title>
      <link>/docs-csm/en-13/operations/network/management_network/dell/qos/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:41 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/management_network/dell/qos/</guid>
      <description>Configure QoS Network traffic is processed based on classification and policies that are created and applied to the traffic.
QoS trust is by default disabled.
Configuration Commands Create a dot1p trust map:
trust dot1p-map dot1p-trust-map switch(config-tmap-dot1p-map)# Define the set of values to match the class:
qos-group 3 dot1p 0-4 qos-group 5 dot1p 5-7 Apply the map on a specific interface or on global level:
trust-map dot1p dot1p-trust-map trust-map dot1p dot1p-trust-map Back to Index</description>
    </item>
    
    <item>
      <title>Network Time Protocol (NTP) Client</title>
      <link>/docs-csm/en-13/operations/network/management_network/dell/ntp/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:41 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/management_network/dell/ntp/</guid>
      <description>Network Time Protocol (NTP) Client Summary of NTP from RFC-1305 Network Time Protocol (Version 3):
 NTP is used to synchronize timekeeping among a set of distributed time servers and clients &amp;hellip; It provides the protocol mechanisms to synchronize time in principle to precisions in the order of nanoseconds while preserving a non-ambiguous date well into the next century.
 The Network Time Protocol (NTP) client is essential for syncing time on various clients in the system.</description>
    </item>
    
    <item>
      <title>Configure Domain Name</title>
      <link>/docs-csm/en-13/operations/network/management_network/dell/domain_name/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:40 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/management_network/dell/domain_name/</guid>
      <description>Configure Domain Name A domain name is a name to identify the person, group, or organization that controls the devices within an area. An example of a domain name could be us.cray.com.
Configuration Commands Create a domain name:
domain-name NAME Show commands to validate functionality:
show domain-name Expected Results  Administrators can configure the domain name The output of all show commands is correct  Back to Index</description>
    </item>
    
    <item>
      <title>Configure Hostnames</title>
      <link>/docs-csm/en-13/operations/network/management_network/dell/hostname/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:40 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/management_network/dell/hostname/</guid>
      <description>Configure Hostnames A hostname is a human-friendly name used to identify a device. An example of a hostname could be the name Test.
Configuration Commands Create a hostname:
hostname NAME Show commands to validate functionality:
show hostname Example Output hostname switch-test show hostname Expected Results  Administrators can configure the hostname The output of all show commands is correct  Back to Index</description>
    </item>
    
    <item>
      <title>Configure Internet Group Multicast Protocol (igmp)</title>
      <link>/docs-csm/en-13/operations/network/management_network/dell/igmp/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:40 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/management_network/dell/igmp/</guid>
      <description>Configure Internet Group Multicast Protocol (IGMP) The Internet Group Multicast Protocol (IGMP) is a communications protocol used by hosts and adjacent routers on IP networks to establish multicast group memberships. The host joins a multicast-group by sending a join request message towards the network router, and responds to queries sent from the network router by dispatching a join report.
Configuration Command ip igmp snooping enable Expected Results show ip igmp-snooping vlan 1 should show IGMP enabled on the VLAN, but no IGMP Querier set</description>
    </item>
    
    <item>
      <title>Configure Link Aggregation Group (lag)</title>
      <link>/docs-csm/en-13/operations/network/management_network/dell/lag/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:40 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/management_network/dell/lag/</guid>
      <description>Configure Link Aggregation Group (LAG) Link aggregation allows administrators to assign multiple physical links to one logical link that functions as a single, higher-speed link providing dramatically increased bandwidth.
Configuration Commands Create and configure the LAG interface:
interface port-channel 10 no shutdown Associate member links with the LAG interface:
interface IFACE`
interface ethernet 1/1/1 channel-group 10 To enable LACP on the LAG:
interface ethernet 1/1/1 switch(conf-if-eth1/1/1)#channel-group 10 mode active Show commands to validate functionality:</description>
    </item>
    
    <item>
      <title>Configure Locator Led</title>
      <link>/docs-csm/en-13/operations/network/management_network/dell/locator_led/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:40 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/management_network/dell/locator_led/</guid>
      <description>Configure Locator LED The Locator LED is an LED in the front of the chassis that can turn on or flash. This is a useful feature when guiding someone to the switch during a &amp;ldquo;remote hands&amp;rdquo; situation, such as asking an engineer to run a cable to the switch.
Configuration Commands Enable LED:
location-led system 1 on Disable LED:
location-led system 1 off Expected Results  The Locator LED is in the off state The Locator LED is now flashing  Back to Index</description>
    </item>
    
    <item>
      <title>Link Layer Discovery Protocol (lldp)</title>
      <link>/docs-csm/en-13/operations/network/management_network/dell/lldp/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:40 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/management_network/dell/lldp/</guid>
      <description>Link layer discovery protocol (LLDP) By default, LLDP is enabled for each interface and globally. Administrators can disable LLDP on an interface or globally. If LLDP is disabled globally, LLDP is disabled on all interfaces irrespective of whether LLDP is previously enabled or disabled on an interface. When administrators enable LLDP globally, the LLDP configuration at the interface level takes precedence over the global LLDP configuration.
Configuration Commands Disable the LLDPDU transmit or receive in INTERFACE mode:</description>
    </item>
    
    <item>
      <title>Back Up A Switch Configuration</title>
      <link>/docs-csm/en-13/operations/network/management_network/dell/backup/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:39 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/management_network/dell/backup/</guid>
      <description>Back Up a Switch Configuration The following command copies the running configuration or the startup configuration to a remote location as a file.
copy running-configuration {config://filepath | home://filepath | ftp://userid:passwd@hostip/filepath | scp://userid:passwd@hostip/filepath | sftp://userid:passwd@hostip/filepath | tftp://hostip/filepath} Back to Index</description>
    </item>
    
    <item>
      <title>Configure Access Control Links (ACLs)</title>
      <link>/docs-csm/en-13/operations/network/management_network/dell/acl/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:39 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/management_network/dell/acl/</guid>
      <description>Configure Access Control Links (ACLs) ACLs are used to help improve network performance and restrict network usage by creating policies to eliminate unwanted IP traffic by filtering packets where they enter the switch on layer 2 and layer 3 interfaces. An ACL is an ordered list of one or more access control list entries (ACEs) prioritized by sequence number. An incoming packet is matched sequentially against each entry in an ACL.</description>
    </item>
    
    <item>
      <title>Configure Address Resolution Protocol (arp)</title>
      <link>/docs-csm/en-13/operations/network/management_network/dell/arp/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:39 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/management_network/dell/arp/</guid>
      <description>Configure Address Resolution Protocol (ARP) ARP is commonly used for mapping IPv4 addresses to MAC addresses.
Configuration Commands Configure static ARP on an interface:
ip arp ipv4 IP-ADDR mac MAC-ADDR Show commands to validate functionality:
show ip arp Expected Results  Administrators are able to ping the connected device Administrators can view the ARP entries  Back to Index</description>
    </item>
    
    <item>
      <title>Configure Domain Name System (DNS) Client</title>
      <link>/docs-csm/en-13/operations/network/management_network/dell/dns-client/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:39 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/management_network/dell/dns-client/</guid>
      <description>Configure Domain Name System (DNS) Client The Domain Name System (DNS) translates domain and host names to and from IP addresses. A DNS client resolves hostnames to IP addresses by querying assigned DNS servers for the appropriate IP address.
Configuration Commands Enter a domain name in CONFIGURATION mode (up to 64 alphanumeric characters):
ip domain-name NAME Add names to complete unqualified host names in CONFIGURATION mode:
ip domain-list NAME Expected Results  Administrators can configure the DNS client The output is correct Administrators can ping the device Back to Index  </description>
    </item>
    
    <item>
      <title>Configure Snmp</title>
      <link>/docs-csm/en-13/operations/network/management_network/configure_snmp/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:39 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/management_network/configure_snmp/</guid>
      <description>Configure SNMP SNMP configuration is required for hardware discovery of the HPE Cray EX system.
These are examples only; verify SNMP credentials before applying this configuration.
For more information on SNMP credentials, see Change SNMP Credentials on Leaf-BMC Switches and Update Default Air-Cooled BMC and Leaf-BMC Switch SNMP Credentials.
Dell SNMP conf t snmp-server group cray-reds-group 3 noauth read cray-reds-view snmp-server user testuser cray-reds-group 3 auth md5 xxxxxxxx priv des xxxxxxx snmp-server view cray-reds-view 1.</description>
    </item>
    
    <item>
      <title>Dell Installation And Configuration Guide</title>
      <link>/docs-csm/en-13/operations/network/management_network/dell/readme/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:39 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/management_network/dell/readme/</guid>
      <description>Dell Installation and Configuration Guide This documentation helps network administrators and support personnel install and manage Dell network devices in a CSM install.
The HPE Cray recommended way of configuring the network is by using the CANU tool. Therefore this guide will not go into detail on how to configure each switch manually using the CLI. Instead, it will give helpful examples of how to configure/use features generated by CANU, in order to provide administrators easy ways to customize their installation.</description>
    </item>
    
    <item>
      <title>Collect Data</title>
      <link>/docs-csm/en-13/operations/network/management_network/collect_data/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:38 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/management_network/collect_data/</guid>
      <description>Collect Data Collect the input data needed to generate switch configurations.
Prerequisites  SSH access to the switches System Layout Service (SLS) API access  Procedure   Retrieve the most up-to-date SHCD spreadsheet. Accuracy in this spreadsheet is critical.
For example:
 Internal repository Customer repository    Get an SLS file from a Shasta system.
Log into any NCN where the Cray CLI is configured. Then run this command to create an SLS file named sls_file.</description>
    </item>
    
    <item>
      <title>Configuration Management</title>
      <link>/docs-csm/en-13/operations/network/management_network/config_management/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:38 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/management_network/config_management/</guid>
      <description>Configuration Management This page is designed for:
 Showing users how initially save switch configurations so they can be used again. Switching between saved configurations.   CAUTION All of these steps should be done using an out of band connection. This process is disruptive and will require downtime.
 All this information can be found in the switch External User Guides.
Prerequisites It is recommended to do a show run on each switch and save that configuration before attempting the following procedures.</description>
    </item>
    
    <item>
      <title>Quick Start Guide To CANu</title>
      <link>/docs-csm/en-13/operations/network/management_network/canu/quick_start_guide_to_canu/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:38 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/management_network/canu/quick_start_guide_to_canu/</guid>
      <description>Quick start guide to CANU Usage To run, type canu. It should run and display help.
To see a list of commands and arguments, just append --help.
When running CANU, the Shasta version is required, you can pass it in with either -s or --shasta for example:
canu -s 1.5 To checkout a fresh system using CSI:
 Make a new directory to save switch IP addresses:  mkdir ips_folder cd ips_folder  Parse CSI files and save switch IP addresses:  canu -s 1.</description>
    </item>
    
    <item>
      <title>Uninstall CANu</title>
      <link>/docs-csm/en-13/operations/network/management_network/canu/uninstall_canu/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:38 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/management_network/canu/uninstall_canu/</guid>
      <description>Uninstall CANU Uninstalling CANU can be achieved by:
pip3 uninstall canu </description>
    </item>
    
    <item>
      <title>Update CANu From Release Tarball</title>
      <link>/docs-csm/en-13/operations/network/management_network/canu/update_canu_from_csm_tarball/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:38 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/management_network/canu/update_canu_from_csm_tarball/</guid>
      <description>Update CANU From CSM Release Tarball If doing a CSM install or upgrade, the release tarball contains a CANU RPM. It can be extracted and installed using the following steps.
Procedure   Display the current CANU version.
canu --version   Set the TARBALL variable to the path and filename of the CSM release tarball:
TARBALL=/your/path/here/csm-version.tar.gz   Extract the CANU RPM from the tarball:
tar -xzvf &amp;#34;$TARBALL&amp;#34; --wildcards &amp;#34;*/canu*.</description>
    </item>
    
    <item>
      <title>Upgrade CANu</title>
      <link>/docs-csm/en-13/operations/network/management_network/canu_install_update/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:38 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/management_network/canu_install_update/</guid>
      <description>Upgrade CANU Prerequisite Before using the CSM Automated Network Utility (CANU) to test, validate, or configure the network, ensure that CANU is running on the latest version.
CANU can be run from your personal workstation (the instructions below are targeted at Mac users), or on the Shasta NCN nodes.
Since CANU is a python application, it can be run on Linux and Mac, but the RPM is not currently designed to support multiple operating system environments.</description>
    </item>
    
    <item>
      <title>Use CANu To Generate Full Network Configuration</title>
      <link>/docs-csm/en-13/operations/network/management_network/canu/using_canu_to_generate_full_network_config/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:38 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/management_network/canu/using_canu_to_generate_full_network_config/</guid>
      <description>Use CANU to Generate Full Network Configuration CANU can also generate switch configurations for all the switches on a network.
In order to generate network configurations, a valid SHCD must be passed in and system variables must be read in from either CSI output or the SLS API.
The instructions are exactly the same as the above except there will not be a hostname and a folder must be specified for configuration output using the --folder FOLDERNAME flag.</description>
    </item>
    
    <item>
      <title>Generate Switch Configs Including Custom Configurations</title>
      <link>/docs-csm/en-13/operations/network/management_network/canu/custom_config/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:37 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/management_network/canu/custom_config/</guid>
      <description>Generate Switch Configs Including Custom Configurations Pass in a switch config file that CANU will inject into the generated config. A use case would be to add custom site connections. This config file will overwrite previously generate config.
The custom config file type is YAML and a single file can be used for multiple switches. You will need to specify the switch name and what config inject. The custom config feature is using the hierarchical configuration library, documentation can be found here.</description>
    </item>
    
    <item>
      <title>Initializing CANu</title>
      <link>/docs-csm/en-13/operations/network/management_network/canu/initializing_canu/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:37 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/management_network/canu/initializing_canu/</guid>
      <description>Initializing CANU Initialize the CSM Automatic Network Utility (CANU) in order to help create the switch configurations. CANU can automatically parse CSI output or the Shasta System Layout Service (SLS) API for switch IPv4 addresses. Using the SLS API is only possible after the CSM install has been completed at least to the point where CSM Services have been deployed. Prior to that, parsing CSI output is the only option.</description>
    </item>
    
    <item>
      <title>Introduction To CANu</title>
      <link>/docs-csm/en-13/operations/network/management_network/canu/introduction_to_canu/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:37 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/management_network/canu/introduction_to_canu/</guid>
      <description>Introduction to CANU The CSM Automatic Network Utility (CANU) guides administrators through the installation of new Shasta networks. CANU helps ensure that the installation follows best practices and helps administrators set up a supported configuration.
The following are some of the tasks that CANU can perform:
 Check if the management switches on a Shasta network meet the firmware version requirements Check the cabling status of the management switches on a Shasta network using LLDP.</description>
    </item>
    
    <item>
      <title>Troubleshoot CANu Validation Errors</title>
      <link>/docs-csm/en-13/operations/network/management_network/canu/canu_validation_error/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:37 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/management_network/canu/canu_validation_error/</guid>
      <description>Troubleshoot CANU Validation Errors Typical CANU validation errors and how to fix them.
Example 1 validate_shcd - CRITICAL: A port number must be specified. Please correct the SHCD for HMN:V36 with an empty value SOLUTION: Blank cell. Minimally the Source or Destination and Port needs to be specified.
Example 2 Tab PDU not found in ./HPE System Hela CCD.revA27.xlsx Available tabs: [&amp;#39;Config. Summary&amp;#39;, &amp;#39;HPE Cables&amp;#39;, &amp;#39;RiverRackLayout &amp;#39;, &amp;#39;Arista&amp;#39;, &amp;#39;River Device Diagrams&amp;#39;, &amp;#39;HPE Devices&amp;#39;, &amp;#39;SCT pt_pt&amp;#39;, &amp;#39;yaml&amp;#39;, &amp;#39;Mountain-TDS-Management&amp;#39;, &amp;#39;MTN Rack Layout&amp;#39;, &amp;#39;10G_25G_40G_100G&amp;#39;, &amp;#39;NMN&amp;#39;, &amp;#39;HMN&amp;#39;, &amp;#39;PDU &amp;#39;] SOLUTION: PDU has an extra space in the tab name.</description>
    </item>
    
    <item>
      <title>Use CANu To Verify, Generate, Or Compare Switch Configurations</title>
      <link>/docs-csm/en-13/operations/network/management_network/canu/canu_verify_generate_compare_switch_configuration/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:37 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/management_network/canu/canu_verify_generate_compare_switch_configuration/</guid>
      <description>Use CANU to Verify, Generate, or Compare Switch Configurations  Common CANU Arguments  SHCD-Related Arguments CSI and SLS API Input to CANU  CSI Input SLS API Input     Check Single Switch Firmware Check Firmware of Multiple Switches JSON Output Check Single Switch Cabling Check Cabling of Multiple Switches Validate SHCD Validate Cabling Validate SHCD and Cabling Validate BGP Configuration Creation For BGP Generate Switch Configurations  Common CANU Arguments The following CANU flags are used for multiple different actions.</description>
    </item>
    
    <item>
      <title>Automatic Network Utility</title>
      <link>/docs-csm/en-13/operations/network/management_network/canu/readme/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:36 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/management_network/canu/readme/</guid>
      <description>CSM Automatic Network Utility CSM Automatic Network Utility (CANU) is a tool used to generate/validate/test the Shasta management network.
 Introduction to CANU Official Documentation Quick Start Guide Install CANU Update CANU From CSM Tarball Initialize CANU Verify, generate, or compare switch configurations Generate full network configuration Uninstall CANU CANU Validation Error  </description>
    </item>
    
    <item>
      <title>BiCAN Summary</title>
      <link>/docs-csm/en-13/operations/network/management_network/bican_technical_summary/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:36 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/management_network/bican_technical_summary/</guid>
      <description>BICAN Summary Bifurcated CAN was designed to separate administrative network traffic and user network traffic.
BICAN terminology  BICAN – Bifurcated Customer Access Network CAN – Customer Access Network CMN – Customer Management Network CHN – Customer High Speed Network NMN – Node Management Network HMN – Hardware Management Network  BICAN features  Bifurcation or splitting of the Customer Access Network (CAN) enables customization of customer traffic to and from the system.</description>
    </item>
    
    <item>
      <title>BiCAN Switch Configuration</title>
      <link>/docs-csm/en-13/operations/network/management_network/bican_switch_configuration/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:36 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/management_network/bican_switch_configuration/</guid>
      <description>BICAN switch configuration Validate cabling and generate switch configurations for BICAN.
Cabling requirements  Customer edge router is cabled to the HSN as described in the Slingshot documentation. Customer edge router is cabled to the management network. Below is an SHCD example of how the edge switches should be cabled to the management network. Note: the last line with the management ports will need to be excluded from CANU validate/generate. CANU currently does not support management ports.</description>
    </item>
    
    <item>
      <title>Bifurcating The CAN - Feature Details</title>
      <link>/docs-csm/en-13/operations/network/management_network/bican_technical_details/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:36 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/management_network/bican_technical_details/</guid>
      <description>Bifurcating the CAN - Feature Details  CAN new features overview Customer High Speed Network (CHN)  Accessible CHN system ingress endpoints Accessible CHN system egress endpoints Endpoint naming  Touchpoints: effects and changes When naming occurs Ability to change post-install   Endpoint addressing  Touchpoints: effects and changes When addressing occurs Ability to change post-install   Traffic separation and routing  Touchpoints: effects and changes When configuration occurs Ability to change post-install     Customer Management Management Network (CMN)  Traffic separation and routing Endpoint naming Endpoint addressing Changes  Touchpoints: effects and changes When configuration occurs Ability to change post install     Customer Access - external/site access  Traffic separation and routing Changes  Touchpoints: effects and changes When configuration occurs Ability to change post-install      1 CAN new features overview Bifurcation or splitting of the Customer Access Network (CAN) enables customization of customer traffic to and from the system.</description>
    </item>
    
    <item>
      <title>Bonded UAN Configuration</title>
      <link>/docs-csm/en-13/operations/network/management_network/bonded_uan/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:36 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/management_network/bonded_uan/</guid>
      <description>Bonded UAN Configuration This document shows how to configure the management network when bonded UANs are used. These configurations should be implemented via the custom configuration feature in CANU. There are two ways to implement bonded UAN configuration.
 25GB Bonded UAN Configuration (Most commonly used)  The only difference between this configuration and the default configuration is that OCP &amp;amp; PCIe port 1 are in a bond. These connections are plugged into a spine (TDS) or leaf (FULL) switch.</description>
    </item>
    
    <item>
      <title>Cable Management Network Servers</title>
      <link>/docs-csm/en-13/operations/network/management_network/cable_management_network_servers/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:36 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/management_network/cable_management_network_servers/</guid>
      <description>Cable Management Network Servers This topic describes nodes in the air-cooled cabinet with diagrams and pictures showing where to find the ports on the nodes and how to cable the nodes to the management network switches.
 HPE Hardware  HPE DL385 HPE DL325 HPE Worker Node Cabling HPE Master Node Cabling HPE Storage Node Cabling HPE UAN Cabling HPE Apollo 6500 XL645D HPE Apollo 6500 XL675D   Gigabyte/Intel Hardware  Worker Node Cabling Master Node Cabling Storage Node Cabling UAN Cabling    HPE Hardware HPE DL385  The OCP Slot is noted (number 7) in the image above.</description>
    </item>
    
    <item>
      <title>CANu Installation</title>
      <link>/docs-csm/en-13/operations/network/management_network/canu/canu_installation/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:36 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/management_network/canu/canu_installation/</guid>
      <description>CANU Installation Prerequisites In order to run CANU, both python3 and pip3 must be installed.
Installation   Install pip3, if it is not already installed.
pip3 install --editable   Install the development build of CANU.
python3 setup.py develop --user   Back to Index</description>
    </item>
    
    <item>
      <title>Backup A Custom Configuration</title>
      <link>/docs-csm/en-13/operations/network/management_network/backup_custom_configurations/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:35 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/management_network/backup_custom_configurations/</guid>
      <description>Backup a Custom Configuration Prerequisites  Access to the switches.  If you are doing a fresh install of CSM but previously had a different version of CSM installed, then you will need to backup/restore certain switch configurations after the switch has been wiped.
The backup needs to be done before wiping the switch.
This includes:
 Users and passwords SNMP credentials Site connections Interface speed commands Default routes Any other customized configuration for this system  This configuration will likely vary from site to site.</description>
    </item>
    
    <item>
      <title>BiCAN Support Matrix - Shasta Customer Access Networks</title>
      <link>/docs-csm/en-13/operations/network/management_network/bican_support_matrix/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:35 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/management_network/bican_support_matrix/</guid>
      <description>BICAN Support Matrix - Shasta Customer Access Networks Data sheet - Shasta networking - Customer Access Networks Overview Customer Access Networks (CANs) provide the interface between HPE Cray EX system networking and the customer site network. CANs are routed networks with broadcast domain separation. CANs provide higher availability and more flexibility in accessing cloud services compared to traditional &amp;ldquo;bastion hosts&amp;rdquo;, and are more in line with the cloud-native architecture of CSM.</description>
    </item>
    
    <item>
      <title>Erase All Zeroize</title>
      <link>/docs-csm/en-13/operations/network/management_network/aruba/zeroize/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:35 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/management_network/aruba/zeroize/</guid>
      <description>Erase All zeroize Erases customer data on the management modules in a secure manner. The command prompts for confirmation of zeroization.
Syntax erase all zeroize Example Erasing Customer Data on the Management Modules in a Secure Manner erase all zeroize This will securely erase all customer data and reset the switch to factory defaults. This will initiate a reboot and render the. switch unavailable until the zeroization is complete.This should take several minutes to one hour to complete.</description>
    </item>
    
    <item>
      <title>Switch Replacement In The Vsx Cluster</title>
      <link>/docs-csm/en-13/operations/network/management_network/aruba/vsx_switch_replacement/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:35 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/management_network/aruba/vsx_switch_replacement/</guid>
      <description>Switch Replacement in the VSX Cluster Replace the VSX primary or the VSX secondary with the following steps.
 Make sure all cables are labelled with clear identification. Unplug power and all fibers and copper cables. Un-rack the failing unit and rack the replacement unit. Power-up the unit. Restore switch firmware (see note below) and configuration. SSH/Console to the replacement switch and shutdown all ports.  Example: 8320, 8325 config Switch(config)# interface 1/1/1-1/1/52 shutdown  NOTE Restoring firmware is required only if replacing the primary VSX member.</description>
    </item>
    
    <item>
      <title>Vsx Sync</title>
      <link>/docs-csm/en-13/operations/network/management_network/aruba/vsx_sync/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:35 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/management_network/aruba/vsx_sync/</guid>
      <description>VSX Sync Configuration synchronization is one aspect of this VSX solution where the primary switch configuration is synced to the secondary switch. This allows for pseudo single pane of glass configuration and helps keep key configuration pieces in sync as operational changes are made. Since the solution is primarily for HA, it is expected that the vast majority of configuration policy is the same across both peers.
Configuration Commands Synchronize VLANs:</description>
    </item>
    
    <item>
      <title>Web User Interface (webui)</title>
      <link>/docs-csm/en-13/operations/network/management_network/aruba/web-ui/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:35 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/management_network/aruba/web-ui/</guid>
      <description>Web User Interface (WebUI) A web-based management user interface provides a visual representation of a subset of the current switch configuration and states. The Web-UI allows for easy access from modern browsers to modify some aspects of the configuration. The Web-UI also provides extensive access to the Network Analytics Engine. Many aspects of the hardware can be monitored in a dashboard view and customized.
Configuration Commands Enable the WebUI on a VRF:</description>
    </item>
    
    <item>
      <title>Verify Route To Tftp</title>
      <link>/docs-csm/en-13/operations/network/management_network/aruba/verify_route_to_tftp/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:34 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/management_network/aruba/verify_route_to_tftp/</guid>
      <description>Verify Route to TFTP On BOTH Aruba switches, a single route to the TFTP server 10.92.100.60 is needed. The configuration may differ on the system in use.
This is needed because there are issues with Aruba ECMP hashing and TFTP traffic.
show ip route 10.92.100.60 Example output:
Displaying ipv4 routes selected for forwarding &#39;[x/y]&#39; denotes [distance/metric] 10.92.100.60/32, vrf default, tag 0 via 10.252.1.9, [70/0], bgp This route can be a static route or a BGP route that is pinned to a single worker.</description>
    </item>
    
    <item>
      <title>Virtual Local Access Networks (vlans)</title>
      <link>/docs-csm/en-13/operations/network/management_network/aruba/vlan/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:34 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/management_network/aruba/vlan/</guid>
      <description>Virtual Local Access Networks (VLANs) VLANs allow for the logical grouping of switch interfaces, enabling communication as if all connected devices were on the same isolated network.
Configuration Commands Create VLAN:
vlan &amp;lt;VLAN&amp;gt; Configure an interface to associate it with a VLAN:
interface &amp;lt;IFACE&amp;gt; no shutdown no routing Configure an interface as an access port:
vlan access VLAN Configure an interface as a trunk port:
vlan trunk native &amp;lt;VLAN&amp;gt; vlan trunk allowed &amp;lt;VLAN&amp;gt; Configure VLAN as Voice:</description>
    </item>
    
    <item>
      <title>Virtual Switching Extension (vsx)</title>
      <link>/docs-csm/en-13/operations/network/management_network/aruba/vsx/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:34 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/management_network/aruba/vsx/</guid>
      <description>Virtual Switching Extension (VSX) Aruba&amp;rsquo;s Virtual Switching Extension (VSX) is a solution that integrates two independent ArubaOS-CX switches into an active/active virtualized high availability (HA) solution. The two switch peers utilize a connected link for control and data. This solution allows the switches to present as one virtualized switch in critical areas.
Configuration synchronization is one aspect of this VSX solution where the primary switch configuration is synced to the secondary switch.</description>
    </item>
    
    <item>
      <title>Virtual Switching Framework (vsf) - 6300 Only</title>
      <link>/docs-csm/en-13/operations/network/management_network/aruba/vsf/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:34 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/management_network/aruba/vsf/</guid>
      <description>Virtual Switching Framework (VSF) - 6300 Only Virtual Switching Framework (VSF) defines a virtual switch comprised of multiple individual physical switches, inter-connected through standard Ethernet links. These physical switches will operate with one control plane, thereby visible to the peers as a virtual switch stack.
Within the stack, one switch is the &amp;ldquo;Master&amp;rdquo; switch, which runs all the control plane software and manages the ASICs of all the stack members. A second switch can be configured as the &amp;ldquo;Standby&amp;rdquo; switch, which will take over as master if the master fails.</description>
    </item>
    
    <item>
      <title>Vlan Interface</title>
      <link>/docs-csm/en-13/operations/network/management_network/aruba/vlan_interface/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:34 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/management_network/aruba/vlan_interface/</guid>
      <description>VLAN Interface The switch also supports classic L3 VLAN interfaces.
Configuration Commands Configure the VLAN:
vlan VLAN Create and enable the VLAN interface, and assign it an IP address:
interface vlan VLAN ip address IP-ADDR/SUBNET no shutdown Show commands to validate functionality:
show vlan [VLAN|interface IFACE|summary] Example Output vlan 10 exit int 1/1/1 vlan access 10 int vlan 10 ip address 10.0.0.1/24 no shutdown end 108 bytes from 10.0.0.101: icmp_seq=4 ttl=64 time=2.</description>
    </item>
    
    <item>
      <title>Vlan Trunking 802.1q</title>
      <link>/docs-csm/en-13/operations/network/management_network/aruba/vlan_trunking_8021q/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:34 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/management_network/aruba/vlan_trunking_8021q/</guid>
      <description>VLAN Trunking 802.1Q A trunk port carries packets on one or more VLANs specified. Packet that ingress on a trunk port are in the VLAN specified in its 802.1Q header, or native VLAN if the packet has no 802.1Q header. A packet that egresses through a trunk port will have an 802.1Q header if it has a nonzero VLAN ID. Any packet that ingresses on a trunk port tagged with a VLAN that the port does not trunk is dropped.</description>
    </item>
    
    <item>
      <title>What Is Vsx?</title>
      <link>/docs-csm/en-13/operations/network/management_network/aruba/vsx_architecture/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:34 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/management_network/aruba/vsx_architecture/</guid>
      <description>What is VSX? Enterprise networks face the challenge of delivering 24x7 always-on reliable access and cloud-based services. As a business and network grows, this non-stop availability becomes more critical due to simple economics. Downtime leads to a loss of productivity, user satisfaction and revenue.
Switches in the campus and data center sit at the heart of the network and are responsible for the delivery of a high availability (HA) solution that is capable of ensuring always-on access with robust performance.</description>
    </item>
    
    <item>
      <title>Perform A Vsx Upgrade On Aruba Switches</title>
      <link>/docs-csm/en-13/operations/network/management_network/aruba/upgrade/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:33 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/management_network/aruba/upgrade/</guid>
      <description>Perform a VSX Upgrade on Aruba Switches The vsx update-software command is used to update the switch software with minimal to no downtime. This command gives administrators the option to save the running configuration on the primary and secondary VSX switches. After the command saves the running configuration, it downloads new software from the TFTP server and verifies the download. After a successful verification, the command installs the software to the alternative image of both the VSX primary and secondary switches.</description>
    </item>
    
    <item>
      <title>Typical Configuration Of Mclag Link</title>
      <link>/docs-csm/en-13/operations/network/management_network/aruba/typical_mclag_port_configuration/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:33 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/management_network/aruba/typical_mclag_port_configuration/</guid>
      <description>Typical Configuration of MCLAG Link The following is a very basic MCLAG link configuration connecting to NCNs. An administrators configuration may differ.
 NOTE The multi-chassis definition after the interface lag xx command. This is what defines the LAG to be able to peer both to Spine-01 and Spine-02.
 Spine-01
interface lag 1 multi-chassis no shutdown no routing vlan trunk native 1 vlan trunk allowed 1-2,4,7,10 lacp mode active lacp fallback spanning-tree bpdu-guard spanning-tree port-type admin-edge interface 1/1/1 no shutdown mtu 9198 lag 1 Spine-02</description>
    </item>
    
    <item>
      <title>Unidirectional Link Detection (udld)</title>
      <link>/docs-csm/en-13/operations/network/management_network/aruba/udld/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:33 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/management_network/aruba/udld/</guid>
      <description>Unidirectional Link Detection (UDLD) &amp;ldquo;The purpose of the UDLD protocol is to detect the presence of anomalous conditions in the Layer 2 communication channel, while relying on the mechanisms defined by the IEEE in the 802.3 standard to properly handle conditions inherent to the physical layer.&amp;rdquo; –rfc5171
 Compatible with existing HPE products:  Forward-then-verify: Packets are forwarded until the link is considered unidirectional Verify-then-forward: Packets are not forwarded until the link has been determined to be bidirectional   Compatible with RFC5171-compliant devices:  Normal: Determines link unidirectionality but will not block the port Aggressive: Once a port has been determined to be bidirectional and then becomes unidirectional, it will be blocked     NOTE The default UDLD mode is forward-then-verify.</description>
    </item>
    
    <item>
      <title>Verify BGP</title>
      <link>/docs-csm/en-13/operations/network/management_network/aruba/verify_bgp/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:33 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/management_network/aruba/verify_bgp/</guid>
      <description>Verify BGP Verify the BGP neighbors are in the established state on BOTH the switches.
Procedure   Check Aruba BGP status.
show bgp ipv4 u s Example output:
VRF : default BGP Summary ----------- Local AS : 65533 BGP Router Identifier : 10.252.0.3 Peers : 4 Log Neighbor Changes : No Cfg. Hold Time : 180 Cfg. Keep Alive : 60 Confederation Id : 0 Neighbor Remote-AS MsgRcvd MsgSent Up/Down Time State AdminStatus 10.</description>
    </item>
    
    <item>
      <title>Verify The DHCP Traffic On The Worker Nodes</title>
      <link>/docs-csm/en-13/operations/network/management_network/aruba/verify_dhcp_traffic_on_workers/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:33 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/management_network/aruba/verify_dhcp_traffic_on_workers/</guid>
      <description>Verify the DHCP Traffic on the Worker Nodes This section is an example issue of where the source address of the DHCP Offer is the Metallb address of KEA &amp;ldquo;10.92.100.222&amp;rdquo;.
The source address of the DHCP Reply/Offer MUST be the address of the VLAN interface on the worker node.
Use the following command to look at DHCP traffic on the workers:
tcpdump -envli bond0 port 67 or 68 Look for the source IP address of the DHCP Reply/Offer.</description>
    </item>
    
    <item>
      <title>Verify The Switches Are Forwarding DHCP Traffic</title>
      <link>/docs-csm/en-13/operations/network/management_network/aruba/verify-switches_are_forwarding_dhcp_traffic/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:33 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/management_network/aruba/verify-switches_are_forwarding_dhcp_traffic/</guid>
      <description>Verify the Switches are Forwarding DHCP Traffic If this point is reached and PXE booting is still not possible, it is likely the IP-Helper is broken on the switch.
Back to index.</description>
    </item>
    
    <item>
      <title>Confirm The Status Of The Cray-DHCP-kea Pods</title>
      <link>/docs-csm/en-13/operations/network/management_network/aruba/status_of_cray-dhcp-kea_pods/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:32 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/management_network/aruba/status_of_cray-dhcp-kea_pods/</guid>
      <description>Confirm the Status of the cray-dhcp-kea Pods Use this procedure to verify the status of the cray-dhcp-kea pods and services. The commands in this procedure must be run on ncn-w001 or a worker/manager NCN with kubectl installed.
Procedure   Check if the Kea DHCP services are running.
kubectl get -n services pods | grep kea The following services should be returned as output:
cray-dhcp-kea-api Cluster IP 10.31.247.201 &amp;lt;none&amp;gt; 8000/TCP 3h36m cray-dhcp-kea-tcp-hmn LoadBalancer 10.</description>
    </item>
    
    <item>
      <title>Secure Shell (SSH)</title>
      <link>/docs-csm/en-13/operations/network/management_network/aruba/ssh/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:32 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/management_network/aruba/ssh/</guid>
      <description>Secure Shell (SSH) SSH server enables an SSH client to make a secure and encrypted connection to a switch. Currently, switch supports SSH version 2.0 only. The user authentication mechanisms supported for SSH are public key authentication and password authentication (RADIUS, TACACS+ or locally stored password). Secure File Transfer Protocol (SFTP) provides file transfer. SSH Server and sftp-client via the copy command are supported for managing the router.
Configuration Commands Configure SSH authentication:</description>
    </item>
    
    <item>
      <title>Static Routing</title>
      <link>/docs-csm/en-13/operations/network/management_network/aruba/static_routing/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:32 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/management_network/aruba/static_routing/</guid>
      <description>Static Routing &amp;ldquo;Static routing is manually performed by the network administrator. The administrator is responsible for discovering and propagating routes through the network. These definitions are manually programmed in every routing device in the environment. After a device has been configured, it simply forwards packets out the predetermined ports. There is no communication between routers regarding the current topology of the network.&amp;rdquo; –IBM Redbook, TCP/IP Tutorial and Technical Overview
Configuration Commands switch(config)# &amp;lt;ip|ipv6&amp;gt; route IP-ADDR/&amp;lt;SUBNET|PREFIX&amp;gt; IP-ADDR Show commands to validate functionality:</description>
    </item>
    
    <item>
      <title>Tacacs</title>
      <link>/docs-csm/en-13/operations/network/management_network/aruba/tacacs/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:32 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/management_network/aruba/tacacs/</guid>
      <description>TACACS &amp;ldquo;TACACS+ provides access control for routers, network access servers and other networked computing devices via one or more centralized servers. TACACS+ provides separate authentication, authorization and accounting services.&amp;rdquo; –ietf draft-grant-tacacs-02
Configuration Commands Configure TACACS:
switch(config)# tacacs-server host IP-ADDR [key &amp;lt;plain|cipher&amp;gt;text KEY] Depending on the TACACS server, change the auth-type from PAP to CHAP:
switch(config)# tacacs-server auth-type [pap|chap] Configure AAA:
switch(config)# aaa authentication login default group tacacs local switch(config)# aaa authorization commands default group tacacs switch(config)# aaa accounting all default start-stop group tacacs Show commands to validate functionality:</description>
    </item>
    
    <item>
      <title>Test Tftp Traffic (aruba Only)</title>
      <link>/docs-csm/en-13/operations/network/management_network/aruba/test_tftp_traffic/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:32 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/management_network/aruba/test_tftp_traffic/</guid>
      <description>Test TFTP Traffic (Aruba Only) TFTP traffic can be tested by attempting to download the ipxe.efi binary.
Log into the leaf switch and try to download the iPXE binary.
This requires that the leaf switch can talk to the TFTP server &amp;ldquo;10.92.100.60&amp;rdquo;.
start-shell sw-leaf-001:~$ sudo su sw-leaf-001:/home/tftp 10.92.100.60 tftp&amp;gt; get ipxe.efi Received 1007200 bytes in 2.2 seconds tftp&amp;gt; get ipxe.efi Received 1007200 bytes in 2.2 seconds tftp&amp;gt; get ipxe.efi Received 1007200 bytes in 2.</description>
    </item>
    
    <item>
      <title>Typical Configuration Of Vsx</title>
      <link>/docs-csm/en-13/operations/network/management_network/aruba/typical_vsx_configuration/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:32 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/management_network/aruba/typical_vsx_configuration/</guid>
      <description>Typical Configuration of VSX The following is a very basic VSX configuration between two spine switches. Do note that the inter-switch-link (ISL) between the two spine switches is configured as regular lag, not a multi-chassis lag like a connected server would.
   Spine-01 vrf keepalive interface lag 254 no shutdown description ISL link no routing vlan trunk native 1 tag vlan trunk allowed all lacp mode active
interface 1/1/51 no shutdown mtu 9198 lag 254 interface 1/1/52 no shutdown mtu 9198 lag 254</description>
    </item>
    
    <item>
      <title>Typical Edge Port Configuration</title>
      <link>/docs-csm/en-13/operations/network/management_network/aruba/typical_edge_port_configuration/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:32 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/management_network/aruba/typical_edge_port_configuration/</guid>
      <description>Typical Edge Port Configuration The following is a very basic configuration for devices that are single homed to the network. For instance, network ILO cards, BMCs, PDUs, and so on.
   Leaf-01 interface 1/1/47 no shutdown mtu 9198 description HMN no routing vlan access 4 spanning-tree bpdu-guard spanning-tree port-type admin-edge     Leaf-02 interface 1/1/47 no shutdown mtu 9198 description BMC no routing vlan access 4 spanning-tree bpdu-guard spanning-tree port-type admin-edge    Back to Index</description>
    </item>
    
    <item>
      <title>Aruba Snmpv3 Users</title>
      <link>/docs-csm/en-13/operations/network/management_network/aruba/snmpv3_users/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:31 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/management_network/aruba/snmpv3_users/</guid>
      <description>Aruba SNMPv3 Users SNMPv3 supports cryptographic security through a combination of authenticating and encrypting the SNMP protocol packets over the network. Read-only access is currently supported. The admin user can add or remove SNMPv3 users.
Configuration Commands Configure a new SNMPv3 user (minimum eight characters for passwords):
switch(config)# snmpv3 user &amp;lt;USER&amp;gt; auth md5 auth-pass &amp;lt;A-PSWD&amp;gt; priv aes priv-pass &amp;lt;P-PSWD&amp;gt; Remove an SNMPv3 user:
switch(config)# no snmpv3 user &amp;lt;USER&amp;gt; Show commands to validate functionality:</description>
    </item>
    
    <item>
      <title>Simple Network Management Protocol (snmp) Agent</title>
      <link>/docs-csm/en-13/operations/network/management_network/aruba/snmp-agent/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:31 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/management_network/aruba/snmp-agent/</guid>
      <description>Simple Network Management Protocol (SNMP) Agent Simple Network Management Protocol (SNMP) minimizes the number and complexity of management functions. For monitoring and control, it is extensible to accommodate additional, possibly unanticipated aspects of network operation and management. SNMP is universal and independent of the architecture and mechanisms of particular hosts or particular gateways. SNMP server is supported either on the default or mgmt VRF.
Configuration Commands Enable SNMP agent:
switch(config)# snmp-server vrf VRF Configure the port to which the SNMP agent is bound:</description>
    </item>
    
    <item>
      <title>Snmp Traps</title>
      <link>/docs-csm/en-13/operations/network/management_network/aruba/snmp_trap/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:31 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/management_network/aruba/snmp_trap/</guid>
      <description>SNMP traps The SNMP agent can send trap notifications to a receiver. The receiver&amp;rsquo;s host IP address and port number can be defined along with the notification type, version, and community string.
Configuration Commands Configure a SNMPv2c trap receiver host:
switch(config)# snmp-server host IP-ADDR trap version v2c community xxx Show commands to validate functionality:
show snmp trap Example Output show snmp trap ------------------------------------------------------------------------------------------ Host Port Type Version SecName vrf ------------------------------------------------------------------------------------------ 1.</description>
    </item>
    
    <item>
      <title>Snmpv2c Community</title>
      <link>/docs-csm/en-13/operations/network/management_network/aruba/snmp-community/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:31 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/management_network/aruba/snmp-community/</guid>
      <description>SNMPv2c Community The switch supports SNMPv2c community-based security for read-only access.
Configuration Commands Configure an SNMPv2c community name:
switch(config)# snmp-server community NAME Bind the SNMP server to a VRF:
switch(config)# snmp-server vrf &amp;lt;default|VRF&amp;gt; Show commands to validate functionality:
show snmp community Example Output switch(config)# snmp-server community public switch(config)# snmp-server vrf default switch(config)# end show snmp community --------------------- SNMP communities --------------------- mysnmp show snmp vrf SNMP enabled VRF ---------------------------- default Expected Results  Administrators can configure the community name Administrators can bind the SNMP server to the default VRF Administrators can connect from the workstation using the community name  Back to Index</description>
    </item>
    
    <item>
      <title>Spine-leaf Architecture</title>
      <link>/docs-csm/en-13/operations/network/management_network/aruba/spine_leaf_architecture/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:31 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/management_network/aruba/spine_leaf_architecture/</guid>
      <description>Spine-Leaf Architecture The network design used in majority of HPE Cray EX installations is spine leaf architecture. In more sizeable systems, we also utilize super-spine to accommodate the number of spines that connect the network to provide additional HA capabilities.
What is Spine-Leaf Architecture? A spine-leaf architecture is data center network topology that consists of two switching layers—a spine and leaf. The leaf layer consists of access switches that aggregate traffic from servers and connect directly into the spine or network core.</description>
    </item>
    
    <item>
      <title>Spine-leaf Architecture</title>
      <link>/docs-csm/en-13/operations/network/management_network/aruba/spine_leaf_architecture2/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:31 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/management_network/aruba/spine_leaf_architecture2/</guid>
      <description>Spine-leaf Architecture How does a spine-leaf architecture differ from traditional network designs? Traditionally, data center networks were based on a three-tier model:
 Access switches connect to servers Leaf or distribution switches provide redundant connections to access switches Core switches provide fast transport between leaf switches, typically connected in a redundant pair for high availability  At the most basic level, a spine-leaf architecture collapses one of these tiers, as depicted in these diagrams.</description>
    </item>
    
    <item>
      <title>Connect The Management Network To A Campus Network</title>
      <link>/docs-csm/en-13/operations/network/management_network/aruba/requirements_and_optional_configuration/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:30 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/management_network/aruba/requirements_and_optional_configuration/</guid>
      <description>Connect the Management Network to a Campus Network There are several ways to connect an HPE Cray EX system directly to a campus network. In this guide, the two most typical ways of accomplishing this will be covered. The Scenario A and Scenario B examples will cover adding connections through the management network or high-speed network.
Requirements and optional configuration:
 System needs to be completely installed and running The edge router should be cabled either to the management network or Highspeed network switch An IP address range on the management or high-speed network switch that is routable to the campus network Other configuration items that may be required to facilitate remote connectivity:  Configuration may require a new LAG Configuration may require a new VLAN Configuration may require a new router OSPF context Other things to consider  ACL Stubby OSPF area Route restrictions i.</description>
    </item>
    
    <item>
      <title>Rebooting NCNs And PXE Fails</title>
      <link>/docs-csm/en-13/operations/network/management_network/aruba/reboot_pxe_fail/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:30 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/management_network/aruba/reboot_pxe_fail/</guid>
      <description>Rebooting NCNs and PXE Fails The following are common error messages when PXE fails:
2021-04-19 23:27:09 PXE-E18: Server response timeout. 2021-02-02 17:06:13 PXE-E99: Unexpected network error. Procedure   Verify the IP helper-address on VLAN 1 on the switches.
This is the same configuration as above &amp;ldquo;Aruba Configuration&amp;rdquo;.
Verify DHCP packets can be forwarded from the workers to the MTL network (VLAN1).
 If the worker nodes cannot reach the Metal (MTL) network DHCP will fail ALL WORKERS need to be able to reach the MTL network This can normally be achieved by having a default route    Run connectivity tests.</description>
    </item>
    
    <item>
      <title>Redundant Power Supplies</title>
      <link>/docs-csm/en-13/operations/network/management_network/aruba/redundant_power_supplies/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:30 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/management_network/aruba/redundant_power_supplies/</guid>
      <description>Redundant Power Supplies There are no configuration commands for switch power supply functionality.
 NOTE HA will be covered in HA section.
 Show commands to validate functionality:
show environment power-supply Expected Results  Validate the switch recognizes the additional power supplies Validate system remains powered after removing power from all but one power supply Validate all power supplies are operational  Example Output show environment power-supply Product Serial PSU Wattage Mbr/PSU Number Number Status --------------------------------------------------------- 1/1 JL372A M031SS004TAPC OK 2701 1/2 JL372A M031SS004UAPC OK 2430 1/3 N/A N/A Absent 0 1/4 N/A N/A Absent 0 Back to Index</description>
    </item>
    
    <item>
      <title>Remote Logging</title>
      <link>/docs-csm/en-13/operations/network/management_network/aruba/remote_logging/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:30 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/management_network/aruba/remote_logging/</guid>
      <description>Remote Logging Configure remote logging to view log files from the switch on a remote server. This functionality is enabled by syslog.
 NOTE The default facility is three (DAEMON).
 Configuration Commands Configure logging:
switch(config)# logging IP-ADDR Expected Results  Administrators can configure remote logging Administrators can see the log files from the switch on the remote server  Back to Index</description>
    </item>
    
    <item>
      <title>Routed Interfaces</title>
      <link>/docs-csm/en-13/operations/network/management_network/aruba/routed_interface/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:30 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/management_network/aruba/routed_interface/</guid>
      <description>Routed interfaces For platforms 8400 and 83xx: By default, all interfaces are configured as routed interfaces with support for both IPv4 and IPv6.
For platforms 6400 and 6300: By default, all interfaces are configured as access ports on VLAN 1
Configuration Commands Give an interface an IP address:
switch(config-if)# &amp;lt;ip|ipv6&amp;gt; address IP-ADDR/&amp;lt;SUBNET|PREFIX&amp;gt; Show commands to validate functionality:
show &amp;lt;ip|ipv6&amp;gt; interface IFACE Expected Results  Administrators are able to configure an IP address on the interface Administrators can configure an IP address on the connected network client The interface is up, and you can validate the IP address and subnet are correct Administrators can ping from the switch to the client and from the client to the switch  Back to Index</description>
    </item>
    
    <item>
      <title>Scenario A Network Connection Via Management Network</title>
      <link>/docs-csm/en-13/operations/network/management_network/aruba/scenario-a/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:30 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/management_network/aruba/scenario-a/</guid>
      <description>Scenario A: Network Connection via Management Network The example here covers outside connections achieved with the management network.
Summary  Create a new VRF Move interfaces to the new VRF Create a new BGP process for the new VRF Setup the edge router Configure MetalLB Verification step for BGP routes Configure default route for workers Verification of external communication  Topology The following is an example topology:
Back to Index</description>
    </item>
    
    <item>
      <title>Scenario B Network Connection Via High-speed Network</title>
      <link>/docs-csm/en-13/operations/network/management_network/aruba/scenario-b/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:30 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/management_network/aruba/scenario-b/</guid>
      <description>Scenario B: Network Connection via High-Speed Network This example covers outside connections achieved via highspeed network.
Summary  Create a new VRF Move interfaces to the new VRF Create a new BGP process for the new VRF Setup the edge router Configure MetalLB Verification step for BGP routes Configure default route for workers Verification of external communication  Topology The following is an example topology:
Back to Index</description>
    </item>
    
    <item>
      <title>Physical Interfaces</title>
      <link>/docs-csm/en-13/operations/network/management_network/aruba/physical_interfaces/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:29 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/management_network/aruba/physical_interfaces/</guid>
      <description>Physical Interfaces Configure the physical interfaces for a switch.
Configuration Commands Enable the interface:
switch(config)# interface IFACE switch(config-if)# no shutdown Show commands to validate functionality:
show interface IFACE [transceiver|brief|dom|extended] Expected Results  The switch recognizes the transceiver without errors Administrators can enter the interface context for the port and enable it Administrators can establish a link with a partner Administrators can pass traffic as expected  Back to Index</description>
    </item>
    
    <item>
      <title>Pim-sm Bootstrap Router (bsr) And Rendezvous Point (rp)</title>
      <link>/docs-csm/en-13/operations/network/management_network/aruba/pim/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:29 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/management_network/aruba/pim/</guid>
      <description>PIM-SM Bootstrap Router (BSR) and Rendezvous Point (RP) &amp;ldquo;Every PIM multicast group needs to be associated with the IP address of a Rendezvous Point (RP) [&amp;hellip;] For all senders to reach all receivers, it is crucial that all routers in the domain use the same mappings of group addresses to RP addresses. [&amp;hellip;] The BSR mechanism provides a way in which viable group-to-RP mappings can be created and rapidly distributed to all the PIM routers in a domain.</description>
    </item>
    
    <item>
      <title>Port Mirroring</title>
      <link>/docs-csm/en-13/operations/network/management_network/aruba/port_mirroring/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:29 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/management_network/aruba/port_mirroring/</guid>
      <description>Port Mirroring Port mirroring, also known as Switched Port Analyzer (SPAN), enables traffic on one or more switch interfaces to be replicated on another interface for purposes such as monitoring.
Configuration Commands Create and enable a mirror session:
switch(config)# mirror session &amp;lt;1-4&amp;gt; switch(config-mirror)# enable Configure a source interface:
switch(config-mirror)# source interface IFACE &amp;lt;both|tx|rx&amp;gt; Configure an interface as the mirror destination:
switch(config-mirror)# destination interface IFACE Configure a tunnel as the mirror destination (ERSPAN):</description>
    </item>
    
    <item>
      <title>Port Security</title>
      <link>/docs-csm/en-13/operations/network/management_network/aruba/port_security/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:29 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/management_network/aruba/port_security/</guid>
      <description>Port Security Port security allows user to do the following:
 Configure each switch port with a list of unique MAC addresses Limit network access to authorized MAC addresses Detect, prevent, and log unauthorized access of devices on individual ports Limit the number of MACs learned  Intrusion detection enables a device to notify the user or shutdown the port in the case of a violation, and a timer can be configured to allow auto-recovery of ports shutdown in a violation state to come back up after the timer expires.</description>
    </item>
    
    <item>
      <title>Queuing And Scheduling</title>
      <link>/docs-csm/en-13/operations/network/management_network/aruba/queuing_and_scheduling/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:29 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/management_network/aruba/queuing_and_scheduling/</guid>
      <description>Queuing and Scheduling When defining end-to-end behavior via CoS or DSCP, different priorities of traffic must be placed in different queues so the network device can service them appropriately. Separate queues allow delay- or jitter-sensitive traffic to be serviced before bulk or less time-critical traffic.
Queue policies configure which queues the different priorities of traffic will use. Queues are numbered in priority order, with zero being the lowest priority. The larger the queue number, the higher the priority of that queue.</description>
    </item>
    
    <item>
      <title>Radius</title>
      <link>/docs-csm/en-13/operations/network/management_network/aruba/radius/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:29 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/management_network/aruba/radius/</guid>
      <description>RADIUS RADIUS servers provide a method for remote users to access the switch. The following commands show how to configure a RADIUS server, and how remote users can authenticate and access the switch.
Configuration Commands Configure RADIUS server:
switch(config)# radius-server host IP-ADDR [key &amp;lt;plain|cipher&amp;gt;text KEY] [timeout VALUE] [port PORT] [auth-type TYPE] [acct-port PORT] [retries VALUE] [vrf VRF] [tracking &amp;lt;enable|disable&amp;gt;] Configure AAA:
switch(config)# aaa authentication login default group radius local switch(config)# aaa accounting all default start-stop group radius Show commands to validate functionality:</description>
    </item>
    
    <item>
      <title>NCNs On Install</title>
      <link>/docs-csm/en-13/operations/network/management_network/aruba/ncns_on_install/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:28 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/management_network/aruba/ncns_on_install/</guid>
      <description>NCNs on Install Use this procedure to verify the DNSMASQ config file on the NCNs is accurate.
Procedure   Verify the DNSMASQ config file matches what is configured on the switches.
The following is a DNSMASQ config file for the Metal network (VLAN1). The router is 10.1.0.1, which has to match what the IP address is on the switches doing the routing for the Metal (MTL) network.
Example MTL DNSMASQ file:</description>
    </item>
    
    <item>
      <title>Network Time Protocol (NTP) Client</title>
      <link>/docs-csm/en-13/operations/network/management_network/aruba/ntp/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:28 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/management_network/aruba/ntp/</guid>
      <description>Network Time Protocol (NTP) Client Summary of NTP from RFC-1305 Network Time Protocol (Version 3):
 NTP is used to synchronize timekeeping among a set of distributed time servers and clients &amp;hellip; It provides the protocol mechanisms to synchronize time in principle to precisions in the order of nanoseconds while preserving a non-ambiguous date well into the next century.
 The Network Time Protocol (NTP) client is essential for syncing time on various clients in the system.</description>
    </item>
    
    <item>
      <title>Network Topologies</title>
      <link>/docs-csm/en-13/operations/network/management_network/aruba/network_topologies/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:28 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/management_network/aruba/network_topologies/</guid>
      <description>Network Topologies The following images are example network topologies for systems of various sizes.
Very Large Large Medium Small </description>
    </item>
    
    <item>
      <title>Network Traffic Pattern</title>
      <link>/docs-csm/en-13/operations/network/management_network/aruba/network_traffic_pattern/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:28 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/management_network/aruba/network_traffic_pattern/</guid>
      <description>Network Traffic Pattern Internal networks:
 Node Management Network (NMN) - Provides the internal control plane for systems management and jobs control Hardware Management Network (HMN) - Provides internal access to system baseboard management controllers (BMC/iLO) and other lower-level hardware access  External and Edge networks:
 Customer Management Network (CMN) - Provides customer access from the site to the system for administrators Customer Access Network (CAN) or Customer High Speed Network (CHN) provide:  Customer access from the site to the system for job control and jobs data movement Access from the system to the site for network services like DNS, LDAP, and more    Back to index.</description>
    </item>
    
    <item>
      <title>Network Types – Naming And Segment Function</title>
      <link>/docs-csm/en-13/operations/network/management_network/aruba/network_naming_function/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:28 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/management_network/aruba/network_naming_function/</guid>
      <description>Network Types – Naming and Segment Function The following table provides an overview of the different network services defined inside of the spine and leaf architectures.
   *********** Administration: Hardware Administration: Cloud/Job Customer: Jobs Customer: Administration Storage     Full name Hardware Management Network Node Management Network Customer Access Network Customer Management Network Storage User Network   Short name / acronym HMN NMN CAN CMN SUN    Back to Index</description>
    </item>
    
    <item>
      <title>Notices</title>
      <link>/docs-csm/en-13/operations/network/management_network/aruba/notice/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:28 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/management_network/aruba/notice/</guid>
      <description>© 2021 Hewlett Packard Enterprise Development LP
Notices The information contained herein is subject to change without notice. The only warranties for Hewlett Packard Enterprise products and services are set forth in the express warranty statements accompanying such products and services. Nothing herein should be construed as constituting an additional warranty. Hewlett Packard Enterprise shall not be liable for technical or editorial errors or omissions contained herein.
Confidential computer software: Valid license from Hewlett Packard Enterprise required for possession, use, or copying.</description>
    </item>
    
    <item>
      <title>Open Shortest Path First (ospf) V2</title>
      <link>/docs-csm/en-13/operations/network/management_network/aruba/ospfv2/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:28 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/management_network/aruba/ospfv2/</guid>
      <description>Open Shortest Path First (OSPF) v2 &amp;ldquo;OSPF is a link-state based routing protocol. It is designed to be run internal to a single Autonomous System. Each OSPF router maintains an identical database describing the Autonomous System&amp;rsquo;s topology. From this database, a routing table is calculated by constructing a shortest-path tree. OSPF recalculates routes quickly in the face of topological changes, utilizing a minimum of routing protocol traffic. OSPF provides support for equal-cost multipath.</description>
    </item>
    
    <item>
      <title>Message-of-the-day (motd)</title>
      <link>/docs-csm/en-13/operations/network/management_network/aruba/motd/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:27 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/management_network/aruba/motd/</guid>
      <description>Message-Of-The-Day (MOTD) Banners are custom messages displayed to users attempting to connect to the management interfaces. MOTD banners are displayed pre-login while exec banners are displayed post-login. Multiple lines of text can be stored using a custom delimiter to mark the end of message.
Configuration Commands Create a banner:
switch(config)# banner &amp;lt;motd|exec&amp;gt; DELIM Show commands to validate functionality:
show banner &amp;lt;motd|exec&amp;gt; Example Output switch(config)# banner motd $ Enter a new banner, when you are done enter a new line containing only your chosen delimiter.</description>
    </item>
    
    <item>
      <title>Multi-chassis Link Aggregation Group (mclag)</title>
      <link>/docs-csm/en-13/operations/network/management_network/aruba/mlag/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:27 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/management_network/aruba/mlag/</guid>
      <description>Multi-Chassis Link Aggregation Group (MCLAG) Multi-Chassis Link Aggregation Group (MCLAG) is a link aggregation technique where two or more links across two switches are aggregated together to form a trunk.
Configuration Commands Create the MCLAG interface:
switch(config)# interface lag LAG multi-chassis switch(config-lag-if)# no shutdown Associate member links with the MCLAG interface:
switch(config)# interface IFACE switch(config-if)# no shutdown switch(config-if)# lag LAG Show commands to validate functionality:
show mclag &amp;lt;brief|configuration|status&amp;gt; Example Output switch(config)# interface lag 23 multi-chassis switch(config-lag-if)# no shutdown switch(config-lag-if)# exit switch(config)# interface 1/1/10 switch(config-if)# no shutdown switch(config-if)# lag 23 switch(config-if)# end Expected Results  Administrators can configure MCLAG Administrators can create an MCLAG interface Administrators can add ports to the MCLAG interface The output of the show commands is correct  Back to Index</description>
    </item>
    
    <item>
      <title>Multicast Source Discovery Protocol (msdp)</title>
      <link>/docs-csm/en-13/operations/network/management_network/aruba/msdp/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:27 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/management_network/aruba/msdp/</guid>
      <description>Multicast Source Discovery Protocol (MSDP) The Multicast Source Discovery Protocol (MSDP) describes a mechanism to connect multiple IP Version 4 Protocol Independent Multicast Sparse-Mode (PIM-SM) domains together. Each PIM-SM domain uses its own independent Rendezvous Point (RP) and does not have to depend on RPs in other domains. When an RP in a PIM-SM domain first learns of a new sender, e.g., via PIM register messages, it constructs a &amp;ldquo;Source-Active&amp;rdquo; (SA) message and sends it to its MSDP peers.</description>
    </item>
    
    <item>
      <title>Multiple Spanning Tree Protocol (mstp)</title>
      <link>/docs-csm/en-13/operations/network/management_network/aruba/mstp/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:27 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/management_network/aruba/mstp/</guid>
      <description>Multiple Spanning Tree Protocol (MSTP) MSTP (802.1s) ensures that only one active path exists between any two nodes in a spanning-tree instance. A spanning-tree instance comprises a unique set of VLANs. MSTP instances significantly improve network resource utilization while maintaining a loop-free environment.
Configuration Commands Enable MSTP (default mode for spanning-tree):
switch(config)# spanning-tree switch(config)# spanning-tree config-name &amp;lt;NAME&amp;gt; switch(config)# spanning-tree config-revision &amp;lt;VALUE&amp;gt; Configure an MSTP instance and priority switch(config)# spanning-tree instance VALUE vlan VLANS switch(config)# spanning-tree instance VALUE priority VALUE Show commands to validate functionality:</description>
    </item>
    
    <item>
      <title>Native Vlan</title>
      <link>/docs-csm/en-13/operations/network/management_network/aruba/native_vlan/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:27 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/management_network/aruba/native_vlan/</guid>
      <description>Native VLAN Untagged ingress packets are destined to the native VLAN. An interface can be configured in one of two native modes - Native-Untagged or Native-Tagged. A native-untagged port accepts any untagged or tagged (with native VLAN ID) traffic on ingress. Packets that egress on a native-untagged port in the native VLAN will not have an 802.1Q header. A native-tagged port accepts only tagged traffic (with native VLAN ID) on ingress.</description>
    </item>
    
    <item>
      <title>NCN Tcpdump</title>
      <link>/docs-csm/en-13/operations/network/management_network/aruba/ncn_tcpdump/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:27 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/management_network/aruba/ncn_tcpdump/</guid>
      <description>NCN tcpdump  Running a packet capture View traffic captured to a file Filter packet capture output  Running a packet capture (ncn-mw#) If a host is not getting an IP address, then run a packet capture to see if DHCP traffic is being transmitted.
 This example will look for DHCP traffic on interface bond0.nmn0. It will collect all DHCP traffic on ports 67 and 68, and write the output to a file named dhcp.</description>
    </item>
    
    <item>
      <title>Example Of How To Configure Scenario A Or B</title>
      <link>/docs-csm/en-13/operations/network/management_network/aruba/management_network_configuration_example/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:26 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/management_network/aruba/management_network_configuration_example/</guid>
      <description>Example of How to Configure Scenario A or B This section provides an example of how to configure the management network.
Procedure   (sw#) Create the Customer Access Network (CAN) VRF for Aruba.
config vrf CAN   (sw#) Move the interfaces into CAN VRF.
If there is an existing CAN interface configuration, it will be deleted the interface is moved into the new VRF; it will have to be re-applied.</description>
    </item>
    
    <item>
      <title>Management Interface</title>
      <link>/docs-csm/en-13/operations/network/management_network/aruba/management_interface/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:26 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/management_network/aruba/management_interface/</guid>
      <description>Management Interface The management interface can be used to gain remote management access to the switch. The management interface is accessible using the &amp;ldquo;mgmt&amp;rdquo; VRF and is separate from the data plane interfaces, which are in the &amp;ldquo;default&amp;rdquo; VRF.
Alternatively, a loopback interface can be configured to be used as management interface.
Address Mode Admin State Mac Address IPv4 address/subnet-mask Default gateway IPv4 IPv6 address/prefix IPv6 link local address/prefix: fe10::96f1:28ff:fe1d:a901/64 Default gateway IPv6 Primary Nameserver Secondary Nameserver : : 10.</description>
    </item>
    
    <item>
      <title>System Management Network Functions</title>
      <link>/docs-csm/en-13/operations/network/management_network/aruba/management_network_function_in_detail/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:26 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/management_network/aruba/management_network_function_in_detail/</guid>
      <description>System Management Network Functions The following is a description of the system management network functions:
  Edge - Any interactions with the Customer network or Internet
 Customer Jobs - Customer Access Network (CAN) User-facing cloud APIs  User Access Instances (UAIs)   Customer Administration - Customer Management Network (CMN)  Administrative Access to the system by Customer Admins Access from the system to external services:  Customer/Internet DNS LDAP authentication System installation and upgrade media (e.</description>
    </item>
    
    <item>
      <title>Vsx Isl Ha</title>
      <link>/docs-csm/en-13/operations/network/management_network/aruba/mclag_isl_ha/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:26 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/management_network/aruba/mclag_isl_ha/</guid>
      <description>VSX: ISL HA The intent here is to showcase an inter-switch-link (ISL) link failover scenario where one of the two links between spine switches goes down, but ISL is still connected with single link.
The following image is a visualization of disconnected ISL link:
The following things are expected to be seen in this scenario:
 After disconnecting one ISL, the VSX functionality should not be affected A small percentage of packets will be dropped when disconnecting the cable where traffic is flowing; A sub second value is expected during this event When connecting back the cable, the hashing needs to be recalculated and some packets may be dropped during this event as well; A sub second value is expected during this event  Back to Index</description>
    </item>
    
    <item>
      <title>Vsx Mclag Link Ha</title>
      <link>/docs-csm/en-13/operations/network/management_network/aruba/mclag_link_ha/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:26 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/management_network/aruba/mclag_link_ha/</guid>
      <description>VSX: MCLAG Link HA The intent here is to showcase a typical MCLAG link failover scenario, where one of the links goes to the edge device, whether that is the connected switch or server.
In the following image, a typical traffic pattern coming off from MCLAG connected device is shown. The traffic is going north to south and ISL is not carrying any traffic. The only time ISL will carry traffic is if one of the links to downstream devices is down.</description>
    </item>
    
    <item>
      <title>Vsx Member Power Failure</title>
      <link>/docs-csm/en-13/operations/network/management_network/aruba/mclag_power_failure/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:26 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/management_network/aruba/mclag_power_failure/</guid>
      <description>VSX: Member Power Failure The intent here is to showcase a complete member failure scenario where the spine-01 switch is completely down.
The following is a visualization of the powered down spine-01:
The following is expected in this scenario:
 After disconnecting the power supply from one member the other member should be able to detect the member is down and continue a normal operation without any problems. If traffic was originally flowing through the member that was shut down, a small percentage of packets may be dropped.</description>
    </item>
    
    <item>
      <title>Vsx Split</title>
      <link>/docs-csm/en-13/operations/network/management_network/aruba/mclag_split/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:26 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/management_network/aruba/mclag_split/</guid>
      <description>VSX: Split The intent here is to showcase a complete inter-switch-link (ISL) link failure scenario where both of the ISL links between spine switches goes down.
The following is a visualization of a disconnected ISL link and how the traffic pattern would look:
The following is expected in this scenario:
 After disconnecting both ISL Links and Keepalive is up and properly configured the VSX Secondary Switch should put all its MCLAGs into lacp-blocked state and traffic should only flow through VSX Primary.</description>
    </item>
    
    <item>
      <title>Key Features Used In The Management Network Configuration</title>
      <link>/docs-csm/en-13/operations/network/management_network/aruba/key_features/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:25 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/management_network/aruba/key_features/</guid>
      <description>Key Features Used in the Management Network Configuration The following is a list of key features used in the management network configuration.
Feature List    Feature Notes Description     VSX MLAG  Layer 2 Redundancy, Allows the NCNs to be bonded so if one link fails they can continue to operate.   VSX  Layer 3 Redundancy, Allows one Spine switch/default gateway to fail and continue to work   Lacp fallback  Allows for LACP links to come up individually without LACP PDUs, used for PXE booting the NCNs.</description>
    </item>
    
    <item>
      <title>Link Aggregation Group (lag)</title>
      <link>/docs-csm/en-13/operations/network/management_network/aruba/lag/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:25 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/management_network/aruba/lag/</guid>
      <description>Link Aggregation Group (LAG) Link Aggregation allows you to assign multiple physical links to one logical link that functions as a single, higher-speed link providing dramatically increased bandwidth.
Configuration Commands Create and configure the LAG interface:
switch(config)# interface lag LAG switch(config-lag-if)# no shutdown switch(config-lag-if)# lacp mode active Associate member links with the LAG interface:
switch(config)# interface IFACE switch(config-if)# no shutdown switch(config-if)# lag LAG Show commands to validate functionality:
show lacp &amp;lt;interfaces|aggregates|configuration&amp;gt; Example Output show interface lag1 Aggregate-name lag1 Aggregated-interfaces : 1/1/1 1/1/4 Aggregation-key : 1 Aggregate mode : active Speed 0 Mb/s qos trust none qos queue-profile default qos schedule-profile default RX TX 409 input packets 0 input error 0 CRC/FCS 530 output packets 0 input error 0 collision 47808 bytes 0 dropped 56975 bytes 0 dropped show lacp interfaces State abbreviations : A - Active P - Passive S - Short-timeout L - Long-timeout N - InSync O - OutofSync C - Collecting D - Distributing X - State m/c expired E - Default neighbor state Actor details of all interfaces: ------------------------------------------------------------------------------ Intf Aggregate Port Port State System-id System Aggr name id Priority Priority Key ------------------------------------------------------------------------------ 1/1/1lag1 59 1 ALFOE 70:72:cf:4d:bb:53 65534 1 1/1/4lag1 41 1 ALFOE 70:72:cf:4d:bb:53 65534 1 Partner details of all interfaces: ------------------------------------------------------------------------------ Intf Aggregate Partner Port State System-id System Aggr name Port-id Priority Priority Key ------------------------------------------------------------------------------ 1/1/1lag1 0 65534 PLFOEX 00:00:00:00:00:00 65534 0 1/1/4lag1 0 65534 PLFOEX 00:00:00:00:00:00 65534 0 show lacp aggregates Aggregate-name : lag1 Aggregated-interfaces : 1/1/1 1/1/4 Heartbeat rate : slow Aggregate mode : active F - Aggregable I - Individual Expected Results  Administrators can create and configure a LAG Administrators can add ports to a LAG Administrators can configure a LAG interface  Back to Index</description>
    </item>
    
    <item>
      <title>Link Layer Discovery Protocol (lldp)</title>
      <link>/docs-csm/en-13/operations/network/management_network/aruba/lldp/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:25 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/management_network/aruba/lldp/</guid>
      <description>Link Layer Discovery Protocol (LLDP) LLDP is used to advertise the device&amp;rsquo;s identity and abilities and read other devices connected to the same network.
 NOTE LLDP is enabled by default.
 Configuration Commands Enable an interface to receive or transmit LLDP packets:
switch(config-if)# lldp &amp;lt;receive|transmit&amp;gt; Show commands to validate functionality:
show lldp [local-device|neighbor-info|statistics] Example Output show lldp configuration LLDP Global Configuration: LLDP Enabled :Yes LLDP Transmit Interval :30 LLDP Hold time Multiplier :4 LLDP Transmit Delay Interval:2 LLDP Reinit time Interval :2 Optional TLVs configured: Management Address Port description Port VLAN-ID System capabilities System description System name LLDP Port Configuration: Port Tx-Enabled Rx-Enabled 1/1/1 Yes Yes .</description>
    </item>
    
    <item>
      <title>Locator Led</title>
      <link>/docs-csm/en-13/operations/network/management_network/aruba/locator_led/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:25 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/management_network/aruba/locator_led/</guid>
      <description>Locator LED The Locator LED is an LED in the front of the chassis that you can turn on or make flash. This is a handy feature when guiding someone to the switch during a &amp;ldquo;remote hands&amp;rdquo; situation, such as asking an engineer to run a cable to the switch.
Configuration Commands Enable LED:
led locator &amp;lt;flashing|off|on&amp;gt; Show commands to validate functionality:
show environment led Example Output show environment led Name State Status ----------------------------------- locator off ok led locator flashing show system led Name State Status ----------------------------------- locator flashing ok led locator on show system led Name State Status ----------------------------------- locator on ok led locator off show system led Name State Status ----------------------------------- locator off ok Expected Results  The Locator LED should be in the off state The Locator LED is now flashing The show command shows the LED is in the flashing state The Locator LED is lit a solid color and it does not flash The show command shows the LED is in the on state The LED is no longer lit The show command shows the LED is in the off state  Back to Index</description>
    </item>
    
    <item>
      <title>Loopback Interface</title>
      <link>/docs-csm/en-13/operations/network/management_network/aruba/loopback/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:25 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/management_network/aruba/loopback/</guid>
      <description>Loopback Interface Loopbacks are essentially internal virtual interfaces. Loopback interfaces are not bound to a physical port and are used for device management and routing protocols.
Configuration Commands switch(config)# interface loopback LOOPBACK switch(config-loopback-if)# ip address IP-ADDR/&amp;lt;SUBNET|PREFIX&amp;gt; Example Output switch(config)# interface loopback 1 switch(config-loopback-if)# ip address 99.99.99.1/32 switch(config-loopback-if)# end show run interface loopback1 interface loopback1 no shutdown ip address 99.99.99.1/32 exit show ip interface loopback1 Interface loopback1 is up Admin state is up Hardware: Loopback IPv4 address 99.</description>
    </item>
    
    <item>
      <title>Mac Authentication</title>
      <link>/docs-csm/en-13/operations/network/management_network/aruba/mac_auth/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:25 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/management_network/aruba/mac_auth/</guid>
      <description>MAC Authentication MAC Authentication (MAC Auth) is a method of authenticating devices for access to the network. The default mode of authentication is RADIUS, through which clients are authenticated by an external RADIUS server.
Configuration Commands Enter MAC Auth context:
switch(config)# aaa authentication port-access mac-auth Enable MAC Auth on all interfaces:
switch(config-macauth)# enable Configure MAC Auth MAC address format:
switch(config-macauth)# addr-format &amp;lt;no-delimiter|single-dash|multi-dash|multi-colon|no-delimiter Enable MAC Auth password:
switch(config-macauth)# password &amp;lt;plaintext|ciphertext&amp;gt; PASSWORD Configure mac-auth RADIUS authentication method:</description>
    </item>
    
    <item>
      <title>Check For Duplicate Ip Addresses</title>
      <link>/docs-csm/en-13/operations/network/management_network/aruba/duplicate_ip/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:24 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/management_network/aruba/duplicate_ip/</guid>
      <description>Check for Duplicate IP Addresses It is common to get an IP address that is not the correct one. A sign of a duplicate IP address is seeing a DECLINE message from the client to the server.
For example:
10.40.0.0.337 &amp;gt; 10.42.0.58.67: BOOTP/DHCP, Request from b4:2e:99:be:1a:d3, length 301, hops 1, xid 0x9d1210d, Flags [none] Gateway-IP 10.252.0.2 Client-Ethernet-Address b4:2e:99:be:1a:d3 Vendor-rfc1048 Extensions Magic Cookie 0x63825363 DHCP-Message Option 53, length 1: Decline Client-ID Option 61, length 19: hardware-type 255, 99:be:1a:d3:00:01:00:01:26:c8:55:c3:b4:2e:99:be:1a:d3 Server-ID Option 54, length 4: 10.</description>
    </item>
    
    <item>
      <title>Configure Exec Banners</title>
      <link>/docs-csm/en-13/operations/network/management_network/aruba/exec_banner/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:24 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/management_network/aruba/exec_banner/</guid>
      <description>Configure Exec Banners Exec banners are custom messages displayed to users attempting to connect to the management interfaces. Multiple lines of text can be stored using a custom delimiter to mark the end of message.
Configuration Commands Create a banner:
switch(config)# banner &amp;lt;motd|exec&amp;gt; DELIM Show commands to validate functionality:
show banner &amp;lt;motd|exec&amp;gt; Example Output switch(config)# banner exec $ Enter a new banner, when you are done enter a new line containing only your chosen delimiter.</description>
    </item>
    
    <item>
      <title>Configure Hostnames</title>
      <link>/docs-csm/en-13/operations/network/management_network/aruba/hostname/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:24 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/management_network/aruba/hostname/</guid>
      <description>Configure Hostnames A hostname is a human-friendly name used to identify a device. An example of a hostname could be the name &amp;ldquo;Test.&amp;rdquo;
Configuration Commands Create a hostname:
switch(config)# hostname &amp;lt;NAME&amp;gt; Show commands to validate functionality:
show hostname Example Output switch(config)# hostname switch-test show hostname switch-test Expected Results  Administrators can configure the hostname The output of all show commands is correct  Back to Index</description>
    </item>
    
    <item>
      <title>Configure Internet Group Multicast Protocol (igmp)</title>
      <link>/docs-csm/en-13/operations/network/management_network/aruba/igmp/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:24 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/management_network/aruba/igmp/</guid>
      <description>Configure Internet Group Multicast Protocol (IGMP) The Internet Group Multicast Protocol (IGMP) is a communications protocol used by hosts and adjacent routers on IP networks to establish multicast group memberships. The host joins a multicast-group by sending a join request message towards the network router, and responds to queries sent from the network router by dispatching a join report.
General notes:
 In ArubaOS-CX igmp snooping is disabled by default IGMP v3 is used by default, supported configuration allows v2 and v3  Configuration Commands switch(config)# interface vlan 1 switch(config-if-vlan)# igmp Expected Results show ip igmp-snooping vlan 1 should show IGMP enabled on the VLAN, but no IGMP Querier set.</description>
    </item>
    
    <item>
      <title>Initial Prioritization</title>
      <link>/docs-csm/en-13/operations/network/management_network/aruba/initial_prioritization/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:24 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/management_network/aruba/initial_prioritization/</guid>
      <description>Initial Prioritization For most switches, the local-priority has eight levels (0-7). Zero is the lowest priority. The allowed maximum will vary per product family. Local priority is used to determine which queue a packet will use. There are multiple options to configure the local-priority:
 qos cos-map: Maps Class of Services (CoS) values from VLAN tags in incoming packets to specific local priorities qos dscp-map: Maps the DSCP from incoming packets to specific local priorities qos trust: Assumes incoming packets are marked correctly, and takes the local-priority from either the CoS or Differentiated Service Code-Points (DSCP) field of the packet, or ignores any values set on incoming packets and places the packets into the default local-priority queue if the none option is given  Configuration Commands Map incoming 802.</description>
    </item>
    
    <item>
      <title>Introduction</title>
      <link>/docs-csm/en-13/operations/network/management_network/aruba/intro/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:24 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/management_network/aruba/intro/</guid>
      <description>Introduction The intent for the this documentation is to help install and manage Aruba CX network devices in an HPE Cray EX system installation.
The HPE Cray recommended way of configuring the network is with the CANU tool. Thus, this guide will not go to into a level of detail of how to configure each switch with the CLI in the topology. However, it will provide examples of how to configure and use features generated by CANU to provide administrators with an easy way to customize their installation.</description>
    </item>
    
    <item>
      <title>Check Kea DHCP Logs</title>
      <link>/docs-csm/en-13/operations/network/management_network/aruba/check_kea_dhcp_logs/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:23 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/management_network/aruba/check_kea_dhcp_logs/</guid>
      <description>Check KEA DHCP Logs Use this procedure to check the logs for a cray-dhcp-kea pod.
Procedure   Retrieve the pod name.
The pod name is required to check the logs for the pod in question.
kubectl logs -n services -l app.kubernetes.io/instance=cray-dhcp-kea -c cray-dhcp-kea Example output:
2020-08-03 21:47:50.580 INFO [kea-dhcp4.dhcpsrv/10] DHCPSRV_MEMFILE_LEASE_FILE_LOAD loading leases from file /cray-dhcp-kea-socket/dhcp4.leases 2020-08-03 21:47:50.580 INFO [kea-dhcp4.dhcpsrv/10] DHCPSRV_MEMFILE_LFC_SETUP setting up the Lease File Cleanup interval to 3600 sec 2020-08-03 21:47:50.</description>
    </item>
    
    <item>
      <title>Classifier Policies</title>
      <link>/docs-csm/en-13/operations/network/management_network/aruba/classifier_policies/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:23 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/management_network/aruba/classifier_policies/</guid>
      <description>Classifier Policies Classifier policies allow a network administrator to define sets of rules based on network traffic addressing or other header content and use these rules to restrict or alter the passage of traffic through the switch.
Choosing the rule criteria is called classification, and one such rule, or list, is called a policy.
Classification is achieved by creating a traffic class. There are three types of classes – MAC, IPv4, and IPv6 – which are each focused on relevant frame/packet characteristics.</description>
    </item>
    
    <item>
      <title>Configure Domain Name Service (DNS) Clients</title>
      <link>/docs-csm/en-13/operations/network/management_network/aruba/dns-client/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:23 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/management_network/aruba/dns-client/</guid>
      <description>Configure Domain Name Service (DNS) Clients The Domain Name Service (DNS) translates domain and host names to and from IP addresses. A DNS client resolves hostnames to IP addresses by querying assigned DNS servers for the appropriate IP address.
Configuration Commands Configure the switch to resolve queries via a DNS server:
ip dns server-address IP-ADDR [vrf VRF] Configure a domain name:
ip dns domain-name NAME Show commands to validate functionality:</description>
    </item>
    
    <item>
      <title>Configure Domain Names</title>
      <link>/docs-csm/en-13/operations/network/management_network/aruba/domain_name/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:23 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/management_network/aruba/domain_name/</guid>
      <description>Configure Domain Names A domain name is a name to identify the person, group, or organization that controls the devices within an area. An example of a domain name could be us.cray.com.
Configuration Commands Create a domain name:
switch(config)# domain-name NAME Show commands to validate functionality:
show domain-name Expected Results  Administrators can configure the domain name The output of all show commands is correct  Back to Index</description>
    </item>
    
    <item>
      <title>Large Number Of DHCP Declines During A Node Boot</title>
      <link>/docs-csm/en-13/operations/network/management_network/aruba/dhcp_decline/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:23 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/management_network/aruba/dhcp_decline/</guid>
      <description>Large Number of DHCP Declines During a Node Boot If something similar to the following is in the logs, then this indicates an issue that an IP address being allocated is already being used. It is not able to get the IP address assigned to the device.
dracut-initqueue[1902]: wicked: eth0: Declining DHCPv4 lease with address 10.252.0.56 dracut-initqueue[1902]: wicked: eth0: Declining DHCPv4 lease with address 10.252.0.57 dracut-initqueue[1902]: wicked: eth0: Declining DHCPv4 lease with address 10.</description>
    </item>
    
    <item>
      <title>Verify Computes/UANs/application Nodes</title>
      <link>/docs-csm/en-13/operations/network/management_network/aruba/compute_uan_application_nodes/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:23 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/management_network/aruba/compute_uan_application_nodes/</guid>
      <description>Verify Computes/UANs/Application Nodes If the computes make it past PXE and go into the PXE shell, verify DNS and connectivity.
iPXE&amp;gt; dhcp Configuring (net0 98:03:9b:a8:60:88).................. No configuration methods succeeded (http://ipxe.org/040ee186) Configuring (net1 b4:2e:99:be:1a:37)...... ok Procedure   Verify DNS:
iPXE&amp;gt; show dns Example output:
net1.dhcp/dns:ipv4 = 10.92.100.225   Verify connectivity:
iPXE&amp;gt; nslookup address api-gw-service-nmn.local iPXE&amp;gt; echo ${address} 10.92.100.71   Back to Index</description>
    </item>
    
    <item>
      <title>Bluetooth Capabilities</title>
      <link>/docs-csm/en-13/operations/network/management_network/aruba/bluetooth/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:22 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/management_network/aruba/bluetooth/</guid>
      <description>Bluetooth Capabilities The Bluetooth feature allows Bluetooth enabled devices to connect to and manage the switch on a wireless Bluetooth Personal Area Network (PAN). The user needs a supported USB Bluetooth dongle and to enable both the USB port and Bluetooth on the switch to use this feature. Bluetooth and REST write permissions for Bluetooth clients are both enabled by default.
Configuration Commands Turn on the USB port:
usb mount Enable Bluetooth:</description>
    </item>
    
    <item>
      <title>Border Gateway Protocol (BGP) Basics</title>
      <link>/docs-csm/en-13/operations/network/management_network/aruba/bgp_basic/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:22 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/management_network/aruba/bgp_basic/</guid>
      <description>Border Gateway Protocol (BGP) Basics &amp;ldquo;The primary function of a Border Gateway Protocol (BGP) speaking system is to exchange network reachability information with other BGP systems. This network reachability information includes information on the list of Autonomous Systems (ASes) that reachability information traverses. This information is sufficient for constructing a graph of AS connectivity for this reachability, from which routing loops may be pruned and, at the AS level, some policy decisions may be enforced.</description>
    </item>
    
    <item>
      <title>Cable Diagnostics</title>
      <link>/docs-csm/en-13/operations/network/management_network/aruba/cable_diagnostics/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:22 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/management_network/aruba/cable_diagnostics/</guid>
      <description>Cable Diagnostics Use the cable-diagnostic feature to test cables in the event where there might be a bad copper cable.
 NOTE This feature is only available on non-SFP copper ports.
 Procedure Enter diagnostics to open up the diagnostics menu:
diagnostics Once done, the diagnostics command set is now available for use, and the cable-diagnostics command can be executed:
diag cable-diagnostic &amp;lt;IFACE&amp;gt; Example output diagnostics &amp;lt;CR&amp;gt; diag ? asic ASIC diagnostics audit-failure-notification Configure audit failure notification bgp IP information cable-diagnostic Cable diagnostic test .</description>
    </item>
    
    <item>
      <title>Check BGP And Metallb</title>
      <link>/docs-csm/en-13/operations/network/management_network/aruba/check_bgp_and_metallb/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:22 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/management_network/aruba/check_bgp_and_metallb/</guid>
      <description>Check BGP and MetalLB Use the following procedure to verify if the spine switches are available and that MetalLB peering to the spine switches via Border Gateway Protocol (BGP) is established.
Prerequisites Access to the spine switches is required.
Procedure   Log in to the spine switches.
  Check that MetalLB is peering to the spines via BGP.
(sw-spine#) Check both spines if they are available (powered up):</description>
    </item>
    
    <item>
      <title>Check Current DHCP Leases</title>
      <link>/docs-csm/en-13/operations/network/management_network/aruba/check_current_dhcp_leases/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:22 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/management_network/aruba/check_current_dhcp_leases/</guid>
      <description>Check Current DHCP Leases Use the Kea API to retrieve data from the DHCP lease database.
Prerequisites An auth token is set up. If one has not been set up, log on to ncn-w001 or a worker/manager with kubectl and run the following:
export TOKEN=$(curl -s -k -S -d grant_type=client_credentials -d client_id=admin-client -d client_secret=`kubectl get secrets admin-client-auth -o jsonpath=&#39;{.data.client-secret}&#39; | base64 -d` https://api-gw-service-nmn.local/keycloak/realms/shasta/protocol/openid-connect/token | jq -r &#39;.access_token&#39;) Once an auth token is generated, these commands can be run on a worker or manager node.</description>
    </item>
    
    <item>
      <title>Check DHCP Lease Is Getting Allocated</title>
      <link>/docs-csm/en-13/operations/network/management_network/aruba/check_dhcp_lease_is_getting_allocated/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:22 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/management_network/aruba/check_dhcp_lease_is_getting_allocated/</guid>
      <description>Check DHCP Lease is Getting Allocated Checking (ncn-mw#) Check the Kea logs and verify that the DHCP lease is getting allocated.
KEA_POD=$(kubectl get pods -n services -l app.kubernetes.io/name=cray-dhcp-kea -o custom-columns=:.metadata.name --no-headers) echo &amp;#34;${KEA_POD}&amp;#34; kubectl logs -n services pod/&amp;#34;${KEA_POD}&amp;#34; -c cray-dhcp-kea The following example shows that Kea is allocating a lease to 10.104.0.23. The lease must say DHCP4_LEASE_ALLOC; if it says DHCP4_LEASE_ADVERT, then there is likely a problem.
2021-04-21 00:13:05.416 INFO [kea-dhcp4.</description>
    </item>
    
    <item>
      <title>Check HSM</title>
      <link>/docs-csm/en-13/operations/network/management_network/aruba/check_hsm/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:22 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/management_network/aruba/check_hsm/</guid>
      <description>Check HSM Hardware State Manager has two important parts:
 System Layout Service (SLS): This is the &amp;ldquo;expected&amp;rdquo; state of the system (as populated by networks.yaml and other sources). State Manager Daemon (SMD): This is the &amp;ldquo;discovered&amp;rdquo; or active state of the system during runtime.  Prerequisites  The API calls on this page require an authorization token to be set in the TOKEN variable. See Retrieve an Authentication Token. The cray CLI commands on this page require the Cray command line interface to be configured.</description>
    </item>
    
    <item>
      <title>802.1x</title>
      <link>/docs-csm/en-13/operations/network/management_network/aruba/8021x/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:21 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/management_network/aruba/8021x/</guid>
      <description>802.1X IEEE 802.1X is an IEEE standard for port-based network access control (PNAC). This standard provides an authentication mechanism to devices wishing to attach to a LAN or WLAN. IEEE 802.1X defines the encapsulation of the Extensible Authentication Protocol (EAP) over IEEE 802, which is known as EAP over LAN (EAPOL).
 NOTE Port security is a feature of &amp;ldquo;edge&amp;rdquo; switches such as 63/6400, and it is not available on 83xx.</description>
    </item>
    
    <item>
      <title>Access Control Lists (ACLs)</title>
      <link>/docs-csm/en-13/operations/network/management_network/aruba/acl/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:21 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/management_network/aruba/acl/</guid>
      <description>Access Control Lists (ACLs) ACLs are used to help improve network performance and restrict network usage by creating policies to eliminate unwanted IP traffic by filtering packets where they enter the switch on layer 2 and layer 3 interfaces. An ACL is an ordered list of one or more access control list entries (ACEs) prioritized by sequence number. An incoming packet is matched sequentially against each entry in an ACL.</description>
    </item>
    
    <item>
      <title>Address Resolution Protocol (arp)</title>
      <link>/docs-csm/en-13/operations/network/management_network/aruba/arp/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:21 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/management_network/aruba/arp/</guid>
      <description>Address Resolution Protocol (ARP) ARP is commonly used for mapping IPv4 addresses to MAC addresses.
Procedure   Configure static ARP on an interface.
switch(config-if)# arp ipv4 IP-ADDR mac MAC-ADDR   Show commands to validate functionality: .
show arp   Expected Results  Administrators are able to ping the connected device Administrators can view the ARP entries  Back to Index</description>
    </item>
    
    <item>
      <title>Apply Custom Switch Configuration 1.2</title>
      <link>/docs-csm/en-13/operations/network/management_network/apply_custom_config_1.2/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:21 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/management_network/apply_custom_config_1.2/</guid>
      <description>Apply Custom Switch Configuration CSM 1.2 Apply the backed up site connection configuration with a couple modifications. Since virtual routing and forwarding (VRF) is now used to separate customer traffic, the site ports and default routes must be added to that VRF.
Prerequisites   Access to the switches
  Custom switch configurations
 Backup Custom Configuration    Generated switch configurations already applied
 Apply Switch Configurations    Aruba Apply Configurations vrf attach Customer will be added to the port configuration that connects to the site.</description>
    </item>
    
    <item>
      <title>Apply Switch Configurations</title>
      <link>/docs-csm/en-13/operations/network/management_network/apply_switch_configurations/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:21 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/management_network/apply_switch_configurations/</guid>
      <description>Apply Switch Configurations This process is generally straightforward and requires the user to copy and paste the generated switch configuration into the terminal.
All ports will be shutdown before applying switch configuration. If the port is in the SHCD and being used, it will be enabled when the configuration is applied.
There are some caveats that are mentioned below.
Prerequisites  Switch without any configuration  Wipe Management Switches   Generated switch configurations  Generate Switch Configuration    Aruba   Shutdown all ports.</description>
    </item>
    
    <item>
      <title>Aruba Installation And Configuration Guide</title>
      <link>/docs-csm/en-13/operations/network/management_network/aruba/readme/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:21 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/management_network/aruba/readme/</guid>
      <description>Aruba Installation and Configuration Guide This documentation helps network administrators and support personnel install and manage Aruba network devices in a CSM install.
The HPE Cray recommended way of configuring the network is by using the CANU tool. Therefore this guide will not go into detail on how to configure each switch manually using the CLI. Instead, it will give helpful examples of how to configure/use features generated by CANU, in order to provide administrators easy ways to customize their installation.</description>
    </item>
    
    <item>
      <title>Backup A Switch Configuration</title>
      <link>/docs-csm/en-13/operations/network/management_network/aruba/backup/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:21 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/management_network/aruba/backup/</guid>
      <description>Backup a Switch Configuration Copies the running configuration or the startup configuration to a remote location as a file. The configuration can be exported to a file of either type CLI or type JSON format. The &amp;lt;VRF-NAME&amp;gt; is used for the configuration of interfaces on a particular VRF.
Procedure Create a copy of a running configuration or the startup configuration using the following command:
copy {running-config | startup-config} &amp;lt;REMOTE-URL&amp;gt; {cli | json} [vrf &amp;lt;VRF-NAME&amp;gt;] The parameters/syntax of the copy command are described below:</description>
    </item>
    
    <item>
      <title>Added Hardware</title>
      <link>/docs-csm/en-13/operations/network/management_network/added_hardware/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:20 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/management_network/added_hardware/</guid>
      <description>Added Hardware Follow this procedure when new hardware is added to the system.
Procedure   Validate the SHCD.
The SHCD defines the topology of a Shasta system, this is needed when generating switch configurations.
Refer to Validate the SHCD.
  Generate the switch configuration file(s).
Refer to Generate Switch Configs.
  Check the differences between the generated configurations and the configurations on the system.
Refer to Validate Switch Configs.</description>
    </item>
    
    <item>
      <title>Apply Custom Switch Configurations For 1.0</title>
      <link>/docs-csm/en-13/operations/network/management_network/apply_custom_config_1.0/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:20 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/management_network/apply_custom_config_1.0/</guid>
      <description>Apply Custom Switch Configurations for CSM 1.0 Apply the backed up site connection configuration with a couple modifications. Since virtual routing and forwarding (VRF) is now used to separate customer traffic, the site ports and default routes must be added to that VRF.
Prerequisites  Access to the switches Custom switch configurations  Backup Custom Configurations   Generated switch configurations already applied  Apply Switch Configurations    Aruba Apply Configurations conf t interface 1/1/36 no shutdown description to:CANswitch_cfcanb6s1-31:from:sw-25g01_x3000u39-j36 ip address 10.</description>
    </item>
    
    <item>
      <title>Management Network Upgrade 1.2 To 1.3</title>
      <link>/docs-csm/en-13/operations/network/management_network/1.2_to_1.3_upgrade/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:20 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/management_network/1.2_to_1.3_upgrade/</guid>
      <description>Management Network Upgrade CSM 1.2 to 1.3  Prerequisites Backup switch configurations Warnings Mellanox Dell Aruba Spine Aruba Leaf and Leaf BMC Run network tests  Prerequisites  System is currently running CANU-generated CSM 1.2 configurations. Switch configurations have been generated for CSM 1.3.  See Generate Switch Configurations. Ensure that site Custom Configurations used during installation have been retrieved or recreated.   CANU is installed with version 1.6.7 or greater.</description>
    </item>
    
    <item>
      <title>Management Network User Guide</title>
      <link>/docs-csm/en-13/operations/network/management_network/readme/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:20 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/management_network/readme/</guid>
      <description>Management Network User Guide This documentation helps network administrators and support personnel install install and manage Aruba, Dell, and Mellanox network devices in a CSM install.
The HPE Cray recommended way of configuring the network is by using the CANU tool. Therefore this guide will not go into detail on how to configure each switch manually using the CLI. Instead, it will give helpful examples of how to configure/use features generated by CANU, in order to provide administrators easy ways to customize their installation.</description>
    </item>
    
    <item>
      <title>Troubleshoot Connectivity To Services With External Ip Addresses</title>
      <link>/docs-csm/en-13/operations/network/external_dns/troubleshoot_systems_not_provisioned_with_external_ip_addresses/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:20 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/external_dns/troubleshoot_systems_not_provisioned_with_external_ip_addresses/</guid>
      <description>Troubleshoot Connectivity to Services with External IP addresses Systems that do not support CMN/CAN/CHN will not have services provisioned with external IP addresses on CMN/CAN/CHN. Kubernetes will report a &amp;lt;pending&amp;gt; status for the external IP address of the service experiencing connectivity issues.
If SSH access to a non-compute node (NCN) is available, it is possible to override resolution of external hostnames and forward local ports into the cluster for the cluster IP address of the corresponding service.</description>
    </item>
    
    <item>
      <title>Update The Cmn-external-DNS Value Post-installation</title>
      <link>/docs-csm/en-13/operations/network/external_dns/update_the_cmn-external-dns_value_post-installation/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:20 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/external_dns/update_the_cmn-external-dns_value_post-installation/</guid>
      <description>Update the cmn-external-dns value post-installation By default, the services/cray-dns-powerdns-cmn-tcp and services/cray-dns-powerdns-cmn-udp services both share the same Customer Management Network (CMN) external IP address. This is defined by the cmn-external-dns value, which is specified during the csi config init input.
The IP address must be in the static range reserved in MetalLB&amp;rsquo;s cmn-static-pool subnet. Currently, this is the only CMN IP address that must be known external to the system, in order for external DNS to delegate the system-name.</description>
    </item>
    
    <item>
      <title>External DNS</title>
      <link>/docs-csm/en-13/operations/network/external_dns/external_dns/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:19 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/external_dns/external_dns/</guid>
      <description>External DNS External DNS, along with the customer accessible networks CMN and CAN/CHN, PowerDNS, Border Gateway Protocol (BGP), and MetalLB, makes it simpler to access the HPE Cray EX API and system management services. Services are accessible directly from a laptop without needing to tunnel into a non-compute node (NCN) or override /etc/hosts settings. Some services may require a JSON Web Token (JWT) to access them, while others may require OAuth2 to login using a DC LDAP password.</description>
    </item>
    
    <item>
      <title>External DNS Csi Input Values</title>
      <link>/docs-csm/en-13/operations/network/external_dns/external_dns_csi_config_init_input_values/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:19 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/external_dns/external_dns_csi_config_init_input_values/</guid>
      <description>External DNS CSI Input Values External DNS requires the system-name, site-domain, and cmn-external-dns values that are defined with the csi config init command. These values are used to customize the External DNS configuration during installation.
The system-name and site-domain values The system-name and site-domain values specified as part of the csi config init are used together in the system-name.site-domain format, creating the external domain for external hostnames for services accessible from the Customer Management Network (CMN).</description>
    </item>
    
    <item>
      <title>External DNS Failing To Discover Services Workaround</title>
      <link>/docs-csm/en-13/operations/network/external_dns/external_dns_failing_to_discover_services_workaround/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:19 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/external_dns/external_dns_failing_to_discover_services_workaround/</guid>
      <description>External DNS Failing to Discover Services Workaround Many external DNS issues can be worked around by directly connecting to the desired backend service. This can circumvent authentication and authorization protections, but it may be necessary to access specific services when mitigating critical issues.
Istio&amp;rsquo;s ingress gateway uses Gateway and VirtualService objects to configure how traffic is routed to backend services. Currently, there are three gateways supporting the externally accessible services: services-gateway, customer-admin-gateway, and customer-user-gateway.</description>
    </item>
    
    <item>
      <title>Ingress Routing</title>
      <link>/docs-csm/en-13/operations/network/external_dns/ingress_routing/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:19 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/external_dns/ingress_routing/</guid>
      <description>Ingress Routing Ingress routing to services via Istio&amp;rsquo;s ingress gateway is configured by VirtualService custom resource definitions (CRD). When using external hostnames, there needs to be a VirtualService CRD that matches the external hostname to the desired destination.
For example, the configuration below controls the ingress routing for prometheus.cmn.SYSTEM_DOMAIN_NAME:
kubectl get vs -n sysmgmt-health cray-sysmgmt-health-prometheus Example output:
NAME GATEWAYS HOSTS AGE cray-sysmgmt-health-prometheus [services/services-gateway] [prometheus.cmn.SYSTEM_DOMAIN_NAME] 22h kubectl get vs -n sysmgmt-health cray-sysmgmt-health-prometheus -o yaml Example output:</description>
    </item>
    
    <item>
      <title>Troubleshoot Common DNS Issues</title>
      <link>/docs-csm/en-13/operations/network/dns/troubleshoot_common_dns_issues/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:19 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/dns/troubleshoot_common_dns_issues/</guid>
      <description>Troubleshoot Common DNS Issues The Domain Name Service (DNS) is part of an integrated infrastructure set designed to provide dynamic host discovery, addressing, and naming. There are several different place to look for troubleshooting as DNS interacts with Dynamic Host Configuration Protocol (DHCP), the Hardware Management Service (HMS), the System Layout Service (SLS), and the State Manager Daemon (SMD).
The information below describes what to check when experiencing issues with DNS.</description>
    </item>
    
    <item>
      <title>Troubleshoot DNS Configuration Issues</title>
      <link>/docs-csm/en-13/operations/network/external_dns/troubleshoot_dns_configuration_issues/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:19 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/external_dns/troubleshoot_dns_configuration_issues/</guid>
      <description>Troubleshoot DNS Configuration Issues Troubleshoot issues when DNS is not properly configured to delegate name resolution to the core DNS instance on a specific cluster. Although the CMN/CAN/CHN IP address may still be routable using the IP address directly, it may not work because Istio&amp;rsquo;s ingress gateway depends on the hostname (or SNI) to route traffic. For command line tools like cURL, using the --resolve option to force correct resolution can be used to work around this issue.</description>
    </item>
    
    <item>
      <title>Troubleshoot PowerDNS</title>
      <link>/docs-csm/en-13/operations/network/dns/troubleshoot_powerdns/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:19 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/dns/troubleshoot_powerdns/</guid>
      <description>Troubleshoot PowerDNS  List DNS zone contents PowerDNS logging Verify DNSSEC operation  Verify zones are being signed with the zone signing key Verify TSIG operation    List DNS zone contents The PowerDNS zone database is populated with data from two sources:
 The cray-powerdns-manager service creates the zones and DNS records based on data sourced from the System Layout Service (SLS). The external DNS records are populated by the cray-externaldns-external-dns service using data sourced from Kubernetes annotations and virtual service definitions.</description>
    </item>
    
    <item>
      <title>DHCP</title>
      <link>/docs-csm/en-13/operations/network/dhcp/dhcp/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:18 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/dhcp/dhcp/</guid>
      <description>DHCP The Dynamic Host Configuration Protocol (DHCP) service on the HPE Cray EX system uses the Internet Systems Consortium (ISC) Kea tool. Kea provides more robust management capabilities for DHCP servers.
For more information: https://www.isc.org/kea/.
The following improvements to the DHCP service are included:
 API access to manage DHCP Scalable pod that uses MetalLB instead of host networking Options for updates to HPE Cray EX management system IP addresses  DHCP Helper Workflow The DHCP-Helper uses the following workflow:</description>
    </item>
    
    <item>
      <title>Domain Name Service (DNS) Overview</title>
      <link>/docs-csm/en-13/operations/network/dns/dns/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:18 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/dns/dns/</guid>
      <description>Domain Name Service (DNS) Overview DNS Architecture This diagram shows how the various components of the DNS infrastructure interact.
DNS Components The DNS infrastructure is comprised of a number of components.
Unbound (cray-dns-unbound) Unbound is a caching DNS resolver which is also used as the primary DNS server.
The DNS records served by Unbound include system component names (xnames), node hostnames, and service names. These records are read from the cray-dns-unbound ConfigMap which is populated by cray-dns-unbound-manager.</description>
    </item>
    
    <item>
      <title>Enable Ncsd On UANs</title>
      <link>/docs-csm/en-13/operations/network/dns/enable_ncsd_on_uans/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:18 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/dns/enable_ncsd_on_uans/</guid>
      <description>Enable ncsd on UANs Configure User Access Nodes (UANs) to start the ncsd service at boot time.
The nscd service is not currently enabled by default and systemd does not start it at boot time. There are two ways to start nscd on UAN nodes: manually starting the service or enabling the service in the UAN image. While restarting nscd manually has to be performed each time the UAN is rebooted, enabling nscd in the image only has to be done once.</description>
    </item>
    
    <item>
      <title>Manage The DNS Unbound Resolver</title>
      <link>/docs-csm/en-13/operations/network/dns/manage_the_dns_unbound_resolver/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:18 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/dns/manage_the_dns_unbound_resolver/</guid>
      <description>Manage the DNS Unbound Resolver The unbound DNS instance is used to resolve names for the physical equipment on the management networks within the system, such as NCNs, UANs, switches, and compute nodes. This instance is accessible only within the HPE Cray EX system.
 Check the status of the cray-dns-unbound pods Unbound logs View manager (DNS Helper) logs Restart Unbound Clear bad data in the Unbound ConfigMap Change the site DNS server  Check the status of the cray-dns-unbound pods (ncn-mw#) Check the status of the pods:</description>
    </item>
    
    <item>
      <title>PowerDNS Configuration</title>
      <link>/docs-csm/en-13/operations/network/dns/powerdns_configuration/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:18 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/dns/powerdns_configuration/</guid>
      <description>PowerDNS Configuration External DNS PowerDNS replaces the CoreDNS server that earlier versions of CSM used to provide External DNS services.
The cray-dns-powerdns-can-tcp and cray-dns-powerdns-can-udp LoadBalancer resources are configured to service external DNS requests using the IP address specified by the CSI --cmn-external-dns command line argument.
The CSI --system-name and --site-domain command line arguments are combined to form the subdomain used for External DNS.
Site setup In the following example, the IP address 10.</description>
    </item>
    
    <item>
      <title>PowerDNS Migration Guide</title>
      <link>/docs-csm/en-13/operations/network/dns/powerdns_migration/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:18 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/dns/powerdns_migration/</guid>
      <description>PowerDNS Migration Guide The migration to PowerDNS as the authoritative DNS source and the introduction of Bifurcated CAN (Customer Access Network) will result in some changes to the node and service naming conventions.
DNS Record Naming Changes Fully qualified domain names will be introduced for all DNS records.
Canonical name: hostname.network-path.system-name.site-domain
 hostname - The hostname of the node or service network-path - The network path used to access the node  .</description>
    </item>
    
    <item>
      <title>Troubleshoot DHCP Issues</title>
      <link>/docs-csm/en-13/operations/network/dhcp/troubleshoot_dhcp_issues/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:18 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/dhcp/troubleshoot_dhcp_issues/</guid>
      <description>Troubleshoot DHCP Issues There are several things to check for when troubleshooting issues with Dynamic Host Configuration Protocol (DHCP) servers.
Incorrect DHCP IP addresses One of the most common issues is when the DHCP IP addresses are not matching in the Domain Name Service (DNS).
(ncn-mw#) Check to make sure cray-dhcp is not running in Kubernetes:
kubectl get pods -A | grep cray-dhcp Example output:
services cray-dhcp-5f8c8767db-hg6ch 1/1 Running 0 35d (ncn-mw#) If the cray-dhcp pod is running, use the following command to shut down the pod:</description>
    </item>
    
    <item>
      <title>Customer Access Networks</title>
      <link>/docs-csm/en-13/operations/network/customer_accessible_networks/can_to_chn/readme/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:17 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/customer_accessible_networks/can_to_chn/readme/</guid>
      <description>Customer Access Networks Customer access to Shasta services is provided by a Customer Access Network (CAN).
Enabling Customer High-speed Network Routing By default, the Customer Access Network (CAN) is provided by the Node Management Network (NMN). CSM 1.3 allows for the system to be optionally configured to allow the CAN to be provided by the High-speed Network (HSN). When customer access is provided by the HSN, this is called the Customer High-speed Network (CHN).</description>
    </item>
    
    <item>
      <title>Enabling Customer High Speed Network Routing</title>
      <link>/docs-csm/en-13/operations/network/customer_accessible_networks/can_to_chn/network/chn_enable/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:17 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/customer_accessible_networks/can_to_chn/network/chn_enable/</guid>
      <description>Enabling Customer High Speed Network Routing  Process overview and warnings Prerequisites Backup phase  Preparation Create system backups   Update phase  Disable CFS for UAN Update SLS Update customizations Update CSM service endpoint data (MetalLB)   Migrate phase  Migrate NCN workers Migrate CSM services (MetalLB) Migrate UAN Minimizing UAN downtime  Enable CFS for UAN Notify UAN users   Migrate UAI Migrate computes (optional)  Add compute IP addresses to CHN SLS data Upload migrated SLS file to SLS service Enable CFS layer     Cleanup phase  Remove CAN from SLS Remove CAN from customizations Remove CAN from BSS Remove CAN from CSM services Remove CAN interfaces from NCNs Remove CAN names from NCN hosts files   Update the management network Testing  Process overview and warnings IMPORTANT This procedure is quite involved and the complexities should be completely understood before beginning.</description>
    </item>
    
    <item>
      <title>Management Network Upgrade 1.2 To 1.3</title>
      <link>/docs-csm/en-13/operations/network/customer_accessible_networks/can_to_chn/network/network_upgrade_1.2_to_1.3/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:17 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/customer_accessible_networks/can_to_chn/network/network_upgrade_1.2_to_1.3/</guid>
      <description>Management Network Upgrade CSM 1.2 to 1.3  Management network changelog When to perform switch upgrades Prerequisites Upgrade CANU to the latest version Backup running switch configurations Retrieve SLS data Generate CSM 1.3 switch configurations Discover CSM 1.2 to CSM 1.3 configuration changes Test the network prior to update Apply and save the new switch configurations Test the network after update  Management network changelog CSM 1.3 includes full support for the Bifurcated CAN feature, specifically deployment of user traffic on the high speed network (CHN).</description>
    </item>
    
    <item>
      <title>SLS Utils Library</title>
      <link>/docs-csm/en-13/operations/network/customer_accessible_networks/can_to_chn/scripts/sls/sls_utils/readme/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:17 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/customer_accessible_networks/can_to_chn/scripts/sls/sls_utils/readme/</guid>
      <description>sls_utils Library This is a reusable Python library for safely interacting with SLS network data (in JSON format).
The library has been tested against Python version 3.6 and up.</description>
    </item>
    
    <item>
      <title>Bi-CAN Aruba/arista Configuration</title>
      <link>/docs-csm/en-13/operations/network/customer_accessible_networks/bi-can_arista_aruba_config/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:16 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/customer_accessible_networks/bi-can_arista_aruba_config/</guid>
      <description>BI-CAN Aruba/Arista Configuration This is an example configuration of how to connect two Aruba spine switches to two Arista switches. This example is from a running system utilizing the bifurcated CAN feature.
Summary:
 Two Aruba 8325 switches running in a VSX cluster. Two Arista 7060CX2-32S switches running MLAG. The Arista switches are connected to the Slingshot/HSN network via static MLAG. The Aruba Spine switches are connected to the Arista switches with point-to-point OSPF links.</description>
    </item>
    
    <item>
      <title>CAN/cmn With Dual-spine Configuration</title>
      <link>/docs-csm/en-13/operations/network/customer_accessible_networks/dual_spine_configuration/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:16 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/customer_accessible_networks/dual_spine_configuration/</guid>
      <description>CAN/CMN with Dual-Spine Configuration The Customer Access Network (CAN) and Customer Management Network (CMN) needs to be connected to both spines in a dual-spine configuration so that each spine can access the outside network. However, the NCNs should only have one default gateway. Therefore, the multi-active gateway protocol (MAGP) on the Mellanox spines can be used to create a virtual router gateway IP address that can direct to either of the spines, depending on the state of the spines.</description>
    </item>
    
    <item>
      <title>Customer Accessible Networks</title>
      <link>/docs-csm/en-13/operations/network/customer_accessible_networks/customer_accessible_networks/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:16 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/customer_accessible_networks/customer_accessible_networks/</guid>
      <description>Customer Accessible Networks There are generally two networks accessible by devices outside of the CSM cluster. One network is for administrators managing the cluster and one is for users accessing user services provided by the cluster.
Customer Management Network The Customer Management Network (CMN) provides access from outside the customer network to administrative services and non-compute nodes (NCNs). This allows for the following:
 Administrator clients outside of the system:  Log in to NCNs.</description>
    </item>
    
    <item>
      <title>Externally Exposed Services</title>
      <link>/docs-csm/en-13/operations/network/customer_accessible_networks/externally_exposed_services/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:16 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/customer_accessible_networks/externally_exposed_services/</guid>
      <description>Externally Exposed Services The following services are exposed on one or more of the external networks (CMN, CAN, and CHN). Each of these services requires an IP address in the relevant subnets so they are reachable on that network. This IP address is allocated by the MetalLB component.
Services under Istio Ingress Gateway and OAuth2 Proxy Ingress share an ingress, so they all use the IP allocated to the Ingress.</description>
    </item>
    
    <item>
      <title>Metallb Peering With Arista Edge Router</title>
      <link>/docs-csm/en-13/operations/network/customer_accessible_networks/bi-can_arista_metallb_peering/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:16 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/customer_accessible_networks/bi-can_arista_metallb_peering/</guid>
      <description>MetalLB Peering with Arista Edge Router This is an example configuration of how to connect a pair of Arista switches to MetalLB running inside of Kubernetes.
Prerequisites  Pair of Arista switches already connected to the high-speed network. Updated System Layout Service (SLS) file that has the CHN network configured.  Example Configuration Below is a snippet from an upgraded SLS.
&amp;#34;CHN&amp;#34;: { &amp;#34;Name&amp;#34;: &amp;#34;CHN&amp;#34;, &amp;#34;FullName&amp;#34;: &amp;#34;Customer High-Speed Network&amp;#34;, &amp;#34;IPRanges&amp;#34;: [ &amp;#34;10.</description>
    </item>
    
    <item>
      <title>Troubleshoot Cmn Issues</title>
      <link>/docs-csm/en-13/operations/network/customer_accessible_networks/troubleshoot_cmn_issues/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:16 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/customer_accessible_networks/troubleshoot_cmn_issues/</guid>
      <description>Troubleshoot CMN issues Various connection points to check when using the CMN and how to fix any issues that arise.
The most frequent issue with the Customer Management Network (CMN) is trouble accessing IP addresses outside of the HPE Cray EX system from a node or pod inside the system.
The best way to resolve this issue is to try to ping an outside IP address from one of the NCNs other than ncn-m001, which has a direct connection that it can use instead of the Customer Management Network (CMN).</description>
    </item>
    
    <item>
      <title>Connect To The Cmn And CAN</title>
      <link>/docs-csm/en-13/operations/network/customer_accessible_networks/connect_to_the_cmn_can/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:15 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/customer_accessible_networks/connect_to_the_cmn_can/</guid>
      <description>Connect to the CMN and CAN How to connect to the CMN and CAN physically and via layer 3.
There are multiple ways to connect to the Customer Management Network (CMN) and Customer Access Network (CAN), both physically and via a layer 3 connection.
Physical Connection to the CMN and CAN The physical connection to the CMN and CAN is made via the load balancer or the spine switches. The uplink connection from the system to the customer network is achieved by using the highest numbered port(s).</description>
    </item>
    
    <item>
      <title>Connect To The HPE Cray Ex Environment</title>
      <link>/docs-csm/en-13/operations/network/connect_to_the_hpe_cray_ex_environment/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:15 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/connect_to_the_hpe_cray_ex_environment/</guid>
      <description>Connect to the HPE Cray EX Environment The HPE Cray EX Management Network (SMNet) has multiple separate physical and logical links that are used to segregate traffic.
The diagram below shows the available connections from within the SMNet, as well as the connections to the customer network:
There are multiple ways to connect to the HPE Cray EX environment. The various methods are described in the following table:
   Role Description     Administrative External customer network connection to the worker node&amp;rsquo;s hardware management and administrative port   Application node access External customer network connection to an Application Node   Customer Access Network (CAN) Customer connection to the CAN gateway to access the HPE Cray EX CAN    There are also several ways to physically connect to the nodes on the system.</description>
    </item>
    
    <item>
      <title>Create A Configuration Upgrade Plan</title>
      <link>/docs-csm/en-13/operations/network/create_a_csm_configuration_upgrade_plan/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:15 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/create_a_csm_configuration_upgrade_plan/</guid>
      <description>Create a CSM Configuration Upgrade Plan Creating an upgrade plan is unique and dependent on the requirements of the upgrade path. Some release versions of the network configuration require coupled upgrade of software to enable new software functionality, or bug fixes that may add time required to do the full upgrade.
For example, in CSM release 1.2, Aruba and Mellanox switches are being upgraded to newer code.
In this case and cases where configuration changes are extensive, consider taking the generated configurations after review and uploading them to the switches startup config prior to booting to new code to upgrade both configuration and software simultaneously.</description>
    </item>
    
    <item>
      <title>Default Ip Address Ranges</title>
      <link>/docs-csm/en-13/operations/network/default_ip_address_ranges/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:15 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/default_ip_address_ranges/</guid>
      <description>Default IP Address Ranges The initial installation of the system creates default networks with default settings and with no external exposure. These IP address default ranges ensure that no nodes in the system attempt to use the same IP address as a Kubernetes service or pod, which would result in undefined behavior that is extremely difficult to reproduce or debug.
The following table shows the default IP address ranges:
   Network IP Address Range     Kubernetes service network 10.</description>
    </item>
    
    <item>
      <title>Gateway Testing</title>
      <link>/docs-csm/en-13/operations/network/gateway_testing/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:15 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/gateway_testing/</guid>
      <description>Gateway Testing With the introduction of BiCAN, service APIs are now available on one or more networks depending on who is allowed access to the services and from where. The services are accessed via three different ingress gateways using a token that can be retrieved from Keycloak.
This page describes how to run a set of tests that determine if the gateways are functioning properly. The gateway test will obtain an API token from Keycloak and then use that token to attempt to access a set of service APIs on one or more networks, as defined in the gateway test definition file (gateway-test-defn.</description>
    </item>
    
    <item>
      <title>Network</title>
      <link>/docs-csm/en-13/operations/network/network/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:15 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/network/</guid>
      <description>Network There are several different networks supported by the HPE Cray EX system. The following are the available internal and external networks, as well as the devices that connect to each network:
 Networks external to the system:  Customer Network (Data Center)  ncn-m001 BMC is connected by the customer network switch to the customer management network All NCNs (worker, master, and storage) are connected ClusterStor System Management Unit (SMU) interfaces User Access Nodes (UANs)     System networks:  Customer Network (Data Center)  ncn-m001 BMC is connected by the customer network switch to the customer management network ClusterStor SMU interfaces User Access Nodes (UANs)   Hardware Management Network (HMN)  BMCs for Admin tasks Power distribution units (PDU) Keyboard/video/mouse (KVM)   Node Management Network (NMN)  All NCNs and compute nodes User Access Nodes (UANs)   ClusterStor Management Network  ClusterStor controller management interfaces of all ClusterStor components (SMU, Metadata Management Unit (MMU), and Scalable Storage Unit (SSU))   High-Speed Network (HSN), which connects the following devices:  Kubernetes worker nodes UANs ClusterStor controller data interfaces of all ClusterStor components (SMU, MMU, and SSU) There must be at least two NCNs whose BMCs are on the HMN.</description>
    </item>
    
    <item>
      <title>Using The Cmn For The Chn BGP Control Plane</title>
      <link>/docs-csm/en-13/operations/network/customer_accessible_networks/chn_bgp_control_plane_over_cmn/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:15 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/customer_accessible_networks/chn_bgp_control_plane_over_cmn/</guid>
      <description>Using the CMN for the CHN BGP control plane Summary In a typical CHN configuration BGP peering is done over the CHN. This means that the BGP control plane packets are sent over the High speed network on the CHN. In some cases, administrators may want to use the CMN as the BGP control plane.
The following example shows how to move the BGP control plane from the CHN to the CMN.</description>
    </item>
    
    <item>
      <title>Access To System Management Services</title>
      <link>/docs-csm/en-13/operations/network/access_to_system_management_services/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:14 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/access_to_system_management_services/</guid>
      <description>Access to System Management Services The standard configuration for System Management Services (SMS) is the containerized REST micro-service with a public API. All of the micro-services provide an HTTP interface and are collectively exposed through a single gateway URL. The API gateway for the system is available at a well known URL based on the domain name of the system. It acts as a single HTTPS endpoint for terminating Transport Layer Security (TLS) using the configured certificate authority.</description>
    </item>
    
    <item>
      <title>Connect To Switch Over Usb-serial Cable</title>
      <link>/docs-csm/en-13/operations/network/connect_to_switch_over_usb_serial_cable/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:14 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/network/connect_to_switch_over_usb_serial_cable/</guid>
      <description>Connect to Switch over USB-Serial Cable In the event that network plumbing is lacking, down, or unconfigured for procuring devices, then it is recommended to use the Serial/COM ports on the management switches.
This guide will instruct the user on procuring MAC addresses for the NCNs metadata files with the serial console.
Mileage may vary, as some obstacles such as BAUDRATE and terminal usage vary per manufacturer.
Common Manufacturers Refer to the external support/documentation portals for more information:</description>
    </item>
    
    <item>
      <title>Removing A Tenant</title>
      <link>/docs-csm/en-13/operations/multi_tenancy/remove_a_tenant/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:14 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/multi_tenancy/remove_a_tenant/</guid>
      <description>Removing a Tenant  Overview Remove the Slurm operator Custom Resource (CR) Delete the tenant&amp;rsquo;s Custom Resource (CR)  Overview This page provides describes how an infrastructure administrator (not a tenant administrator) can remove a tenant when appropriate.
IMPORTANT Removing a tenant is a permanent operation and cannot be reverted. Removing a tenant will remove all tenant-related namespaces from Kubernetes, along with any Kubernetes resources in those namespaces.
Remove the Slurm operator Custom Resource (CR) (ncn-mw#) To remove a Slurm tenant instance, delete the custom resource with kubectl:</description>
    </item>
    
    <item>
      <title>Slurm Operator</title>
      <link>/docs-csm/en-13/operations/multi_tenancy/slurmoperator/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:14 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/multi_tenancy/slurmoperator/</guid>
      <description>Slurm Operator  Overview Install the Slurm operator Troubleshooting  Overview The Slurm operator can be used to deploy Slurm within a tenant, so each tenant can have a separate instance of Slurm. This page explains how to install the Slurm operator.
Install the Slurm operator The Slurm operator must be installed in order to create Slurm tenants. The Slurm operator runs in a Kubernetes pod and watches for SlurmCluster custom resources.</description>
    </item>
    
    <item>
      <title>Tapms (tenant And Partition Management System) Overview</title>
      <link>/docs-csm/en-13/operations/multi_tenancy/tapms/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:14 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/multi_tenancy/tapms/</guid>
      <description>TAPMS (Tenant and Partition Management System) Overview  Overview Tenant schema Reconcile operations Tenant states  Overview tapms is the primary Kubernetes operator through which tenant creation and management is handled. This operator interacts with several other services in the CSM software stack to provision the necessary components for a given tenant. This document gives an overview of its functionality.
Tenant schema See the Tenant Custom Resource Definition for the full schema.</description>
    </item>
    
    <item>
      <title>Tenant Administrator Configuration</title>
      <link>/docs-csm/en-13/operations/multi_tenancy/tenantadminconfig/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:14 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/multi_tenancy/tenantadminconfig/</guid>
      <description>Tenant Administrator Configuration  Overview Kubernetes OIDC API integration Tenant-specific Keycloak groups Roles and Rolebindings Retrieve an OIDC token Using kubelogin  Overview This page describes how to configure a user as a Tenant Administrator, allowing that person to perform administrative functions on one or more tenants, without giving them the same permissions an Infrastructure Administrator would have.
Kubernetes OIDC API integration When CSM is installed, the Keycloak Helm chart will run a setup script which creates a Keycloak client for OIDC integration.</description>
    </item>
    
    <item>
      <title>Cray Hnc Manager</title>
      <link>/docs-csm/en-13/operations/multi_tenancy/crayhncmanager/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:13 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/multi_tenancy/crayhncmanager/</guid>
      <description>Cray HNC Manager  Overview Terminology Tenant naming requirements kubectl HNS plugin Resource propagation  Overview The HNC controller is deployed as part of the multi-tenancy solution for namespace management and object propagation. This controller is deployed in CSM with the cray-hnc-manager Helm chart. This HNC controller is only managing the following namespaces related to multi-tenancy:
 multi-tenancy slurm-operator tapms-operator tenants Any namespaces created for a given tenant  Generally, operations for managing tenants do not require interacting explicitly with the HNC controller, aside from the initial configuration.</description>
    </item>
    
    <item>
      <title>Creating A Tenant</title>
      <link>/docs-csm/en-13/operations/multi_tenancy/create_a_tenant/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:13 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/multi_tenancy/create_a_tenant/</guid>
      <description>Creating a Tenant  Overview TAPMS CRD Apply the TAPMS CR slurm operator CRD Apply the slurm operator CR  Overview This page provides information about how to create a tenant. This procedure involves creating a Custom Resource Definition (CRD) and then applying the Custom Resource (CR), for both tapms and the slurm operator.
TAPMS CRD Tenant provisioning is handled in a declarative fashion, by creating a CR with the specification for the tenant.</description>
    </item>
    
    <item>
      <title>Kubernetes Encryption</title>
      <link>/docs-csm/en-13/operations/kubernetes/encryption/readme/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:13 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/kubernetes/encryption/readme/</guid>
      <description>Kubernetes Encryption Beginning in CSM 1.3, support is enabled for data encryption in etcd for Kubernetes secrets at rest.
This controller is deployed by default in CSM with the cray-kubernetes-encryption Helm chart.
By default, encryption is not enabled and must be enabled after install.
Note that control plane is used in this document elsewhere master or management nodes may be used. Control plane is used to be consistent with upstream Kubernetes documentation.</description>
    </item>
    
    <item>
      <title>Modifying A Tenant</title>
      <link>/docs-csm/en-13/operations/multi_tenancy/modify_a_tenant/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:13 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/multi_tenancy/modify_a_tenant/</guid>
      <description>Modifying a Tenant  Overview Modify the existing TAPMS CR Apply the modified TAPMS CR Modify the slurm operator CR Apply the slurm operator CR Modify the Slurm configuration  Overview This page provides information about how to modify a tenant. Modifications that are supported are limited to:
 Updating the list of component names (xnames) assigned to this tenant. Adding/deleting childNamespaces.  Modify the existing TAPMS CR An example of a change to add a component name (xname) to a tenant:</description>
    </item>
    
    <item>
      <title>Multi-tenancy Support</title>
      <link>/docs-csm/en-13/operations/multi_tenancy/overview/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:13 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/multi_tenancy/overview/</guid>
      <description>Multi-Tenancy Support  Disclaimer Components  Hierarchical Namespace Controller (HNC) Tenant and Partition Management System (TAPMS) Slurm operator   Getting started  Create a tenant Modify a tenant Remove a tenant Tenant administrator configuration    Disclaimer IMPORTANT Beginning in the CSM 1.3 release, this feature is offered as a preview only, and is not considered production-ready. This first release should be considered soft multi-tenancy, with additional functionality which hardens this feature in subsequent releases.</description>
    </item>
    
    <item>
      <title>Restore An Etcd Cluster From A Backup</title>
      <link>/docs-csm/en-13/operations/kubernetes/restore_an_etcd_cluster_from_a_backup/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:12 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/kubernetes/restore_an_etcd_cluster_from_a_backup/</guid>
      <description>Restore an etcd Cluster from a Backup Use an existing backup of a healthy etcd cluster to restore an unhealthy cluster to a healthy state.
The commands in this procedure can be run on any Kubernetes master or worker node on the system.
 Prerequisites Restore procedure  Automated script Manual procedure    Prerequisites A backup of a healthy etcd cluster has been created.
Restore procedure Etcd clusters can be restored using an automated script or a manual procedure.</description>
    </item>
    
    <item>
      <title>Restore Postgres</title>
      <link>/docs-csm/en-13/operations/kubernetes/restore_postgres/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:12 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/kubernetes/restore_postgres/</guid>
      <description>Restore Postgres Below are the service-specific steps required to restore data to a Postgres cluster.
Restore Postgres procedures by service:
 Restore Postgres for Spire  Restore from backup Restore without backup   Restore Postgres for Keycloak Restore Postgres for VCS Restore Postgres for HSM  Restore from backup Restore without backup   Restore Postgres for SLS  Restore from backup Restore without backup    Restore Postgres for Spire In the event that the Spire Postgres cluster must be rebuilt and the data restored, then the following procedures are recommended.</description>
    </item>
    
    <item>
      <title>Retrieve Cluster Health Information Using Kubernetes</title>
      <link>/docs-csm/en-13/operations/kubernetes/retrieve_cluster_health_information_using_kubernetes/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:12 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/kubernetes/retrieve_cluster_health_information_using_kubernetes/</guid>
      <description>Retrieve Cluster Health Information Using Kubernetes The kubectl CLI commands can be used to retrieve information about the Kubernetes cluster components.
Nodes Retrieve node status kubectl get nodes Example output:
NAME STATUS ROLES AGE VERSION ncn-m001 Ready control-plane,master 27h v1.20.13 ncn-m002 Ready control-plane,master 8d v1.20.13 ncn-m003 Ready control-plane,master 8d v1.20.13 ncn-w001 Ready &amp;lt;none&amp;gt; 8d v1.20.13 ncn-w002 Ready &amp;lt;none&amp;gt; 8d v1.20.13 ncn-w003 Ready &amp;lt;none&amp;gt; 8d v1.20.13 Pods Retrieve information about individual pods kubectl describe pod POD_NAME -n NAMESPACE_NAME Retrieve a list of all pods kubectl get pods -A Retrieve a list of healthy pods kubectl get pods -A | grep -E &amp;#39;Completed|Running&amp;#39; Retrieve a list of unhealthy pods   Option 1: List all pods that are not reported as Completed or Running.</description>
    </item>
    
    <item>
      <title>Tds Lower Cpu Requests</title>
      <link>/docs-csm/en-13/operations/kubernetes/tds_lower_cpu_requests/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:12 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/kubernetes/tds_lower_cpu_requests/</guid>
      <description>TDS Lower CPU Requests Systems with only three worker nodes (typically Test and Development Systems (TDS)) will encounter pod scheduling issues when worker nodes are taken out of the Kubernetes cluster to be upgraded.
(ncn-mw#) For systems with only three worker nodes, execute the following script to reduce the CPU request for some services with high CPU requests, in order to allow critical upgrade-related services to be successfully scheduled on only two worker nodes:</description>
    </item>
    
    <item>
      <title>Troubleshoot Intermittent HTTP 503 Code Failures</title>
      <link>/docs-csm/en-13/operations/kubernetes/troubleshoot_intermittent_503s/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:12 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/kubernetes/troubleshoot_intermittent_503s/</guid>
      <description>Troubleshoot Intermittent HTTP 503 Code Failures There are cases where API calls or cray command invocations will fail (sometimes intermittently) with an HTTP 503 error code. In the event that this occurs, attempt to remediate the issue by taking the following actions, according to specific error codes found in the pod or Envoy container log.
(ncn-mw#) The Envoy container is typically named istio-proxy, and it runs as a sidecar for pods that are part of the Istio mesh.</description>
    </item>
    
    <item>
      <title>Troubleshoot Postgres Database</title>
      <link>/docs-csm/en-13/operations/kubernetes/troubleshoot_postgres_database/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:12 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/kubernetes/troubleshoot_postgres_database/</guid>
      <description>Troubleshoot Postgres Database This page contains general Postgres troubleshooting topics.
 The patronictl tool Database unavailable Database disk full Replication lagging  Check if replication is working Recover replication Setup alerts for replication lag   Postgres status SyncFailed  Check all the postgresql resources  Case 1: some persistent volumes are not compatible with existing resizing providers Case 2: could not init db connection Case 3: password authentication failed for user     Cluster member missing  Determine if a cluster member is missing Recover from a missing member   Postgres leader missing  Determine if the Postgres leader is missing Recover from a missing Postgres leader    The patronictl tool The patronictl tool is used to call a REST API that interacts with Postgres databases.</description>
    </item>
    
    <item>
      <title>View Postgres Information For System Databases</title>
      <link>/docs-csm/en-13/operations/kubernetes/view_postgres_information_for_system_databases/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:12 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/kubernetes/view_postgres_information_for_system_databases/</guid>
      <description>View Postgres Information for System Databases Postgres uses SQL language to store and manage databases on the system. This procedure describes how to view and obtain helpful information about system databases, as well as the types of data being stored.
Prerequisites This procedure requires administrative privileges.
Procedure   (ncn-mw#) Log in to the Postgres container.
kubectl -n services exec -it cray-smd-postgres-0 -c postgres -- bash Example output:
Defaulting container name to postgres.</description>
    </item>
    
    <item>
      <title>Rebalance Healthy Etcd Clusters</title>
      <link>/docs-csm/en-13/operations/kubernetes/rebalance_healthy_etcd_clusters/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:11 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/kubernetes/rebalance_healthy_etcd_clusters/</guid>
      <description>Rebalance Healthy etcd Clusters Rebalance the etcd clusters. The clusters need to be in a healthy state, and there needs to be the same number of pods running on each worker node for the etcd clusters to be balanced.
Restoring the balance of etcd clusters will help with the storage of Kubernetes cluster data.
Prerequisites  etcd clusters are in a healthy state. etcd clusters do not have the same number of pods on each worker node.</description>
    </item>
    
    <item>
      <title>Rebuild Unhealthy Etcd Clusters</title>
      <link>/docs-csm/en-13/operations/kubernetes/rebuild_unhealthy_etcd_clusters/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:11 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/kubernetes/rebuild_unhealthy_etcd_clusters/</guid>
      <description>Rebuild Unhealthy etcd Clusters Rebuild any cluster that does not have healthy pods by deleting and redeploying unhealthy pods. This procedure includes examples for rebuilding etcd clusters in the services namespace. This procedure must be used for each unhealthy cluster, and not just those used in the following examples.
This process also applies when etcd is not visible when running the kubectl get pods command.
The commands in this procedure can be run on any Kubernetes master or worker node on the system.</description>
    </item>
    
    <item>
      <title>Recover From Postgres Wal Event</title>
      <link>/docs-csm/en-13/operations/kubernetes/recover_from_postgres_wal_event/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:11 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/kubernetes/recover_from_postgres_wal_event/</guid>
      <description>Recover from Postgres WAL Event A WAL event can occur because of lag, network communication, or bandwidth issues. This can cause the PVC hosted by Ceph and mounted inside the container on /home/postgres/pgdata to fill and the database to stop running. If no database dump exists, the disk space issue needs to be fixed so that a dump can be taken. Then the dump can be restored to a newly created postgresql cluster.</description>
    </item>
    
    <item>
      <title>Repopulate Data In Etcd Clusters When Rebuilding Them</title>
      <link>/docs-csm/en-13/operations/kubernetes/repopulate_data_in_etcd_clusters_when_rebuilding_them/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:11 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/kubernetes/repopulate_data_in_etcd_clusters_when_rebuilding_them/</guid>
      <description>Repopulate Data in etcd Clusters When Rebuilding Them When an etcd cluster is not healthy, it needs to be rebuilt. During that process, the pods that rely on etcd clusters lose data. That data needs to be repopulated in order for the cluster to go back to a healthy state.
 Repopulate Data in etcd Clusters When Rebuilding Them  Applicable services Prerequisites Procedures  BOS BSS CPS CRUS FAS HMNFD MEDS REDS      Applicable services The following services need their data repopulated in the etcd cluster:</description>
    </item>
    
    <item>
      <title>Report The Endpoint Status For Etcd Clusters</title>
      <link>/docs-csm/en-13/operations/kubernetes/report_the_endpoint_status_for_etcd_clusters/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:11 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/kubernetes/report_the_endpoint_status_for_etcd_clusters/</guid>
      <description>Report the Endpoint Status for etcd Clusters Report etcd cluster end point status. The report includes a cluster&amp;rsquo;s endpoint, database size, and leader status.
This procedure provides the ability to view the etcd cluster endpoint status.
Prerequisites  This procedure requires root privileges. The etcd clusters are in a healthy state.  Procedure   Report the endpoint status for all etcd clusters in a namespace.
The following example is for the services namespace.</description>
    </item>
    
    <item>
      <title>Restore Bare-metal Etcd Clusters From An S3 Snapshot</title>
      <link>/docs-csm/en-13/operations/kubernetes/restore_bare-metal_etcd_clusters_from_an_s3_snapshot/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:11 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/kubernetes/restore_bare-metal_etcd_clusters_from_an_s3_snapshot/</guid>
      <description>Restore Bare-Metal etcd Clusters from an S3 Snapshot The etcd cluster that serves Kubernetes on master nodes is backed up every 10 minutes. These backups are pushed to Ceph Rados Gateway (S3).
Restoring the etcd cluster from backup is only meant to be used in a catastrophic scenario, in which the Kubernetes cluster and master nodes are being rebuilt. This procedure shows how to restore the bare-metal etcd cluster from a Simple Storage Service (S3) snapshot.</description>
    </item>
    
    <item>
      <title>Configure Kubernetes Api Audit Log Maximum Backups</title>
      <link>/docs-csm/en-13/operations/kubernetes/limit_kubernetes_api_audit_log_maxbackups/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:10 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/kubernetes/limit_kubernetes_api_audit_log_maxbackups/</guid>
      <description>Configure Kubernetes API Audit Log Maximum Backups If Kubernetes API Auditing was enabled at install or upgrade, via the CSI option --k8s-api-auditing-enabled true or the system_config.yaml option k8s-api-auditing-enabled: true, apply this procedure to running Kubernetes Master Nodes.
Prerequisites This procedure requires administrative privileges and assumes that the device being used has:
 kubectl is installed Access to the site admin network  Procedure   SSH as root to the first Kubernetes Master Node, canonically ncn-m001.</description>
    </item>
    
    <item>
      <title>Kubernetes</title>
      <link>/docs-csm/en-13/operations/kubernetes/kubernetes/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:10 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/kubernetes/kubernetes/</guid>
      <description>Kubernetes The system management components are broken down into a series of micro-services. Each service is independently deployable, fine-grained, and uses lightweight protocols. As a result, the system&amp;rsquo;s micro-services are modular, resilient, and can be updated independently. Services within this architecture communicate via REST APIs.
About Kubernetes Kubernetes is a portable and extensible platform for managing containerized workloads and services. Kubernetes serves as a micro-services platform on the system that facilitates application deployment, scaling, and management.</description>
    </item>
    
    <item>
      <title>Kubernetes Networking</title>
      <link>/docs-csm/en-13/operations/kubernetes/kubernetes_networking/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:10 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/kubernetes/kubernetes_networking/</guid>
      <description>Kubernetes Networking Every Kubernetes pod has an IP address in the pod network that is reachable within the cluster. The system uses the weave-net plugin for inter-node communication.
Access services from outside the cluster All services with a REST API must be accessed from outside the cluster using the Istio Ingress Gateway. This gateway can be accessed using a URL in the following format:
https://api.cmn.SYSTEM-NAME_DOMAIN-NAME https://api.can.SYSTEM-NAME_DOMAIN-NAME https://api.chn.SYSTEM-NAME_DOMAIN-NAME The API requests then get routed to the appropriate node running that service.</description>
    </item>
    
    <item>
      <title>Kubernetes Storage</title>
      <link>/docs-csm/en-13/operations/kubernetes/kubernetes_storage/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:10 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/kubernetes/kubernetes_storage/</guid>
      <description>Kubernetes Storage Data belonging to micro-services in the management cluster is managed through persistent storage, which provides reliable and resilient data protection for containers running in the Kubernetes cluster.
The backing storage for this service is currently provided by JBOD disks that are spread across several nodes of the management cluster. These node disks are managed by Ceph, and are exposed to containers in the form of persistent volumes.</description>
    </item>
    
    <item>
      <title>Kyverno Policy Management</title>
      <link>/docs-csm/en-13/operations/kubernetes/kyverno/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:10 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/kubernetes/kyverno/</guid>
      <description>Kyverno policy management Kyverno is a policy engine designed specifically for Kubernetes.
Kyverno allows cluster administrators to manage environment-specific configurations (independently of workload configurations) and enforce configuration best practices for their clusters.
Kyverno can be used to scan existing workloads for best practices, or it can be used to enforce best practices by blocking or mutating API requests.
Kyverno enables administrators to do the following:
 Manage policies as Kubernetes resources.</description>
    </item>
    
    <item>
      <title>Pod Resource Limits</title>
      <link>/docs-csm/en-13/operations/kubernetes/pod_resource_limits/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:10 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/kubernetes/pod_resource_limits/</guid>
      <description>Pod Resource Limits Kubernetes uses resource requests and Quality of Service (QoS) for scheduling pods. Resource requests can be provided explicitly for pods and containers, whereas pod QoS is implicit, based on the resource requests and limits of the containers in the pod. There are three types of QoS:
 Guaranteed: All containers in a pod have explicit memory and CPU resource requests and limits. For each resource, the limit equals the request.</description>
    </item>
    
    <item>
      <title>Create A Manual Backup Of A Healthy Etcd Cluster</title>
      <link>/docs-csm/en-13/operations/kubernetes/create_a_manual_backup_of_a_healthy_etcd_cluster/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:09 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/kubernetes/create_a_manual_backup_of_a_healthy_etcd_cluster/</guid>
      <description>Create a Manual Backup of a Healthy etcd Cluster Manually create a backup of a healthy etcd cluster and check to see if the backup was created successfully.
Backups of healthy etcd clusters can be used to restore the cluster if it becomes unhealthy at any point.
The commands in this procedure can be run on any master node (ncn-mXXX) or worker node (ncn-wXXX) on the system.
Prerequisites A healthy etcd cluster is available on the system.</description>
    </item>
    
    <item>
      <title>Create A Manual Backup Of Bare-metal Etcd Cluster</title>
      <link>/docs-csm/en-13/operations/kubernetes/create_a_manual_backup_of_a_healthy_bare-metal_etcd_cluster/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:09 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/kubernetes/create_a_manual_backup_of_a_healthy_bare-metal_etcd_cluster/</guid>
      <description>Create a Manual Backup of Bare-Metal etcd Cluster Manually create a backup of a healthy Bare-Metal etcd cluster.
Bare-Metal etcd cluster backups are automatically created every ten minutes and deleted after 24 hours. When necessary, these procedures can be used to create an additional backup, and then save a separate copy of it, or one of the automated backups. When needed, a Bare-Metal etcd cluster can be restored from a saved backup.</description>
    </item>
    
    <item>
      <title>Determine If Pods Are Hitting Resource Limits</title>
      <link>/docs-csm/en-13/operations/kubernetes/determine_if_pods_are_hitting_resource_limits/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:09 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/kubernetes/determine_if_pods_are_hitting_resource_limits/</guid>
      <description>Determine if Pods are Hitting Resource Limits Determine if a pod is being CPU throttled or hitting its memory limits (OOMKilled). Use the /opt/cray/platform-utils/detect_cpu_throttling.sh script to determine if any pods are being CPU throttled, and check the Kubernetes events to see if any pods are hitting a memory limit.
IMPORTANT: The presence of CPU throttling does not always indicate a problem, but if a service is being slow or experiencing latency issues, this procedure can be used to evaluate if it is not performing well as a result of CPU throttling.</description>
    </item>
    
    <item>
      <title>Disaster Recovery For Postgres</title>
      <link>/docs-csm/en-13/operations/kubernetes/disaster_recovery_postgres/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:09 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/kubernetes/disaster_recovery_postgres/</guid>
      <description>Disaster Recovery for Postgres In the event that the Postgres cluster has failed to the point that it must be recovered and there is no dump available to restore the data, a full service specific disaster recovery is needed.
Below are the service specific steps required to cleanup any existing resources, redeploy the resources, and repopulate the data.
Disaster recovery procedures by service:
 Restore HSM (Hardware State Manger) Postgres without a Backup Restore SLS (System Layout Service) Postgres without a Backup Restore Spire Postgres without a Backup Restore Keycloak Postgres without a Backup Restore console Postgres  Restore Keycloak Postgres without a backup The following procedures are required to rebuild the automatically populated contents of Keycloak&amp;rsquo;s PostgreSQL database if the database has been lost and recreated.</description>
    </item>
    
    <item>
      <title>Increase Kafka Pod Resource Limits</title>
      <link>/docs-csm/en-13/operations/kubernetes/increase_kafka_pod_resource_limits/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:09 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/kubernetes/increase_kafka_pod_resource_limits/</guid>
      <description>Increase Kafka Pod Resource Limits For larger scale systems, the Kafka resource limits may need to be increased. See Increase Pod Resource Limits for details on how to increase limits.
Increase Kafka Resource Limits Example
For a 1500 compute node system, increasing the cpu count to 6 and memory limits to 128G should be adequate.</description>
    </item>
    
    <item>
      <title>Increase Pod Resource Limits</title>
      <link>/docs-csm/en-13/operations/kubernetes/increase_pod_resource_limits/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:09 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/kubernetes/increase_pod_resource_limits/</guid>
      <description>Increase Pod Resource Limits Increase the appropriate resource limits for pods after determining if a pod is being CPU throttled or OOMKilled.
Return Kubernetes pods to a healthy state with resources available.
Prerequisites  kubectl is installed. The names of the pods hitting their resource limits are known. See Determine if Pods are Hitting Resource Limits.  Procedure   Determine the current limits of a pod.
In the example below, cray-hbtd-etcd-8r2scmpb58 is the POD_ID being used.</description>
    </item>
    
    <item>
      <title>Backups For Etcd-operator Clusters</title>
      <link>/docs-csm/en-13/operations/kubernetes/backups_for_etcd-operator_clusters/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:08 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/kubernetes/backups_for_etcd-operator_clusters/</guid>
      <description>Backups for etcd-operator Clusters Backups are periodically created for etcd clusters. These backups are stored in the Ceph Rados Gateway (S3). Not all services are backed up automatically. Services that are not backed up automatically will need to be manually rediscovered if the cluster is unhealthy.
 Clusters with automated backups Clusters without automated backups  Clusters with automated backups The following services are backed up daily (one week of backups retained) as part of the automated solution:</description>
    </item>
    
    <item>
      <title>Check For And Clear Etcd Cluster Alarms</title>
      <link>/docs-csm/en-13/operations/kubernetes/check_for_and_clear_etcd_cluster_alarms/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:08 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/kubernetes/check_for_and_clear_etcd_cluster_alarms/</guid>
      <description>Check for and Clear etcd Cluster Alarms Check for any etcd cluster alarms and clear them as needed. An etcd cluster alarm must be manually cleared.
For example, a cluster&amp;rsquo;s database NOSPACE alarm is set when database storage space is no longer available. A subsequent defrag may free up database storage space, but writes to the database will continue to fail while the NOSPACE alarm is set.
Prerequisites  This procedure requires root privileges.</description>
    </item>
    
    <item>
      <title>Check The Health And Balance Of Etcd Clusters</title>
      <link>/docs-csm/en-13/operations/kubernetes/check_the_health_and_balance_of_etcd_clusters/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:08 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/kubernetes/check_the_health_and_balance_of_etcd_clusters/</guid>
      <description>Check the Health and Balance of etcd Clusters Check to see if all of the etcd clusters have healthy pods, are balanced, and have a healthy cluster database. There needs to be the same number of pods running on each worker node for the etcd clusters to be balanced. If the number of pods is not the same for each worker node, the cluster is not balanced.
Any clusters that do not have healthy pods will need to be rebuilt.</description>
    </item>
    
    <item>
      <title>Clear Space In An Etcd Cluster Database</title>
      <link>/docs-csm/en-13/operations/kubernetes/clear_space_in_an_etcd_cluster_database/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:08 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/kubernetes/clear_space_in_an_etcd_cluster_database/</guid>
      <description>Clear Space in an etcd Cluster Database Use this procedure to clear the etcd cluster NOSPACE alarm. Once it is set it will remain set. If needed, defrag the database cluster before clearing the NOSPACE alarm.
Defragging the database cluster and clearing the etcd cluster NOSPACE alarm will free up database space.
Prerequisites  This procedure requires root privileges The etcd clusters are in a healthy state  Procedure   (ncn-mw#) Clear up space when the etcd database space has exceeded and has been defragged, but the NOSPACE alarm remains set.</description>
    </item>
    
    <item>
      <title>Configure Kubectl Credentials To Access The Kubernetes Apis</title>
      <link>/docs-csm/en-13/operations/kubernetes/configure_kubectl_credentials_to_access_the_kubernetes_apis/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:08 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/kubernetes/configure_kubectl_credentials_to_access_the_kubernetes_apis/</guid>
      <description>Configure kubectl Credentials to Access the Kubernetes APIs The credentials for kubectl are located in the admin configuration file on all non-compute node (NCN) master and worker nodes. They can be found at /etc/kubernetes/admin.conf for the root user. Use kubectl to access the Kubernetes cluster from a device outside the cluster.
For more information, refer to https://kubernetes.io/
Prerequisites This procedure requires administrative privileges and assumes that the device being used has:</description>
    </item>
    
    <item>
      <title>Containerd</title>
      <link>/docs-csm/en-13/operations/kubernetes/containerd/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:08 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/kubernetes/containerd/</guid>
      <description>containerd containerd is a container runtime (systemd service) that runs on the host. It is used to run containers on the Kubernetes platform.
 /var/lib/containerd filling up Restarting containerd on a worker NCN  /var/lib/containerd filling up In older versions of containerd, there are cases where the /var/lib/containerd directory fills up. In the event that this occurs, the following steps can be used to remediate the issue.
  (ncn-mw#) Restart containerd on the NCN.</description>
    </item>
    
    <item>
      <title>Kubernetes And Bare Metal Etcd Certificate Renewal</title>
      <link>/docs-csm/en-13/operations/kubernetes/cert_renewal_for_kubernetes_and_bare_metal_etcd/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:08 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/kubernetes/cert_renewal_for_kubernetes_and_bare_metal_etcd/</guid>
      <description>Kubernetes and Bare Metal EtcD Certificate Renewal As part of the installation, Kubernetes generates certificates for the required subcomponents. This document will help walk through the process of renewing the certificates.
IMPORTANT:
 Depending on the version of Kubernetes, the command may or may not reside under the alpha category. Use kubectl certs --help and kubectl alpha certs --help to determine this. The overall command syntax is the same; the only difference is whether or not the command structure includes alpha.</description>
    </item>
    
    <item>
      <title>About Etcd</title>
      <link>/docs-csm/en-13/operations/kubernetes/about_etcd/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:07 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/kubernetes/about_etcd/</guid>
      <description>About etcd The system uses etcd for storing all of its cluster data. It is an open source database that is excellent for maintaining the state of Kubernetes. Failures in the etcd cluster at the heart of Kubernetes will cause a failure of Kubernetes. To mitigate this risk, the system is deployed with etcd on dedicated disks and with a specific configuration to optimize Kubernetes workloads. The system also provides additional etcd cluster(s) as necessary to help maintain an operational state of services.</description>
    </item>
    
    <item>
      <title>About Kubectl</title>
      <link>/docs-csm/en-13/operations/kubernetes/about_kubectl/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:07 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/kubernetes/about_kubectl/</guid>
      <description>About kubectl kubectl is a CLI that can be used to run commands against a Kubernetes cluster. The format of the kubectl command is shown below:
kubectl COMMAND RESOURCE_TYPE RESOURCE_NAME FLAGS An example of using kubectl to retrieve information about a pod is shown below:
kubectl get pod POD_NAME1 POD_NAME2 kubectl is installed by default on the non-compute node (NCN) image. To learn more about kubectl, refer to https://kubernetes.io/docs</description>
    </item>
    
    <item>
      <title>About Kubernetes Taints And Labels</title>
      <link>/docs-csm/en-13/operations/kubernetes/about_kubernetes_taints_and_labels/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:07 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/kubernetes/about_kubernetes_taints_and_labels/</guid>
      <description>About Kubernetes Taints and Labels Kubernetes labels control node affinity, which is the property of pods that attracts them to a set of nodes. On the other hand, Kubernetes taints enable a node to repel a set of pods. In addition, pods can have tolerances for taints to allow them to run on nodes with certain taints.
Taints are controlled with the kubectl taint nodes command, while node labels for various nodes can be customized with a configmap that contains the desired values.</description>
    </item>
    
    <item>
      <title>About Postgres</title>
      <link>/docs-csm/en-13/operations/kubernetes/about_postgres/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:07 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/kubernetes/about_postgres/</guid>
      <description>About Postgres The system uses PostgreSQL (known as Postgres) as a database solution. Postgres databases use SQL language to store and manage databases on the system.
To learn more about Postgres, see https://www.postgresql.org/docs/.
The Patroni tool can be used to manage and maintain information in a Postgres database. It handles tasks such as listing cluster members and the replication status, configuring and restarting databases, and more. For more information about this tool, refer to Troubleshoot Postgres Database.</description>
    </item>
    
    <item>
      <title>Upload And Register An Image Recipe</title>
      <link>/docs-csm/en-13/operations/image_management/upload_and_register_an_image_recipe/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:07 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/image_management/upload_and_register_an_image_recipe/</guid>
      <description>Upload and Register an Image Recipe Download and expand recipe archives from S3 and IMS. Modify and upload a recipe archive, and then register that recipe archive with IMS.
Prerequisites  The Cray command line interface (CLI) tool is initialized and configured on the system. System management services (SMS) are running in a Kubernetes cluster on non-compute nodes (NCNs) and include the following deployment:  cray-ims, the Image Management Service (IMS)   The NCN Certificate Authority (CA) public key has been properly installed into the CA cache for this system.</description>
    </item>
    
    <item>
      <title>Customize An Image Root Using IMS</title>
      <link>/docs-csm/en-13/operations/image_management/customize_an_image_root_using_ims/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:06 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/image_management/customize_an_image_root_using_ims/</guid>
      <description>Customize an Image Root Using IMS The Image Management Service (IMS) customization workflow sets up a temporary image customization environment within a Kubernetes pod and mounts the image to be customized in that environment. A system administrator then makes the desired changes to the image root within the customization environment.
Afterwards, the IMS customization workflow automatically copies the NCN CA public key to /etc/cray/ca/certificate_authority.crt within the image root being customized, in order to enable secure communications between NCNs and client nodes.</description>
    </item>
    
    <item>
      <title>Delete Or Recover Deleted IMS Content</title>
      <link>/docs-csm/en-13/operations/image_management/delete_or_recover_deleted_ims_content/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:06 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/image_management/delete_or_recover_deleted_ims_content/</guid>
      <description>Delete or Recover Deleted IMS Content The Image Management System (IMS) manages user supplied SSH public Keys, customizable image recipes, images, and IMS jobs that are used to build or customize images. In previous versions of IMS, deleting an IMS public key, recipe, or image resulted in that item being permanently deleted. Additionally, IMS recipes and images store linked artifacts in the Simple Storage Service (S3) datastore. These artifacts are referenced by the IMS recipe and image records.</description>
    </item>
    
    <item>
      <title>Export And Importing IMS Data</title>
      <link>/docs-csm/en-13/operations/image_management/exporting_and_importing_ims_data/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:06 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/image_management/exporting_and_importing_ims_data/</guid>
      <description>Export and Importing IMS Data IMS recipe and image data, including associated artifacts stored in Ceph S3, can be exported and imported either using an automated script, or manually one at a time.
 Prerequisites Exporting images and recipes  Automated export procedure Manual recipe export procedure Manual image export procedure   Importing images and recipes  Automated import procedure Manual recipe import procedure Manual image import procedure    Prerequisites  Ensure that the cray command line interface (CLI) is authenticated and configured to talk to system management services.</description>
    </item>
    
    <item>
      <title>Image Management</title>
      <link>/docs-csm/en-13/operations/image_management/image_management/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:06 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/image_management/image_management/</guid>
      <description>Image Management The Image Management Service (IMS) uses the open source Kiwi-NG tool to build image roots from compressed Kiwi image descriptions. These compressed Kiwi image descriptions are referred to as &amp;ldquo;recipes.&amp;rdquo; Kiwi-NG builds images based on a variety of different Linux distributions, specifically SUSE, RHEL, and their derivatives. Kiwi image descriptions must follow the Kiwi development schema. More information about the development schema and the Kiwi-NG tool can be found in the documentation: https://doc.</description>
    </item>
    
    <item>
      <title>Image Management Workflows</title>
      <link>/docs-csm/en-13/operations/image_management/image_management_workflows/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:06 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/image_management/image_management_workflows/</guid>
      <description>Image Management Workflows Overview of how to create an image and how to customize and image.
The following workflows are intended to be high-level overviews of image management tasks. These workflows depict how services interact with each other during image management and help to provide a quicker and deeper understanding of how the system functions.
The workflows in this section include:
 Create a New Image Customize an Image  Create a New Image Use Case: The system administrator creates an image root from a customized recipe.</description>
    </item>
    
    <item>
      <title>Import An External Image To IMS</title>
      <link>/docs-csm/en-13/operations/image_management/import_external_image_to_ims/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:06 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/image_management/import_external_image_to_ims/</guid>
      <description>Import an External Image to IMS The Image Management Service (IMS) is typically used to build images from IMS recipes and customize Images that are already known to IMS. However, it is sometimes the case that an image is built using a mechanism other than by IMS and needs to be added to IMS. In these cases, the following procedure can be used to add this external image to IMS and upload the image&amp;rsquo;s artifacts to the Simple Storage Service (S3).</description>
    </item>
    
    <item>
      <title>Import An NCN Image To IMS</title>
      <link>/docs-csm/en-13/operations/image_management/import_ncn_image_to_ims/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:06 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/image_management/import_ncn_image_to_ims/</guid>
      <description>Import an NCN Image to IMS This page documents an automated tool which takes a set of Non-Compute Node (NCN) kernel, initrd, and SquashFS artifact files, uploads them into the Simple Storage Service (S3), and registers them as an image in the Image Management Service (IMS).
For the more general (and more manual) procedure for how to register images in IMS, see Import an External Image to IMS. In addition to providing a more detailed explanation of the various subtasks being carried out, that procedure also covers such variations as:</description>
    </item>
    
    <item>
      <title>Build A New UAN Image Using The Default Recipe</title>
      <link>/docs-csm/en-13/operations/image_management/build_a_new_uan_image_using_the_default_recipe/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:05 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/image_management/build_a_new_uan_image_using_the_default_recipe/</guid>
      <description>Build a New UAN Image Using the Default Recipe Build or rebuild the User Access Node (UAN) image using either the default UAN image or image recipe. Both of these are supplied by the UAN product stream installer.
 Prerequisites Overview Remove Slingshot Diagnostics RPM From Default UAN Recipe Build the UAN Image Automatically Using IMS Build the UAN Image By Customizing It Manually  Prerequisites  Both the Cray Operation System (COS) and UAN product streams must be installed.</description>
    </item>
    
    <item>
      <title>Build An Image Using IMS Rest Service</title>
      <link>/docs-csm/en-13/operations/image_management/build_an_image_using_ims_rest_service/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:05 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/image_management/build_an_image_using_ims_rest_service/</guid>
      <description>Build an Image Using IMS REST Service Create an image root from an IMS recipe.
Prerequisites  The Cray command line interface (CLI) tool is initialized and configured on the system. System management services (SMS) are running in a Kubernetes cluster on non-compute nodes (NCN) and include the following deployments:  cray-ims, the Image Management Service (IMS) cray-nexus, the Nexus repository manager service   The NCN Certificate Authority (CA) public key has been properly installed into the CA cache for this system.</description>
    </item>
    
    <item>
      <title>Configure IMS To Validate Rpms</title>
      <link>/docs-csm/en-13/operations/image_management/configure_ims_to_validate_rpms/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:05 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/image_management/configure_ims_to_validate_rpms/</guid>
      <description>Configure IMS to Validate RPMs Configuring the Image Management Service (IMS) to validate the GPG signatures of RPMs during IMS Build operations involves the following two steps:
  Create and update IMS to use a new Kiwi-NG Image with the Signing Keys embedded.
 NOTE The default IMS Kiwi-NG Image is already configured with the signing keys needed to validate HPE and SuSE RPMs and repositories.
   Update IMS Recipes to require GPG verification of RPMs, repositories, or both.</description>
    </item>
    
    <item>
      <title>Convert Tgz Archives To SqUAShfs Images</title>
      <link>/docs-csm/en-13/operations/image_management/convert_tgz_archives_to_squashfs_images/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:05 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/image_management/convert_tgz_archives_to_squashfs_images/</guid>
      <description>Convert TGZ Archives to SquashFS Images If customizing a pre-built image root archive compressed as a .txz or other non-SquashFS format, convert the image root to SquashFS and upload the SquashFS archive to S3.
The steps in this section only apply if the image root is not in SquashFS format.
Prerequisites There is a pre-built image that is not currently in SquashFS format.
Procedure This procedure can be run on any master or worker NCN.</description>
    </item>
    
    <item>
      <title>Create UAN Boot Images</title>
      <link>/docs-csm/en-13/operations/image_management/create_uan_boot_images/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:05 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/image_management/create_uan_boot_images/</guid>
      <description>Create UAN Boot Images Update configuration management Git repository to match the installed version of the UAN product. Then use that updated configuration to create UAN boot images and a BOS session template.
This is the overall workflow for preparing UAN images for booting UANs:
 Clone the UAN configuration Git repository and create a branch based on the branch imported by the UAN installation. Update the configuration content and push the changes to the newly created branch.</description>
    </item>
    
    <item>
      <title>HPE Pdu Admin Procedures</title>
      <link>/docs-csm/en-13/operations/hpe_pdu/hpe_pdu_admin_procedures/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:05 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/hpe_pdu/hpe_pdu_admin_procedures/</guid>
      <description>HPE PDU Admin Procedures The following procedures are used to manage the HPE Power Distribution Unit (PDU):
 Verify PDU vendor Connect to HPE PDU web interface HPE PDU initial set-up Update HPE PDU firmware Change HPE PDU user passwords Discover HPE PDU after upgrading CSM   IMPORTANT: Because of the polling method used to process sensor data from the HPE PDU, telemetry data may take up to six minutes to refresh; this includes the outlet status reported by the Hardware State Manager (HSM).</description>
    </item>
    
    <item>
      <title>Adjust Hm Collector Ingress Replicas And Resource Limits</title>
      <link>/docs-csm/en-13/operations/hmcollector/adjust_hmcollector_resource_limits_requests/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:04 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/hmcollector/adjust_hmcollector_resource_limits_requests/</guid>
      <description>Adjust HM Collector Ingress Replicas and Resource Limits  Inspect current resource usage Inspect pods for OOMKilled events How to adjust replicas and limits  Inspect current resource usage (ncn-mw#) View resource usage of the containers in the cray-hms-hmcollector-ingress pods:
kubectl -n services top pod -l app.kubernetes.io/name=cray-hms-hmcollector-ingress --containers Example output:
POD NAME CPU(cores) MEMORY(bytes) cray-hms-hmcollector-ingress-554bb46784-dvjzq cray-hms-hmcollector-ingress 7m 99Mi cray-hms-hmcollector-ingress-554bb46784-dvjzq istio-proxy 5m 132Mi cray-hms-hmcollector-ingress-554bb46784-hctwm cray-hms-hmcollector-ingress 4m 82Mi cray-hms-hmcollector-ingress-554bb46784-hctwm istio-proxy 4m 120Mi cray-hms-hmcollector-ingress-554bb46784-zdhwc cray-hms-hmcollector-ingress 5m 97Mi cray-hms-hmcollector-ingress-554bb46784-zdhwc istio-proxy 4m 133Mi The default replica count for the cray-hms-hmcollector-ingress deployment is 3.</description>
    </item>
    
    <item>
      <title>Manage Component Groups</title>
      <link>/docs-csm/en-13/operations/hardware_state_manager/manage_component_groups/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:04 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/hardware_state_manager/manage_component_groups/</guid>
      <description>Manage Component Groups The creation, deletion, and modification of groups is enabled by the Hardware State Manager (HSM) APIs.
 Example group Prerequisites Create and modify a group  Create a group Modify a group   Retrieve a group Delete a group  Example group The following is an example group that contains the optional fields tags and exclusiveGroup:
{ &amp;#34;label&amp;#34; : &amp;#34;blue&amp;#34;, &amp;#34;description&amp;#34; : &amp;#34;blue node group&amp;#34;, &amp;#34;tags&amp;#34; : [ &amp;#34;tag1&amp;#34;, &amp;#34;tag2&amp;#34; ], &amp;#34;members&amp;#34; : { &amp;#34;ids&amp;#34; : [ &amp;#34;x0c0s0b0n0&amp;#34;, &amp;#34;x0c0s0b0n1&amp;#34;, &amp;#34;x0c0s0b1n0&amp;#34;, &amp;#34;x0c0s0b1n1&amp;#34; ] }, &amp;#34;exclusiveGroup&amp;#34; : &amp;#34;colors&amp;#34; } Prerequisites The commands on this page will not work unless the Cray CLI has been initialized on the node where the commands are being run.</description>
    </item>
    
    <item>
      <title>Manage Component Partitions</title>
      <link>/docs-csm/en-13/operations/hardware_state_manager/manage_component_partitions/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:04 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/hardware_state_manager/manage_component_partitions/</guid>
      <description>Manage Component Partitions The creation, deletion, and modification of partitions is enabled by the Hardware State Manager (HSM) APIs.
The following is an example partition that contains the optional tags field:
{ &amp;#34;name&amp;#34; : &amp;#34;partition 1&amp;#34;, &amp;#34;description&amp;#34; : &amp;#34;partition 1&amp;#34;, &amp;#34;tags&amp;#34; : [ &amp;#34;tag2&amp;#34; ], &amp;#34;members&amp;#34; : { &amp;#34;ids&amp;#34; : [ &amp;#34;x0c0s0b0n0&amp;#34;, &amp;#34;x0c0s0b0n1&amp;#34;, &amp;#34;x0c0s0b1n0&amp;#34; ] }, } Troubleshooting: If the Cray CLI has not been initialized, the CLI commands will not work.</description>
    </item>
    
    <item>
      <title>Manage Hms Locks</title>
      <link>/docs-csm/en-13/operations/hardware_state_manager/manage_hms_locks/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:04 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/hardware_state_manager/manage_hms_locks/</guid>
      <description>Manage HMS Locks This section describes how to check the status of a lock, disable reservations, and repair reservations. The disable and repair operations only affect the ability to make reservations on hardware devices.
Some of the common scenarios an admin might encounter when working with the Hardware State Manager (HSM) Locking API are also described.
Check Lock Status Use the following command to verify if a component name (xname) is locked or not.</description>
    </item>
    
    <item>
      <title>Restore Hardware State Manager (HSM) Postgres Database From Backup</title>
      <link>/docs-csm/en-13/operations/hardware_state_manager/restore_hsm_postgres_from_backup/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:04 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/hardware_state_manager/restore_hsm_postgres_from_backup/</guid>
      <description>Restore Hardware State Manager (HSM) Postgres Database from Backup This procedure can be used to restore the HSM Postgres database from a previously taken backup. This can be a manual backup created by the Create a Backup of the HSM Postgres Database procedure, or an automatic backup created by the cray-smd-postgresql-db-backup Kubernetes cronjob.
Prerequisites   Healthy System Layout Service (SLS). Recovered first if also affected.
  (ncn#) Healthy HSM Postgres Cluster.</description>
    </item>
    
    <item>
      <title>Restore Hardware State Manager (HSM) Postgres Without An Existing Backup</title>
      <link>/docs-csm/en-13/operations/hardware_state_manager/restore_hsm_postgres_without_a_backup/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:04 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/hardware_state_manager/restore_hsm_postgres_without_a_backup/</guid>
      <description>Restore Hardware State Manager (HSM) Postgres without an Existing Backup This procedure is intended to repopulate HSM in the event when no Postgres backup exists.
Prerequisite   Healthy System Layout Service (SLS). Recovered first if also affected.
  Healthy HSM service.
Verify all 3 HSM Postgres replicas are up and running:
kubectl -n services get pods -l cluster-name=cray-smd-postgres Example output:
NAME READY STATUS RESTARTS AGE cray-smd-postgres-0 3/3 Running 0 18d cray-smd-postgres-1 3/3 Running 0 18d cray-smd-postgres-2 3/3 Running 0 18d   Procedure   Re-run the HSM loader job.</description>
    </item>
    
    <item>
      <title>Set BMC Management Roles</title>
      <link>/docs-csm/en-13/operations/hardware_state_manager/set_bmc_management_role/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:04 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/hardware_state_manager/set_bmc_management_role/</guid>
      <description>Set BMC Management Roles The ability to ignore non-compute nodes (NCNs) is turned off by default. Management nodes and NCNs are also not locked by default. The administrator must lock the NCNs and their BMCs to prevent unwanted actions from affecting these nodes. To more easily identify the BMCs that are associated with the management nodes, they need to be marked with the Management role in the Hardware State Manager (HSM), just like their associated nodes.</description>
    </item>
    
    <item>
      <title>Create A Backup Of The HSM Postgres Database</title>
      <link>/docs-csm/en-13/operations/hardware_state_manager/create_a_backup_of_the_hsm_postgres_database/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:03 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/hardware_state_manager/create_a_backup_of_the_hsm_postgres_database/</guid>
      <description>Create a Backup of the HSM Postgres Database Perform a manual backup of the contents of the Hardware State Manager (HSM) Postgres database. This backup can be used to restore the contents of the HSM Postgres database at a later point in time using the Restore HSM Postgres from Backup procedure.
Prerequisites   Healthy HSM Postgres Cluster.
Use patronictl list on the HSM Postgres cluster to determine the current state of the cluster, and a healthy cluster will look similar to the following:</description>
    </item>
    
    <item>
      <title>Hardware Management Services (hms) Locking Api</title>
      <link>/docs-csm/en-13/operations/hardware_state_manager/hardware_management_services_hms_locking_api/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:03 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/hardware_state_manager/hardware_management_services_hms_locking_api/</guid>
      <description>Hardware Management Services (HMS) Locking API The locking feature is a part of the Hardware State Manager (HSM) API. The locking API enables administrators to lock components on the system. Locking components ensures other system actors, such as administrators or running services, cannot perform a firmware update with the Firmware Action Service (FAS) or a power state change with the Cray Advanced Platform Monitoring and Control (CAPMC). Locks only constrain FAS and CAPMC from each other and help ensure that a firmware update action will not be interfered with by a request to power off the device through CAPMC.</description>
    </item>
    
    <item>
      <title>Hardware State Manager (HSM)</title>
      <link>/docs-csm/en-13/operations/hardware_state_manager/hardware_state_manager/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:03 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/hardware_state_manager/hardware_state_manager/</guid>
      <description>Hardware State Manager (HSM) The Hardware State Manager (HSM) monitors and interrogates hardware components in the HPE Cray EX system, tracking hardware state and inventory information, and making it available via REST queries and message bus events when changes occur.
In the CSM 0.9.3 release, v1 of the HSM API has begun its deprecation process in favor of the new HSM v2 API. Refer to the HSM API documentation for more information on the changes.</description>
    </item>
    
    <item>
      <title>Hardware State Manager (HSM) State And Flag Fields</title>
      <link>/docs-csm/en-13/operations/hardware_state_manager/hardware_state_manager_hsm_state_and_flag_fields/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:03 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/hardware_state_manager/hardware_state_manager_hsm_state_and_flag_fields/</guid>
      <description>Hardware State Manager (HSM) State and Flag Fields HSM manages important information for hardware components in the system. Administrators can use the data returned by HSM to learn about the state of the system. To do so, it is critical that the State and Flag fields are understood, and the next steps to take are known when viewing output returned by HSM commands. It is also beneficial to understand what services can cause State or Flag changes in HSM.</description>
    </item>
    
    <item>
      <title>HSM Roles And Subroles</title>
      <link>/docs-csm/en-13/operations/hardware_state_manager/hsm_roles_and_subroles/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:03 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/hardware_state_manager/hsm_roles_and_subroles/</guid>
      <description>HSM Roles and Subroles The Hardware State Manager (HSM) contains several pre-defined roles and subroles that can be assigned to components and used to target specific hardware devices.
Roles and subroles assignments come from the System Layout Service (SLS) and are applied by HSM when a node is discovered.
 HSM roles HSM subroles Add custom roles and subroles  HSM roles The following is a list of all pre-defined roles:</description>
    </item>
    
    <item>
      <title>Lock And Unlock Management Nodes</title>
      <link>/docs-csm/en-13/operations/hardware_state_manager/lock_and_unlock_management_nodes/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:03 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/hardware_state_manager/lock_and_unlock_management_nodes/</guid>
      <description>Lock and Unlock Management Nodes The ability to ignore non-compute nodes (NCNs) is turned off by default. Management nodes, NCNs, and their BMCs are also not locked by default. The administrator must lock the NCNs and their BMCs to prevent unwanted actions from affecting these nodes.
This section only covers using locks with the Hardware State Manager (HSM). For more information on ignoring nodes, refer to the following sections:
 Firmware Action Service (FAS): See Ignore Node within FAS.</description>
    </item>
    
    <item>
      <title>Add A Switch To The HSM Database</title>
      <link>/docs-csm/en-13/operations/hardware_state_manager/add_a_switch_to_the_hsm_database/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:02 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/hardware_state_manager/add_a_switch_to_the_hsm_database/</guid>
      <description>Add a Switch to the HSM Database Manually add a switch to the Hardware State Manager (HSM) database. Switches need to be in the HSM database in order to update their firmware with the Firmware Action Service (FAS).
Prerequisites  The Cray command line interface (CLI) tool is initialized and configured on the system. See Configure the Cray CLI. The component name (xname), IP address, user name, and password are known for the switch being added.</description>
    </item>
    
    <item>
      <title>Add An NCN To The HSM Database</title>
      <link>/docs-csm/en-13/operations/hardware_state_manager/add_an_ncn_to_the_hsm_database/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:02 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/hardware_state_manager/add_an_ncn_to_the_hsm_database/</guid>
      <description>Add an NCN to the HSM Database This procedure details how to customize the bare-metal non-compute node (NCN) on a system and add the NCN to the Hardware State Manager (HSM) database.
The examples in this procedure use ncn-w0003-nmn as the Customer Access Node (CAN). Use the correct CAN for the system.
Prerequisites  The Cray command line interface (CLI) tool is initialized and configured on the system. See Configure the Cray CLI.</description>
    </item>
    
    <item>
      <title>Component Group Members</title>
      <link>/docs-csm/en-13/operations/hardware_state_manager/component_group_members/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:02 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/hardware_state_manager/component_group_members/</guid>
      <description>Component Group Members The members object in the group definition has additional actions available for managing the members after the group has been created.
The following is an example of group members:
{ &amp;#34;ids&amp;#34; : [ &amp;#34;x0c0s0b0n0&amp;#34;,&amp;#34;x0c0s0b0n1&amp;#34;,&amp;#34;x0c0s0b1n0&amp;#34; ] } Retrieve Group Members Retrieve just the members array for a group:
cray hsm groups members list GROUP_LABEL Retrieve only the members of a group that are also in a specific partition:</description>
    </item>
    
    <item>
      <title>Component Groups And Partitions</title>
      <link>/docs-csm/en-13/operations/hardware_state_manager/component_groups_and_partitions/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:02 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/hardware_state_manager/component_groups_and_partitions/</guid>
      <description>Component Groups and Partitions The Hardware State Manager (HSM) provides the group and partition services. Both are means of grouping (also known as labeling) system components that are tracked by HSM. Components include the nodes, blades, controllers, and more on a system.
There is no limit to the number of members a group or partition contains. The only limitation is that all members must be actual members of the system. The HSM needs to know that the components exist.</description>
    </item>
    
    <item>
      <title>Component Memberships</title>
      <link>/docs-csm/en-13/operations/hardware_state_manager/component_memberships/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:02 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/hardware_state_manager/component_memberships/</guid>
      <description>Component Memberships Memberships are a read-only resource that is generated automatically by changes to groups and partitions. Each component in /hsm/v2/State/Components is represented. Filter options are available to prune the list, or a specific component name (xname) can be given. All groups and the partition (if any) of each component are listed.
At this point in time, only information about node components is needed. The --type node filter option is used in the commands below to retrieve information about node memberships only.</description>
    </item>
    
    <item>
      <title>Component Partition Members</title>
      <link>/docs-csm/en-13/operations/hardware_state_manager/component_partition_members/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:02 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/hardware_state_manager/component_partition_members/</guid>
      <description>Component Partition Members The members object in the partition definition has additional actions available for managing the members after the partition has been created.
The following is an example of partition members:
{ &amp;#34;ids&amp;#34; : [ &amp;#34;x0c0s0b0n0&amp;#34;,&amp;#34;x0c0s0b0n1&amp;#34;,&amp;#34;x0c0s0b1n0&amp;#34;,&amp;#34;x0c0s0b1n1&amp;#34; ] } Retrieve Partition Members Retrieving members of a partition is very similar to how group members are retrieved and modified. No filtering options are available in partitions. However, there are partition and group filtering parameters for the /hsm/v2/State/Components and /hsm/v2/memberships collections, with both essentially working the same way.</description>
    </item>
    
    <item>
      <title>FAS Recipes</title>
      <link>/docs-csm/en-13/operations/firmware/fas_recipes/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:01 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/firmware/fas_recipes/</guid>
      <description>FAS Recipes  NOTE This is a collection of various FAS recipes for performing updates. For step by step directions and commands, see FAS Use Cases.
 The following example JSON files are useful to reference when updating specific hardware components. In all of these examples, the overrideDryrun field will be set to false; set them to true to perform a live update.
When updating an entire system, walk down the device hierarchy component type by component type, starting first with routers (switches), proceeding to chassis, and then finally to nodes.</description>
    </item>
    
    <item>
      <title>FAS Use Cases</title>
      <link>/docs-csm/en-13/operations/firmware/fas_use_cases/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:01 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/firmware/fas_use_cases/</guid>
      <description>FAS Use Cases Use the Firmware Action Service (FAS) to update the firmware on supported hardware devices. Each procedure includes the prerequisites and example recipes required to update the firmware.
When updating an entire system, walk down the device hierarchy component type by component type, starting first with Routers (switches), proceeding to Chassis, and then finally to Nodes. While this is not strictly necessary, it does help eliminate confusion.
NOTE: Any node that is locked remains in the state inProgress with the stateHelper message of &amp;quot;failed to lock&amp;quot; until the action times out, or the lock is released.</description>
    </item>
    
    <item>
      <title>Update Firmware With FAS</title>
      <link>/docs-csm/en-13/operations/firmware/update_firmware_with_fas/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:01 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/firmware/update_firmware_with_fas/</guid>
      <description>Update Firmware with FAS If FAS has not yet been installed, firmware for NCNs can be updated manually without FAS. See Updating Firmware without FAS.
The Firmware Action Service (FAS) provides an interface for managing firmware versions of Redfish-enabled hardware in the system. FAS interacts with the Hardware State Managers (HSM), device data, and image data in order to update firmware.
Gigabyte nodes that are having issues with ipmitool, Redfish, or flashing firmware may need to be reset to factory defaults.</description>
    </item>
    
    <item>
      <title>Update Ilo 5 Firmware Above V2.78</title>
      <link>/docs-csm/en-13/operations/firmware/fas_update_ilo5_2.78/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:01 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/firmware/fas_update_ilo5_2.78/</guid>
      <description>Update iLO 5 firmware above v2.78 This procedure is only required if the current version if iLO 5 is below v2.78.
iLO 5 versions above v2.78 are larger than 32MB in size. iLO 5 versions prior to v2.78 will only accept 32MB size binary files. iLO 5 v2.78 removed this restriction. To upgrade to a version after v2.78, iLO 5 must first be update to v2.78.
Find Image Id for iLO 5 version 2.</description>
    </item>
    
    <item>
      <title>Updating BMC Firmware And Bios For NCN-m001</title>
      <link>/docs-csm/en-13/operations/firmware/updating_firmware_m001/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:01 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/firmware/updating_firmware_m001/</guid>
      <description>Updating BMC Firmware and BIOS for ncn-m001 Retrieve the model name and firmware image required to update an HPE or Gigabyte ncn-m001 node.
 NOTE
 On HPE nodes, the BMC firmware is iLO 5 and BIOS is System ROM. The commands in the procedure must be run on ncn-m001.    Prerequisites Find the model name Get the firmware images Flash the firmware  Flash Gigabyte ncn-m001 Flash HPE ncn-m001    Prerequisites  WARNING: This procedure should not be performed during a CSM install while ncn-m001 is booted as the PIT node using a remote ISO image.</description>
    </item>
    
    <item>
      <title>Updating BMC Firmware And Bios For NCNs Without FAS</title>
      <link>/docs-csm/en-13/operations/firmware/updating_firmware_without_fas/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:01 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/firmware/updating_firmware_without_fas/</guid>
      <description>Updating BMC Firmware and BIOS for NCNs without FAS  NOTE
 On HPE nodes, the BMC firmware is iLO 5 and BIOS is System ROM. The commands in the procedure must be run on ncn-m001. This procedure should only be used if FAS is not available, such as during initial CSM install. In order to update the firmware or BIOS for ncn-m001 itself, see Updating BMC Firmware and BIOS for ncn-m001.</description>
    </item>
    
    <item>
      <title>Upload BMC Recovery Firmware Into Tftp Server</title>
      <link>/docs-csm/en-13/operations/firmware/upload_olympus_bmc_recovery_firmware_into_tftp_server/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:01 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/firmware/upload_olympus_bmc_recovery_firmware_into_tftp_server/</guid>
      <description>Upload BMC Recovery Firmware into TFTP Server cray-upload-recovery-images is a utility for uploading the BMC recovery files for ChassisBMCs, NodeBMCs, and RouterBMCs to be served by the cray-tftp service. The tool uses the cray CLI (fas, artifacts) and cray-tftp to download the S3 recovery images (as remembered by FAS), then upload them into the PVC that is used by cray-tftp. cray-upload-recovery-images should be run on every system.
Prerequisites  Cray System Management (CSM) software is installed.</description>
    </item>
    
    <item>
      <title>FAS Admin Procedures</title>
      <link>/docs-csm/en-13/operations/firmware/fas_admin_procedures/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:00 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/firmware/fas_admin_procedures/</guid>
      <description>FAS Admin Procedures Procedures for leveraging the Firmware Action Service (FAS) CLI to manage firmware.
Topics  Warning for Non-Compute Nodes (NCNs) Declarative vs Imperative FAS Updates Ignore Nodes within FAS Override an Image for an Update Check for New Firmware Versions with a Dry-Run Load Firmware from Nexus Load Firmware from RPM or ZIP file   Warning for Non-Compute Nodes (NCNs) NCNs and their BMCs must be locked with the HSM locking API to ensure they are not unintentionally updated by FAS.</description>
    </item>
    
    <item>
      <title>FAS Cli</title>
      <link>/docs-csm/en-13/operations/firmware/fas_cli/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:00 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/firmware/fas_cli/</guid>
      <description>FAS CLI This section describes the basic capabilities of the Firmware Action Service (FAS) CLI commands. These commands can be used to manage firmware for system hardware supported by FAS. Refer to the prerequisites section before proceeding to any of the sections for the supported operations.
The following CLI operations are described:
 Prerequisites Actions  Execute an action  Procedure   Abort an action  Procedure   Describe an action  Interpreting output Procedure  Get high level summary Get details of action Get details of operation       Snapshots  Create a snapshot  Procedure   List snapshots  Procedure   View snapshots  Procedure     Update a firmware image  Procedure   FAS loader commands  Loader status Load firmware from Nexus Load individual RPM or ZIP into FAS Display results of loader run Delete loader run data    Prerequisites The Cray command line interface (CLI) tool is initialized and configured on the system.</description>
    </item>
    
    <item>
      <title>FAS Filters</title>
      <link>/docs-csm/en-13/operations/firmware/fas_filters/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:00 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/firmware/fas_filters/</guid>
      <description>FAS Filters FAS uses five primary filters for actions and snapshots to determine what operations to create. The filters are listed below:
 Selection Filters - Determine what operations will be created. The following selection filters are available:  stateComponentFilter targetFilter inventoryHardwareFilter  imageFilter   Command Filters - Determine how the operations will be executed. The following command filters are available:  command    All filters are logically connected with AND logic.</description>
    </item>
    
    <item>
      <title>Troubleshoot Conman Blocking Access To A Node BMC</title>
      <link>/docs-csm/en-13/operations/conman/troubleshoot_conman_blocking_access_to_a_node_bmc/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:00 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/conman/troubleshoot_conman_blocking_access_to_a_node_bmc/</guid>
      <description>Troubleshoot ConMan Blocking Access to a Node BMC Disable ConMan if it is blocking access to a node by other means. ConMan runs on the system as a containerized service, and it is enabled by default. However, the use of ConMan to connect to a node blocks access to that node by other Serial over LAN (SOL) utilities or by a virtual KVM.
For information about how ConMan works, see ConMan.</description>
    </item>
    
    <item>
      <title>Troubleshoot Conman Failing To Connect To A Console</title>
      <link>/docs-csm/en-13/operations/conman/troubleshoot_conman_failing_to_connect_to_a_console/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:00 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/conman/troubleshoot_conman_failing_to_connect_to_a_console/</guid>
      <description>Troubleshoot ConMan Failing to Connect to a Console There are many reasons that ConMan may not be able to connect to a specific console. This procedure outlines several things to check that may impact the connectivity with a console.
Prerequisites This procedure requires administrative privileges.
Procedure   (ncn-mw#) Find the cray-console-operator pod.
OP_POD=$(kubectl get pods -n services -o wide|grep cray-console-operator|awk &amp;#39;{print $1}&amp;#39;) echo ${OP_POD} Example output:
cray-console-operator-6cf89ff566-kfnjr   (ncn-mw#) Set the XNAME variable to the component name (xname) of the node whose console is of interest.</description>
    </item>
    
    <item>
      <title>Troubleshoot Console Node Pod Stuck In Terminating State</title>
      <link>/docs-csm/en-13/operations/conman/troubleshoot_conman_node_pod_stuck_terminating/</link>
      <pubDate>Mon, 17 Jul 2023 03:17:00 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/conman/troubleshoot_conman_node_pod_stuck_terminating/</guid>
      <description>Troubleshoot Console Node Pod Stuck in Terminating State When a worker node goes down unexpectedly, a cray-console-node pod running on that worker may get stuck in a Terminating state that prevents it moving to a healthy worker. This can leave consoles unmonitored and not available for interactive access.
Prerequisite Determine if this is a scenario where the cray-console-node pod is stuck in Terminating because of the failure of the worker node it is running on, or for some other reason.</description>
    </item>
    
    <item>
      <title>Conman</title>
      <link>/docs-csm/en-13/operations/conman/conman/</link>
      <pubDate>Mon, 17 Jul 2023 03:16:59 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/conman/conman/</guid>
      <description>ConMan ConMan is a tool used for connecting to remote consoles and collecting console logs. These node logs can then be used for various administrative purposes, such as troubleshooting node boot issues.
ConMan runs on the system as a containerized service. It runs in a set of Docker containers within Kubernetes pods named cray-console-operator and cray-console-node. Node console logs are stored locally within the cray-console-node pods in the /var/log/conman/ directory, as well as being collected by the System Monitoring Framework (SMF).</description>
    </item>
    
    <item>
      <title>Disable Conman After The System Software Installation</title>
      <link>/docs-csm/en-13/operations/conman/disable_conman_after_system_software_installation/</link>
      <pubDate>Mon, 17 Jul 2023 03:16:59 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/conman/disable_conman_after_system_software_installation/</guid>
      <description>Disable ConMan After the System Software Installation The ConMan utility is enabled by default. The first procedure provides instructions for disabling it after the system software has been installed, and the second procedure provides instructions on how to later re-enable it.
Prerequisites This procedure requires administrative privileges.
Disable Procedure  NOTE this procedure has changed since the CSM 0.9 release.
   Log on to a Kubernetes master or worker node.</description>
    </item>
    
    <item>
      <title>Establish A Serial Connection To NCNs</title>
      <link>/docs-csm/en-13/operations/conman/establish_a_serial_connection_to_ncns/</link>
      <pubDate>Mon, 17 Jul 2023 03:16:59 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/conman/establish_a_serial_connection_to_ncns/</guid>
      <description>Establish a Serial Connection to NCNs The ConMan pod can be used to establish a serial console connection with each non-compute node (NCN) in the system.
In the scenario of a power down or reboot of an NCN worker, one must first determine if any cray-console pods are running on that NCN. It is important to move cray-console pods to other worker nodes before rebooting or powering off a worker node to minimize disruption in console logging.</description>
    </item>
    
    <item>
      <title>Log In To A Node Using Conman</title>
      <link>/docs-csm/en-13/operations/conman/log_in_to_a_node_using_conman/</link>
      <pubDate>Mon, 17 Jul 2023 03:16:59 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/conman/log_in_to_a_node_using_conman/</guid>
      <description>Log in to a Node Using ConMan This procedure shows how to connect to the node&amp;rsquo;s Serial Over Lan (SOL) via ConMan.
Prerequisites The user performing this procedure needs to have access permission to the cray-console-operator and cray-console-node pods.
Procedure  NOTE this procedure has changed since the CSM 0.9 release.
   Log on to a Kubernetes master or worker node.
  Find the cray-console-operator pod.
OP_POD=$(kubectl get pods -n services \  -o wide|grep cray-console-operator|awk &amp;#39;{print $1}&amp;#39;) echo $OP_POD Example output:</description>
    </item>
    
    <item>
      <title>Manage Node Consoles</title>
      <link>/docs-csm/en-13/operations/conman/manage_node_consoles/</link>
      <pubDate>Mon, 17 Jul 2023 03:16:59 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/conman/manage_node_consoles/</guid>
      <description>Manage Node Consoles ConMan is used for connecting to remote consoles and collecting console logs. These node logs can then be used for various administrative purposes, such as troubleshooting node boot issues.
ConMan runs on the system in a set of containers within Kubernetes pods named cray-console-operator and cray-console-node.
The cray-console-operator and cray-console-node pods determine which nodes they should monitor by checking with the Hardware State Manager (HSM) service. They do this once when they starts.</description>
    </item>
    
    <item>
      <title>Troubleshoot Conman Asking For Password On SSH Connection</title>
      <link>/docs-csm/en-13/operations/conman/troubleshoot_conman_asking_for_password_on_ssh_connection/</link>
      <pubDate>Mon, 17 Jul 2023 03:16:59 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/conman/troubleshoot_conman_asking_for_password_on_ssh_connection/</guid>
      <description>Troubleshoot ConMan Asking for Password on SSH Connection If ConMan starts to ask for a password when there is an SSH connection to the node on liquid-cooled hardware, that usually indicates there is a problem with the SSH key that was established on the node BMC. The key may have been replaced or overwritten on the hardware.
Use this procedure to renew or reinstall the SSH key on the BMCs.</description>
    </item>
    
    <item>
      <title>Access Compute Node Logs</title>
      <link>/docs-csm/en-13/operations/conman/access_compute_node_logs/</link>
      <pubDate>Mon, 17 Jul 2023 03:16:58 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/conman/access_compute_node_logs/</guid>
      <description>Access Compute Node Logs This procedure shows how the ConMan utility can be used to retrieve compute node logs.
Prerequisites The user performing this procedure needs to have access permission to the cray-console-operator pod.
Limitations Encryption of compute node logs is not enabled, so the passwords may be passed in clear text.
Procedure  NOTE this procedure has changed since the CSM 0.9 release.
   Log on to a Kubernetes master or worker node.</description>
    </item>
    
    <item>
      <title>Access Console Log Data Via The System Monitoring Framework (smf)</title>
      <link>/docs-csm/en-13/operations/conman/access_console_log_data_via_the_system_monitoring_framework_smf/</link>
      <pubDate>Mon, 17 Jul 2023 03:16:58 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/conman/access_console_log_data_via_the_system_monitoring_framework_smf/</guid>
      <description>Access Console Log Data Via the System Monitoring Framework (SMF) Console log data is collected by SMF and can be queried through the Kibana UI or Elasticsearch. Each line of the console logs are an individual record in the SMF database.
 Prerequisites System domain name Procedure  Prerequisites This procedure requires the Kibana service to be up and running on a non-compute node (NCN).
System domain name The SYSTEM_DOMAIN_NAME value found in some of the URLs on this page is expected to be the system&amp;rsquo;s fully qualified domain name (FQDN).</description>
    </item>
    
    <item>
      <title>Configure The Cray Command Line Interface (cray Cli)</title>
      <link>/docs-csm/en-13/operations/configure_cray_cli/</link>
      <pubDate>Mon, 17 Jul 2023 03:16:58 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/configure_cray_cli/</guid>
      <description>Configure the Cray Command Line Interface (cray CLI) The cray command line interface (CLI) is a framework created to integrate all of the system management REST APIs into easily usable commands.
Procedures in the CSM installation workflow use the cray CLI to interact with multiple services. The cray CLI configuration needs to be initialized for the Linux account, and the Keycloak user running the procedure needs to be authorized. This section describes how to initialize the cray CLI for use by a user and how to authorize that user.</description>
    </item>
    
    <item>
      <title>Worker Image Customization</title>
      <link>/docs-csm/en-13/operations/configuration_management/worker_image_customization/</link>
      <pubDate>Mon, 17 Jul 2023 03:16:58 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/configuration_management/worker_image_customization/</guid>
      <description>Worker Image Customization  Background Prerequisites Procedure  Background NCN image customization refers to the process of CFS applying a configuration directly to an NCN image. During CSM installs and upgrades1, NCN image customization must be performed on the NCN worker node image, and the worker NCNs must then be configured to boot from the customized image. The purpose is to ensure that the appropriate CFS layers are applied to the NCN worker image before the workers are booted.</description>
    </item>
    
    <item>
      <title>Write Ansible Code For CFS</title>
      <link>/docs-csm/en-13/operations/configuration_management/write_ansible_code_for_cfs/</link>
      <pubDate>Mon, 17 Jul 2023 03:16:58 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/configuration_management/write_ansible_code_for_cfs/</guid>
      <description>Write Ansible Code for CFS HPE Cray provides Ansible plays and roles for software products deemed necessary for the system to function. Customers are free to write their own Ansible plays and roles to augment what HPE Cray provides or implement new features. Basic knowledge of Ansible is needed to write plays and roles. The information below includes recommendations and best practices for writing and running Ansible code on the system successfully with the Configuration Framework Service (CFS).</description>
    </item>
    
    <item>
      <title>Use A Custom Ansible.cfg File</title>
      <link>/docs-csm/en-13/operations/configuration_management/use_a_custom_ansible-cfg_file/</link>
      <pubDate>Mon, 17 Jul 2023 03:16:57 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/configuration_management/use_a_custom_ansible-cfg_file/</guid>
      <description>Use a Custom ansible.cfg File The Configuration Framework Service (CFS) allows for flexibility with the Ansible Execution Environment (AEE) by allowing for changes to included ansible.cfg file. When installed, CFS imports a custom ansible.cfg file into the cfs-default-ansible-cfg Kubernetes ConfigMap in the services namespace.
Administrators who want to make changes to the ansible.cfg file on a per-session or system-wide basis can upload a new file to a new ConfigMap in the services namespace, and then direct CFS to use their file.</description>
    </item>
    
    <item>
      <title>Use A Specific Inventory In A Configuration Session</title>
      <link>/docs-csm/en-13/operations/configuration_management/use_a_specific_inventory_in_a_configuration_session/</link>
      <pubDate>Mon, 17 Jul 2023 03:16:57 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/configuration_management/use_a_specific_inventory_in_a_configuration_session/</guid>
      <description>Use a Specific Inventory in a Configuration Session A special repository can be added to a Configuration Framework Service (CFS) configuration to help with certain scenarios, specifically when developing Ansible plays for use on the system. A static inventory often changes along with the Ansible content, and CFS users may need to test different configuration values simultaneously and not be forced to use the global additionalInventoryUrl.
Therefore, an additional_inventory mapping can be added to the CFS configuration.</description>
    </item>
    
    <item>
      <title>Vcs Branching Strategy</title>
      <link>/docs-csm/en-13/operations/configuration_management/vcs_branching_strategy/</link>
      <pubDate>Mon, 17 Jul 2023 03:16:57 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/configuration_management/vcs_branching_strategy/</guid>
      <description>VCS Branching Strategy Individual products import configuration content (Ansible plays, roles, and more) into a repository in the Version Control Service (VCS) through their installation process. Typically, this repository exists in the cray organization in VCS and its name has the format [product name]-config-management.
The import branch of the product is considered &amp;ldquo;pristine content&amp;rdquo; and is added to VCS in a read-only branch. This step is taken to ensure the future updates of the product&amp;rsquo;s configuration content can be based on a clean branch, and that upgrades can proceed without merging issues.</description>
    </item>
    
    <item>
      <title>Version Control Service (vcs)</title>
      <link>/docs-csm/en-13/operations/configuration_management/version_control_service_vcs/</link>
      <pubDate>Mon, 17 Jul 2023 03:16:57 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/configuration_management/version_control_service_vcs/</guid>
      <description>Version Control Service (VCS)  VCS overview Cloning a VCS repository VCS administrative user  Change VCS administrative user password   Access the cray Gitea organization Backup and restore data  Backup Postgres data Backup PVC data Restore Postgres data Restore PVC data Alternative backup/restore strategy  Alternative export method Alternative import method      VCS overview The Version Control Service (VCS) includes a web interface for repository management, pull requests, and a visual view of all repositories and organizations.</description>
    </item>
    
    <item>
      <title>View Configuration Session Logs</title>
      <link>/docs-csm/en-13/operations/configuration_management/view_configuration_session_logs/</link>
      <pubDate>Mon, 17 Jul 2023 03:16:57 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/configuration_management/view_configuration_session_logs/</guid>
      <description>View Configuration Session Logs Logs for the individual steps of a session are available via the kubectl log command for each container of a Configuration Framework Service (CFS) session. Refer to Configuration Sessions for more info about these containers.
These can be run on any master or worker NCN.
To find the name of the Kubernetes pod that is running the CFS session:
kubectl get pods --no-headers -o custom-columns=&amp;#34;:metadata.name&amp;#34; -n services -l cfsession=example Store the returned pod name as the CFS_POD_NAME variable for future use:</description>
    </item>
    
    <item>
      <title>Track The Status Of A Session</title>
      <link>/docs-csm/en-13/operations/configuration_management/track_the_status_of_a_session/</link>
      <pubDate>Mon, 17 Jul 2023 03:16:56 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/configuration_management/track_the_status_of_a_session/</guid>
      <description>Track the Status of a Session A configuration session can be a long-running process, and depends on many system factors, as well as the number of configuration layers and Ansible tasks that are run in each layer. The Configuration Framework Service (CFS) provides the session status through the session metadata to allow for tracking progress and session state.
 Prerequisites View session status Troubleshooting  Prerequisites  A configuration session exists in CFS.</description>
    </item>
    
    <item>
      <title>Troubleshoot Ansible Play Failures In CFS Sessions</title>
      <link>/docs-csm/en-13/operations/configuration_management/troubleshoot_ansible_play_failures_in_cfs_sessions/</link>
      <pubDate>Mon, 17 Jul 2023 03:16:56 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/configuration_management/troubleshoot_ansible_play_failures_in_cfs_sessions/</guid>
      <description>Troubleshoot Ansible Play Failures in CFS Sessions View the Kubernetes logs for a Configuration Framework Service (CFS) pod in an error state to determine whether the error resulted from the CFS infrastructure or from an Ansible play that was run by a specific configuration layer in a CFS session.
Use this procedure to obtain important triage information for Ansible plays being called by CFS.
Prerequisites  A failed configuration session exists in CFS.</description>
    </item>
    
    <item>
      <title>Troubleshoot CFS Session Failing To Complete</title>
      <link>/docs-csm/en-13/operations/configuration_management/troubleshoot_cfs_session_failing_to_complete/</link>
      <pubDate>Mon, 17 Jul 2023 03:16:56 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/configuration_management/troubleshoot_cfs_session_failing_to_complete/</guid>
      <description>Troubleshoot CFS Session Failing to Complete Troubleshoot issues where Configuration Framework Service (CFS) sessions/pods fail and Ansible hangs. These issues can be resolved by modifying Ansible to produce less output.
Prerequisites A CFS session or pod is failing to complete, and the Ansible logs are not showing progress or completion.
The following is an example of the error causing Ansible to hang:
PLAY [Compute] ***************************************************************** META: ran handlers META: ran handlers META: ran handlers PLAY [Compute] ***************************************************************** Using module file /usr/lib/python3.</description>
    </item>
    
    <item>
      <title>Troubleshoot CFS Sessions Failing To Start</title>
      <link>/docs-csm/en-13/operations/configuration_management/troubleshoot_cfs_sessions_failing_to_start/</link>
      <pubDate>Mon, 17 Jul 2023 03:16:56 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/configuration_management/troubleshoot_cfs_sessions_failing_to_start/</guid>
      <description>Troubleshoot CFS Sessions Failing to Start Troubleshoot issues where Configuration Framework Service (CFS) sessions are being created, but the pods are never created and never run.
Prerequisites CFS-batcher is creating automatic sessions, but pods are not starting for those sessions. There are a number of communication reasons this could be happening, so check the cfs-batcher and cfs-operator logs for these signatures.
CFS-batcher logs should show that it is creating sessions, but giving up waiting for those sessions to start:</description>
    </item>
    
    <item>
      <title>Update A CFS Configuration</title>
      <link>/docs-csm/en-13/operations/configuration_management/update_a_cfs_configuration/</link>
      <pubDate>Mon, 17 Jul 2023 03:16:56 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/configuration_management/update_a_cfs_configuration/</guid>
      <description>Update a CFS Configuration Modify a Configuration Framework Service (CFS) configuration by specifying the JSON of the configuration and its layers. Use the cray cfs configurations update command, similar to creating a configuration.
Prerequisites  A CFS configuration has been created. The Cray command line interface (CLI) tool is initialized and configured on the system. See Configure the Cray CLI.  Procedure   (ncn-mw#) Add and/or remove the configuration layers from an existing JSON configuration file.</description>
    </item>
    
    <item>
      <title>Update The Privacy Settings For Gitea Configuration Content Repositories</title>
      <link>/docs-csm/en-13/operations/configuration_management/update_the_privacy_settings_for_gitea_configuration_content_repositories/</link>
      <pubDate>Mon, 17 Jul 2023 03:16:56 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/configuration_management/update_the_privacy_settings_for_gitea_configuration_content_repositories/</guid>
      <description>Update the Privacy Settings for Gitea Configuration Content Repositories Change the visibility of Gitea configuration content repositories from public to private. All Cray-provided repositories are created as private by default.
Prerequisites  Know the system&amp;rsquo;s external fully qualified domain name, referred to on this page as SYSTEM_DOMAIN_NAME. See System domain name for more information.  Procedure   Log in to the Version Control Service (VCS) as the crayvcs user.</description>
    </item>
    
    <item>
      <title>NCN Node Personalization</title>
      <link>/docs-csm/en-13/operations/configuration_management/ncn_node_personalization/</link>
      <pubDate>Mon, 17 Jul 2023 03:16:55 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/configuration_management/ncn_node_personalization/</guid>
      <description>NCN Node Personalization  Background Prerequisites Procedure  Preparation Remove layers for absent products Edit CPE and Analytics layers Disable CFS on management NCNs Update CFS configuration and components Cleanup    Background NCN node personalization refers to the process of CFS applying a configuration to a management NCN after it is booted. During CSM installs and upgrades1, this CFS configuration must be created and set as the desired configuration in CFS for all management NCNs.</description>
    </item>
    
    <item>
      <title>Set Limits For A Configuration Session</title>
      <link>/docs-csm/en-13/operations/configuration_management/set_limits_for_a_configuration_session/</link>
      <pubDate>Mon, 17 Jul 2023 03:16:55 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/configuration_management/set_limits_for_a_configuration_session/</guid>
      <description>Set Limits for a Configuration Session The configuration layers and session hosts can be limited when running a Configuration Framework Service (CFS) session.
Limit CFS session hosts Subsets of nodes can be targeted in the inventory when running CFS sessions, which is useful specifically when running a session with dynamic inventory. Use the CFS --ansible-limit option when creating a session to apply the limits. The option directly corresponds to the --limit option offered by ansible-playbook, and can be used to specify hosts, groups, or combinations of them with patterns.</description>
    </item>
    
    <item>
      <title>Set The Ansible.cfg For A Session</title>
      <link>/docs-csm/en-13/operations/configuration_management/set_the_ansible-cfg_for_a_session/</link>
      <pubDate>Mon, 17 Jul 2023 03:16:55 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/configuration_management/set_the_ansible-cfg_for_a_session/</guid>
      <description>Set the ansible.cfg for a Session View and update the Ansible configuration used by the Configuration Framework Service (CFS).
Ansible configuration is available through the ansible.cfg file. See the Configuring Ansible external documentation for more information about what values can be set.
CFS provides a default ansible.cfg file in the cfs-default-ansible-cfg Kubernetes ConfigMap in the services namespace.
(ncn-mw#) To view the ansible.cfg file:
kubectl get cm -n services cfs-default-ansible-cfg -o json | jq -r &amp;#39;.</description>
    </item>
    
    <item>
      <title>Specifying Hosts And Groups</title>
      <link>/docs-csm/en-13/operations/configuration_management/specifying_hosts_and_groups/</link>
      <pubDate>Mon, 17 Jul 2023 03:16:55 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/configuration_management/specifying_hosts_and_groups/</guid>
      <description>Specifying Hosts and Groups When using the Configuration Framework Service (CFS), there are many steps where users may need to specify the hosts that CFS should configure. This can be done by specifying individual hosts, or groups of hosts. There are several places where a user may need to provide this information, particularly groups, and depending on where this information is provided, the behavior can change greatly.
Inventories CFS has multiple options for generating inventories, but regardless of which option is used, the information is then converted into an Ansible inventory/hosts file.</description>
    </item>
    
    <item>
      <title>Target Ansible Tasks For Image Customization</title>
      <link>/docs-csm/en-13/operations/configuration_management/target_ansible_tasks_for_image_customization/</link>
      <pubDate>Mon, 17 Jul 2023 03:16:55 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/configuration_management/target_ansible_tasks_for_image_customization/</guid>
      <description>Target Ansible Tasks for Image Customization The Configuration Framework Service (CFS) enables Ansible playbooks to run against both running nodes (node personalization) and images prior to boot(image customization). See Configuration Management Use Cases for more information about image customization and when it should be used.
Using a host group Ansible is most efficient at skipping tasks when they don&amp;rsquo;t apply to any nodes in the current hosts target. For this reason it&amp;rsquo;s recommended that Ansible plays first setup a host group using the add_host module, and then use this group to target plays for image customization rather than using the when conditional.</description>
    </item>
    
    <item>
      <title>Enable Ansible Profiling</title>
      <link>/docs-csm/en-13/operations/configuration_management/enable_ansible_profiling/</link>
      <pubDate>Mon, 17 Jul 2023 03:16:54 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/configuration_management/enable_ansible_profiling/</guid>
      <description>Enable Ansible Profiling Ansible tasks and playbooks can be profiled in order to determine execution times and single out poor performance in runtime. The default Configuration Framework Service (CFS) ansible.cfg in the cfs-default-ansible-cfg ConfigMap does not enable these profiling tools. If profiling tools are desired, modify the default Ansible configuration file to enable them.
Procedure   Edit the cfs-default-ansible-cfg ConfigMap.
kubectl edit cm cfs-default-ansible-cfg -n services   Uncomment the indicated line by removing the # character from the beginning of the line.</description>
    </item>
    
    <item>
      <title>Exporting And Importing CFS Data</title>
      <link>/docs-csm/en-13/operations/configuration_management/exporting_and_importing_cfs_data/</link>
      <pubDate>Mon, 17 Jul 2023 03:16:54 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/configuration_management/exporting_and_importing_cfs_data/</guid>
      <description>Exporting and Importing CFS Data  Prerequisites Export Import  Prerequisites  Ensure that the cray command line interface (CLI) is authenticated and configured to talk to system management services.  See Configure the Cray CLI.   The latest CSM documentation RPM must be installed on the node where the procedure is being performed.  See Check for latest documentation.   If importing both CFS and VCS data, the VCS import should be done before the CFS import.</description>
    </item>
    
    <item>
      <title>Git Operations</title>
      <link>/docs-csm/en-13/operations/configuration_management/git_operations/</link>
      <pubDate>Mon, 17 Jul 2023 03:16:54 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/configuration_management/git_operations/</guid>
      <description>Git Operations Use the git command to manage repository content in the Version Control Service (VCS).
Once a repository is cloned, the git command line tool is available to interact with a repository from VCS. The git command is used for making commits, creating new branches, and pushing new branches, tags, and commits to the remote repository stored in VCS.
When pushing changes to the VCS server using the crayvcs user, input the password retrieved from the Kubernetes secret as the credentials.</description>
    </item>
    
    <item>
      <title>Manage Multiple Inventories In A Single Location</title>
      <link>/docs-csm/en-13/operations/configuration_management/manage_multiple_inventories_in_a_single_location/</link>
      <pubDate>Mon, 17 Jul 2023 03:16:54 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/configuration_management/manage_multiple_inventories_in_a_single_location/</guid>
      <description>Manage Multiple Inventories in a Single Location Many configuration layers may be present in a single configuration for larger systems that configure multiple Cray products. When values for each of these layers need to be customized, it can be tedious to override values in each of the respective repositories. The CFS additionalInventoryUrl option allows for static inventory files to be automatically added to the hosts directory of each configuration layer before it is applied by Ansible.</description>
    </item>
    
    <item>
      <title>Management Node Image Customization</title>
      <link>/docs-csm/en-13/operations/configuration_management/management_node_image_customization/</link>
      <pubDate>Mon, 17 Jul 2023 03:16:54 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/configuration_management/management_node_image_customization/</guid>
      <description>Management Node Image Customization NOTE: Some of the documentation linked from this page mentions use of the Boot Orchestration Service (BOS). The use of BOS is only relevant for booting compute nodes and should be ignored when working with NCN images.
This document describes the configuration of a Kubernetes NCN image. The same steps could be used to modify a Ceph NCN image with minor command modifications.
 Prerequisites Procedure  Obtain NCN image artifacts Import the NCN image into IMS Create a CFS configuration, if needed Run the CFS image customization session Update NCN boot parameters   Next steps  Prerequisites The Cray command line interface must be configured on the node where the commands are being run.</description>
    </item>
    
    <item>
      <title>Create A CFS Session With Dynamic Inventory</title>
      <link>/docs-csm/en-13/operations/configuration_management/create_a_cfs_session_with_dynamic_inventory/</link>
      <pubDate>Mon, 17 Jul 2023 03:16:53 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/configuration_management/create_a_cfs_session_with_dynamic_inventory/</guid>
      <description>Create a CFS Session with Dynamic Inventory A Configuration Framework Service (CFS) session using dynamic inventory is used to configure live nodes. To create a CFS session using the default dynamic inventory, simply provide a session name and the name of the configuration to apply:
cray cfs sessions create --name example \  --configuration-name configurations-example Example output:
{ &amp;#34;ansible&amp;#34;: { &amp;#34;config&amp;#34;: &amp;#34;cfs-default-ansible-cfg&amp;#34;, &amp;#34;limit&amp;#34;: &amp;#34;&amp;#34;, &amp;#34;verbosity&amp;#34;: 0 }, &amp;#34;configuration&amp;#34;: { &amp;#34;limit&amp;#34;: &amp;#34;&amp;#34;, &amp;#34;name&amp;#34;: &amp;#34;configurations-example&amp;#34; }, &amp;#34;name&amp;#34;: &amp;#34;example&amp;#34;, &amp;#34;status&amp;#34;: { &amp;#34;artifacts&amp;#34;: [], &amp;#34;session&amp;#34;: { &amp;#34;status&amp;#34;: &amp;#34;pending&amp;#34;, &amp;#34;succeeded&amp;#34;: &amp;#34;none&amp;#34; } }, &amp;#34;tags&amp;#34;: {}, &amp;#34;target&amp;#34;: { &amp;#34;definition&amp;#34;: &amp;#34;dynamic&amp;#34;, &amp;#34;groups&amp;#34;: null } } Add the --target-definition dynamic parameter to the create command to explicitly define the inventory type to be dynamic.</description>
    </item>
    
    <item>
      <title>Create An Image Customization CFS Session</title>
      <link>/docs-csm/en-13/operations/configuration_management/create_an_image_customization_cfs_session/</link>
      <pubDate>Mon, 17 Jul 2023 03:16:53 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/configuration_management/create_an_image_customization_cfs_session/</guid>
      <description>Create an Image Customization CFS Session A configuration session that is meant to customize image roots tracked by the Image Management Service (IMS) can be created using the --target-definition image option. This option will instruct the Configuration Framework Service (CFS) to prepare the image IDs specified and assign them to the groups specified in Ansible inventory. IMS will then provide SSH connection information to each image root that CFS will use to configure Ansible.</description>
    </item>
    
    <item>
      <title>Create And Populate A Vcs Configuration Repository</title>
      <link>/docs-csm/en-13/operations/configuration_management/create_and_populate_a_vcs_configuration_repository/</link>
      <pubDate>Mon, 17 Jul 2023 03:16:53 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/configuration_management/create_and_populate_a_vcs_configuration_repository/</guid>
      <description>Create and Populate a VCS Configuration Repository Create a new repository in the VCS and populate it with content for site customizations in a custom Configuration Framework Service (CFS) configuration layer.
Prerequisites  The Version Control Service (VCS) login credentials for the crayvcs user are set up. See VCS Administrative User in Version Control Service (VCS) for more information.  Procedure   (ncn-mw#) Create the empty repository in VCS.</description>
    </item>
    
    <item>
      <title>Customize Configuration Values</title>
      <link>/docs-csm/en-13/operations/configuration_management/customize_configuration_values/</link>
      <pubDate>Mon, 17 Jul 2023 03:16:53 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/configuration_management/customize_configuration_values/</guid>
      <description>Customize Configuration Values In general, most systems will require some customization from the default values provided by HPE Cray products. As stated in the previous section, these changes cannot be made on the pristine product branches that are imported during product installation and upgrades. Changes can only be made in Git branches that are based on the pristine branches.
Changing or overriding default values should be done in accordance with Ansible best practices (see the external Ansible best practices guide) and variable precedence (see the external Ansible variable guide) in mind.</description>
    </item>
    
    <item>
      <title>Delete CFS Sessions</title>
      <link>/docs-csm/en-13/operations/configuration_management/delete_cfs_sessions/</link>
      <pubDate>Mon, 17 Jul 2023 03:16:53 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/configuration_management/delete_cfs_sessions/</guid>
      <description>Delete CFS Sessions Delete an existing Configuration Framework Service (CFS) configuration session with the CFS delete command.
Prerequisites This requires that the Cray command line interface is configured. See Configure the Cray Command Line Interface.
Delete single CFS session Use the session name to delete the session:
cray cfs sessions delete &amp;lt;session_name&amp;gt; No output is expected.
Delete multiple CFS sessions To delete all completed CFS sessions, use the deleteall command.</description>
    </item>
    
    <item>
      <title>Configuration Management</title>
      <link>/docs-csm/en-13/operations/configuration_management/configuration_management/</link>
      <pubDate>Mon, 17 Jul 2023 03:16:52 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/configuration_management/configuration_management/</guid>
      <description>Configuration Management The Configuration Framework Service (CFS) is available on systems for remote execution and configuration management of nodes and boot images. This includes nodes available in the Hardware State Manager (HSM) inventory (compute, management, and application nodes), and boot images hosted by the Image Management Service (IMS).
CFS configures nodes and images via a gitops methodology. All configuration content is stored in a version control service (VCS), and is managed by authorized system administrators.</description>
    </item>
    
    <item>
      <title>Configuration Management Of System Components</title>
      <link>/docs-csm/en-13/operations/configuration_management/configuration_management_of_system_components/</link>
      <pubDate>Mon, 17 Jul 2023 03:16:52 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/configuration_management/configuration_management_of_system_components/</guid>
      <description>Configuration Management of System Components The configuration of individual system components is managed with the cray cfs components command. The Configuration Framework Service (CFS) contains a database of the configuration state of available hardware known to the Hardware State Manager (HSM). When new nodes are added to the HSM database, a CFS Hardware Sync Agent enters the component into the CFS database with a null state of configuration.
Administrators are able to set a desired CFS configuration for each component, and the CFS Batcher ensures the desired configuration state and the current configuration state match.</description>
    </item>
    
    <item>
      <title>Configuration Management With The CFS Batcher</title>
      <link>/docs-csm/en-13/operations/configuration_management/configuration_management_with_the_cfs_batcher/</link>
      <pubDate>Mon, 17 Jul 2023 03:16:52 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/configuration_management/configuration_management_with_the_cfs_batcher/</guid>
      <description>Configuration Management with the CFS Batcher Creating configuration sessions with the Configuration Framework Service (CFS) enables remote execution for configuring live nodes and boot images prior to booting. CFS also provides its Batcher component for configuration management of registered system components. The CFS Batcher periodically examines the aggregated configuration state of registered components and schedules CFS sessions against those that have not been configured to their desired state. The frequency of scheduling, the maximum number of components to schedule in the same CFS session, and the expiration time for scheduling less than full sessions are configurable.</description>
    </item>
    
    <item>
      <title>Configuration Sessions</title>
      <link>/docs-csm/en-13/operations/configuration_management/configuration_sessions/</link>
      <pubDate>Mon, 17 Jul 2023 03:16:52 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/configuration_management/configuration_sessions/</guid>
      <description>Configuration Sessions Once configurations have been created with the required layers and values set in the configuration repositories (or the additional inventory repository), create a Configuration Framework Session (CFS) session to apply the configuration to the targets.
Sessions are created via the Cray CLI or through the CFS REST API. A session stages Ansible inventory (whether dynamic, static, or image customization), launches Ansible Execution Environments (AEE) in order for each configuration layer in the service mesh, tears down the environments as required, and reports the session status to the CFS API.</description>
    </item>
    
    <item>
      <title>Create A CFS Configuration</title>
      <link>/docs-csm/en-13/operations/configuration_management/create_a_cfs_configuration/</link>
      <pubDate>Mon, 17 Jul 2023 03:16:52 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/configuration_management/create_a_cfs_configuration/</guid>
      <description>Create a CFS Configuration Create a Configuration Framework Service (CFS) configuration, which contains an ordered list of layers. Each layer is defined by a Git repository clone URL, a Git commit, a name, and the path in the repository to an Ansible playbook to execute.
Prerequisites  The Cray command line interface (CLI) tool is initialized and configured on the system.  Procedure   Create a JSON file to hold data about the CFS configuration.</description>
    </item>
    
    <item>
      <title>CFS Flow</title>
      <link>/docs-csm/en-13/operations/configuration_management/cfs_flow_diagrams/</link>
      <pubDate>Mon, 17 Jul 2023 03:16:51 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/configuration_management/cfs_flow_diagrams/</guid>
      <description>CFS Flow  Single Session Flow Automated Session Flow  Single Session Flow This section covers the components and actions taken when a user or service creates a session using the CFS sessions endpoint.
 A user creates a CFS configuration. A user creates a CFS session, causing a session record to be created. When a session record is created, the CFS-API also posts an event to a Kafka queue. The CFS-Operator is always monitoring the Kafka queue, and handles events as they come in.</description>
    </item>
    
    <item>
      <title>CFS Global Options</title>
      <link>/docs-csm/en-13/operations/configuration_management/cfs_global_options/</link>
      <pubDate>Mon, 17 Jul 2023 03:16:51 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/configuration_management/cfs_global_options/</guid>
      <description>CFS Global Options The Configuration Framework Service (CFS) provides a global service options endpoint for modifying the base configuration of the service itself.
View the options with the following command:
cray cfs options list --format json Example output:
{ &amp;#34;additionalInventoryUrl&amp;#34;: &amp;#34;&amp;#34;, &amp;#34;batchSize&amp;#34;: 25, &amp;#34;batchWindow&amp;#34;: 60, &amp;#34;batcherCheckInterval&amp;#34;: 10, &amp;#34;defaultAnsibleConfig&amp;#34;: &amp;#34;cfs-default-ansible-cfg&amp;#34;, &amp;#34;defaultBatcherRetryPolicy&amp;#34;: 1, &amp;#34;defaultPlaybook&amp;#34;: &amp;#34;site.yml&amp;#34;, &amp;#34;hardwareSyncInterval&amp;#34;: 10, &amp;#34;sessionTTL&amp;#34;: &amp;#34;7d&amp;#34; } The following are the CFS global options:
  additionalInventoryUrl
A Git clone URL to supply additional inventory content to all CFS sessions.</description>
    </item>
    
    <item>
      <title>CFS Key Management And Permission Denied Errors</title>
      <link>/docs-csm/en-13/operations/configuration_management/cfs_key_management/</link>
      <pubDate>Mon, 17 Jul 2023 03:16:51 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/configuration_management/cfs_key_management/</guid>
      <description>CFS Key Management and Permission Denied Errors Configuration Framework Service (CFS) manages its own keys separate from keys for communication between CFS and the components or images that it is configuring. These are separate from the keys used by users and should not need to be managed.
Permission Denied Errors If Ansible is unable to connect with its target and fails with an Unreachable - Permission denied error, the first place to check is the cfs-state-reporter on the target node.</description>
    </item>
    
    <item>
      <title>Change The Ansible VerBOSity Logs</title>
      <link>/docs-csm/en-13/operations/configuration_management/change_the_ansible_verbosity_logs/</link>
      <pubDate>Mon, 17 Jul 2023 03:16:51 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/configuration_management/change_the_ansible_verbosity_logs/</guid>
      <description>Change the Ansible Verbosity Logs It is useful to view the Ansible logs in a Configuration Framework Session (CFS) session with greater verbosity than the default. CFS sessions are able to set the Ansible verbosity from the command line when the session is created. The verbosity will apply to all configuration layers in the session.
Specify an integer using the &amp;ndash;ansible-verbosity option, where 1 = -v, 2 = -vv, and so on.</description>
    </item>
    
    <item>
      <title>Configuration Layers</title>
      <link>/docs-csm/en-13/operations/configuration_management/configuration_layers/</link>
      <pubDate>Mon, 17 Jul 2023 03:16:51 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/configuration_management/configuration_layers/</guid>
      <description>Configuration Layers The Configuration Framework Service (CFS) uses configuration layers to specify the location of configuration content that will be applied. Configurations may include one or more layers. Each layer is defined by a Git repository clone URL, a Git commit, a name (optional), and the path in the repository to an Ansible playbook to execute.
Configurations with a single layer are useful when testing out a new configuration on targets, or when configuring system components with one product at a time.</description>
    </item>
    
    <item>
      <title>Accessing SAT Bootprep Files</title>
      <link>/docs-csm/en-13/operations/configuration_management/accessing_sat_bootprep_files/</link>
      <pubDate>Mon, 17 Jul 2023 03:16:50 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/configuration_management/accessing_sat_bootprep_files/</guid>
      <description>Accessing sat bootprep Files When performing an upgrade, NCN image customization and node personalization must be performed with the NCN worker node image to ensure the appropriate CFS layers are applied. This step involves configuring CFS to use the default sat bootprep files from the hpc-csm-software-recipe repository and rebuilding the NCN worker nodes so they boot the newly customized image.
The following procedure describes how to access the CFS configuration. This procedure is used for both image customization and node personalization of NCNs.</description>
    </item>
    
    <item>
      <title>Ansible Execution Environments</title>
      <link>/docs-csm/en-13/operations/configuration_management/ansible_execution_environments/</link>
      <pubDate>Mon, 17 Jul 2023 03:16:50 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/configuration_management/ansible_execution_environments/</guid>
      <description>Ansible Execution Environments Configuration Framework Service (CFS) sessions are comprised of a single Kubernetes pod with several containers. The inventory and git-clone containers run first, and a teardown container runs last (if the session is running an image customization).
The container that runs the Ansible code cloned from the Git repositories in the configuration layers is the Ansible Execution Environments (AEE). The AEE is provided as a SLES-based docker image, which includes Ansible version 2.</description>
    </item>
    
    <item>
      <title>Ansible Inventory</title>
      <link>/docs-csm/en-13/operations/configuration_management/ansible_inventory/</link>
      <pubDate>Mon, 17 Jul 2023 03:16:50 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/configuration_management/ansible_inventory/</guid>
      <description>Ansible Inventory The Configuration Framework Service (CFS) provides several options for targeting nodes or boot images for configuration by Ansible. The contents of the Ansible inventory determine which nodes are available for configuration in each CFS session and how default configuration values can be customized. For more information on what it means to define an inventory, see Specifying Hosts and Groups.
The following are the inventory options provided by CFS:</description>
    </item>
    
    <item>
      <title>Automatic Session Deletion With Sessionttl</title>
      <link>/docs-csm/en-13/operations/configuration_management/automatic_session_deletion_with_sessionttl/</link>
      <pubDate>Mon, 17 Jul 2023 03:16:50 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/configuration_management/automatic_session_deletion_with_sessionttl/</guid>
      <description>Automatic Session Deletion with sessionTTL By default, the Configuration Framework Service (CFS) will delete completed CFS sessions whose start date was more than seven days prior. Kubernetes jobs associated with these sessions will also be deleted as part of this process. This is done to ensure that CFS sessions do not accumulate and eventually adversely affect the performance of the Kubernetes cluster.
For larger systems or systems that do frequent reboots of nodes that are configured with CFS sessions, this setting may need to be reduced.</description>
    </item>
    
    <item>
      <title>Upgrade Compute Nodes With CRUS</title>
      <link>/docs-csm/en-13/operations/compute_rolling_upgrades/upgrade_compute_nodes_with_crus/</link>
      <pubDate>Mon, 17 Jul 2023 03:16:50 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/compute_rolling_upgrades/upgrade_compute_nodes_with_crus/</guid>
      <description>Upgrade Compute Nodes with CRUS  NOTE CRUS was deprecated in CSM 1.2.0 and it will be removed in CSM 1.5.0. See the following links for more information:
 Rolling Upgrades with BOS V2 Deprecated features   Upgrade a set of compute nodes with the Compute Rolling Upgrade Service (CRUS). Manage the workload management status of nodes and quiesce each node before taking the node out of service and upgrading it.</description>
    </item>
    
    <item>
      <title>Compute Rolling Upgrades</title>
      <link>/docs-csm/en-13/operations/compute_rolling_upgrades/compute_rolling_upgrades/</link>
      <pubDate>Mon, 17 Jul 2023 03:16:49 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/compute_rolling_upgrades/compute_rolling_upgrades/</guid>
      <description>Compute Rolling Upgrades  NOTE CRUS was deprecated in CSM 1.2.0 and it will be removed in CSM 1.5.0. See the following links for more information:
 Rolling Upgrades with BOS V2 Deprecated features   The Compute Rolling Upgrade Service (CRUS) upgrades sets of compute nodes without requiring an entire set of nodes to be out of service at once. CRUS manages the workload management status of nodes, handling each of the following steps required to upgrade compute nodes:</description>
    </item>
    
    <item>
      <title>CRUS Workflow</title>
      <link>/docs-csm/en-13/operations/compute_rolling_upgrades/crus_workflow/</link>
      <pubDate>Mon, 17 Jul 2023 03:16:49 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/compute_rolling_upgrades/crus_workflow/</guid>
      <description>CRUS Workflow  NOTE CRUS was deprecated in CSM 1.2.0 and it will be removed in CSM 1.5.0. See the following links for more information:
 Rolling Upgrades with BOS V2 Deprecated features   The following workflow is intended to be a high-level overview of how to upgrade compute nodes. This workflow depicts how services interact with each other during the compute node upgrade process, and helps to provide a quicker and deeper understanding of how the system functions.</description>
    </item>
    
    <item>
      <title>Troubleshoot A Failed CRUS Session Because Of Bad Parameters</title>
      <link>/docs-csm/en-13/operations/compute_rolling_upgrades/troubleshoot_a_failed_crus_session_due_to_bad_parameters/</link>
      <pubDate>Mon, 17 Jul 2023 03:16:49 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/compute_rolling_upgrades/troubleshoot_a_failed_crus_session_due_to_bad_parameters/</guid>
      <description>Troubleshoot a Failed CRUS Session Because of Bad Parameters  NOTE CRUS was deprecated in CSM 1.2.0 and it will be removed in CSM 1.5.0. See the following links for more information:
 Rolling Upgrades with BOS V2 Deprecated features   A CRUS session must be deleted and recreated if it does not start or complete because of parameters having incorrect values.
The following are examples of incorrect parameters:</description>
    </item>
    
    <item>
      <title>Troubleshoot A Failed CRUS Session Because Of Unmet Conditions</title>
      <link>/docs-csm/en-13/operations/compute_rolling_upgrades/troubleshoot_a_failed_crus_session_due_to_unmet_conditions/</link>
      <pubDate>Mon, 17 Jul 2023 03:16:49 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/compute_rolling_upgrades/troubleshoot_a_failed_crus_session_due_to_unmet_conditions/</guid>
      <description>Troubleshoot a Failed CRUS Session Because of Unmet Conditions  NOTE CRUS was deprecated in CSM 1.2.0 and it will be removed in CSM 1.5.0. See the following links for more information:
 Rolling Upgrades with BOS V2 Deprecated features   If a CRUS session has any unmet conditions, adding or fixing them will cause the session to continue from wherever it got stuck. Updating other parts of the system to meet the required conditions of a CRUS session will unblock the upgrade session.</description>
    </item>
    
    <item>
      <title>Troubleshoot Nodes Failing To Upgrade In A CRUS Session</title>
      <link>/docs-csm/en-13/operations/compute_rolling_upgrades/troubleshoot_nodes_failing_to_upgrade_in_a_crus_session/</link>
      <pubDate>Mon, 17 Jul 2023 03:16:49 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/compute_rolling_upgrades/troubleshoot_nodes_failing_to_upgrade_in_a_crus_session/</guid>
      <description>Troubleshoot Nodes Failing to Upgrade in a CRUS Session  NOTE CRUS was deprecated in CSM 1.2.0 and it will be removed in CSM 1.5.0. See the following links for more information:
 Rolling Upgrades with BOS V2 Deprecated features   Troubleshoot compute nodes failing to upgrade during a Compute Rolling Upgrade Service (CRUS) session and rerun the session on the failed nodes.
When nodes are marked as failed they are added to the failed node group associated with the upgrade session, and the nodes are marked as down in the workload manager (WLM).</description>
    </item>
    
    <item>
      <title>Troubleshoot Compute Node Boot Issues Related To The Boot Script Service (bss)</title>
      <link>/docs-csm/en-13/operations/boot_orchestration/troubleshoot_compute_node_boot_issues_related_to_the_boot_script_service_bss/</link>
      <pubDate>Mon, 17 Jul 2023 03:16:48 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/boot_orchestration/troubleshoot_compute_node_boot_issues_related_to_the_boot_script_service_bss/</guid>
      <description>Troubleshoot Compute Node Boot Issues Related to the Boot Script Service (BSS) Boot Script Service (BSS) delivers a boot script to a node based on its MAC address. This boot script tells the node where to obtain its boot artifacts, which include:
 kernel initrd  In addition, the boot script also contains the kernel boot parameters. This procedure helps resolve issues related to missing boot artifacts.
Prerequisites This procedure requires administrative privileges.</description>
    </item>
    
    <item>
      <title>Troubleshoot Compute Node Boot Issues Related To Unified Extensible Firmware Interface (uefi)</title>
      <link>/docs-csm/en-13/operations/boot_orchestration/troubleshoot_compute_node_boot_issues_related_to_unified_extensible_firmware_interface_uefi/</link>
      <pubDate>Mon, 17 Jul 2023 03:16:48 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/boot_orchestration/troubleshoot_compute_node_boot_issues_related_to_unified_extensible_firmware_interface_uefi/</guid>
      <description>Troubleshoot Compute Node Boot Issues Related to Unified Extensible Firmware Interface (UEFI) If a node is stuck in the UEFI shell, ConMan will be able to connect to it, but nothing else will appear in its logs. The node&amp;rsquo;s logs will look similar to the following, indicating that ConMan is updating its log hourly:
&amp;lt;ConMan&amp;gt; Console [86] log at 2018-09-07 20:00:00 CDT. &amp;lt;ConMan&amp;gt; Console [86] log at 2018-09-07 21:00:00 CDT. &amp;lt;ConMan&amp;gt; Console [86] log at 2018-09-07 22:00:00 CDT.</description>
    </item>
    
    <item>
      <title>Troubleshoot Compute Node Boot Issues Using Kubernetes</title>
      <link>/docs-csm/en-13/operations/boot_orchestration/troubleshoot_compute_node_boot_issues_using_kubernetes/</link>
      <pubDate>Mon, 17 Jul 2023 03:16:48 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/boot_orchestration/troubleshoot_compute_node_boot_issues_using_kubernetes/</guid>
      <description>Troubleshoot Compute Node Boot Issues Using Kubernetes A number of Kubernetes commands can be used to debug issues related to the node boot process. All of the traffic bound for the DHCP server, TFTP server, and Boot Script Service (BSS) is sent on the Node Management Network (NMN).
In the current arrangement, all three services are located on a non-compute node (NCN). Thus, traffic must first travel through the NCN to reach these services inside their pods.</description>
    </item>
    
    <item>
      <title>Troubleshoot UAN Boot Issues</title>
      <link>/docs-csm/en-13/operations/boot_orchestration/troubleshoot_uan_boot_issues/</link>
      <pubDate>Mon, 17 Jul 2023 03:16:48 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/boot_orchestration/troubleshoot_uan_boot_issues/</guid>
      <description>Troubleshoot UAN Boot Issues Use this topic to guide troubleshooting of UAN boot issues.
The UAN boot process BOS boots UANs. BOS uses session templates to define various parameters such as:
 Which nodes to boot Which image to boot Kernel parameters Whether to perform post-boot configuration (Node Personalization) of the nodes by CFS. Which CFS configuration to use if Node Personalization is enabled.  UAN boots are performed in three phases:</description>
    </item>
    
    <item>
      <title>Upload Node Boot Information To Boot Script Service (bss)</title>
      <link>/docs-csm/en-13/operations/boot_orchestration/upload_node_boot_information_to_boot_script_service_bss/</link>
      <pubDate>Mon, 17 Jul 2023 03:16:48 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/boot_orchestration/upload_node_boot_information_to_boot_script_service_bss/</guid>
      <description>Upload Node Boot Information to Boot Script Service (BSS) The following information must be uploaded to BSS as a prerequisite to booting a node via iPXE:
 The location of an initrd image in the artifact repository The location of a kernel image in the artifact repository Kernel boot parameters The node(s) associated with that information, using either host name or NID  BSS manages the iPXE boot scripts that coordinate the boot process for nodes, and it enables basic association of boot scripts with nodes.</description>
    </item>
    
    <item>
      <title>View The Status Of A BOS Session</title>
      <link>/docs-csm/en-13/operations/boot_orchestration/view_the_status_of_a_bos_session/</link>
      <pubDate>Mon, 17 Jul 2023 03:16:48 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/boot_orchestration/view_the_status_of_a_bos_session/</guid>
      <description>View the Status of a BOS Session The Boot Orchestration Service (BOS) supports a status endpoint that reports detailed status information for individual BOS sessions.
 BOS v2 session status  View the status of a v2 session Session status details   BOS v1 session status  Metadata View the status of a v1 session View the status of a boot set View the status for an individual phase View the status for an individual category    BOS v2 session status BOS v2 session status offers an overall status, as well as information about the percentage of components in each state, and any errors being experienced.</description>
    </item>
    
    <item>
      <title>Tools For Resolving Compute Node Boot Issues</title>
      <link>/docs-csm/en-13/operations/boot_orchestration/tools_for_resolving_boot_issues/</link>
      <pubDate>Mon, 17 Jul 2023 03:16:47 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/boot_orchestration/tools_for_resolving_boot_issues/</guid>
      <description>Tools for Resolving Compute Node Boot Issues A number of tools can be used to analyze and debug issues encountered during the compute node boot process. The underlying issue and symptoms dictate the type of tool required.
nmap Use nmap to send out DHCP discover requests to test DHCP. nmap can be installed using the following command:
zypper install nmap To reach the DHCP server, the request generally needs to be sent over the Node Management network (NMN) from a non-compute node (NCN).</description>
    </item>
    
    <item>
      <title>Troubleshoot Booting Nodes With Hardware Issues</title>
      <link>/docs-csm/en-13/operations/boot_orchestration/troubleshoot_booting_nodes_with_hardware_issues/</link>
      <pubDate>Mon, 17 Jul 2023 03:16:47 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/boot_orchestration/troubleshoot_booting_nodes_with_hardware_issues/</guid>
      <description>Troubleshoot Booting Nodes with Hardware Issues  NOTE This section is for Boot Orchestration Service (BOS) v1 only. Bad components will not impact the booting of other components in BOS v2.
 This document explains how to identify a node with hardware issues.
If a node included in a BOS session template is having hardware issues, then it can prevent the node from powering back up correctly. The entire BOS session will fail with a timeout error waiting for the node to become ready.</description>
    </item>
    
    <item>
      <title>Troubleshoot Compute Node Boot Issues Related To Dynamic Host Configuration Protocol (DHCP)</title>
      <link>/docs-csm/en-13/operations/boot_orchestration/troubleshoot_compute_node_boot_issues_related_to_dynamic_host_configuration_protocol_dhcp/</link>
      <pubDate>Mon, 17 Jul 2023 03:16:47 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/boot_orchestration/troubleshoot_compute_node_boot_issues_related_to_dynamic_host_configuration_protocol_dhcp/</guid>
      <description>Troubleshoot Compute Node Boot Issues Related to Dynamic Host Configuration Protocol (DHCP) DHCP issues can result in node boot failures. This procedure helps investigate and resolve such issues.
Prerequisites  This procedure requires administrative privileges. kubectl is installed.  Limitations Encryption of compute node logs is not enabled, so the passwords may be passed in clear text.
Procedure   Log in to a non-compute node (NCN) as root.
  Check that the DHCP service is running.</description>
    </item>
    
    <item>
      <title>Troubleshoot Compute Node Boot Issues Related To Slow Boot Times</title>
      <link>/docs-csm/en-13/operations/boot_orchestration/troubleshoot_compute_node_boot_issues_related_to_slow_boot_times/</link>
      <pubDate>Mon, 17 Jul 2023 03:16:47 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/boot_orchestration/troubleshoot_compute_node_boot_issues_related_to_slow_boot_times/</guid>
      <description>Troubleshoot Compute Node Boot Issues Related to Slow Boot Times  NOTE This section is for Boot Orchestration Service (BOS) v1 only. BOS v2 does not use Cray Advanced Platform Monitoring and Control (CAPMC), nor does it have the Boot Orchestration Agent (BOA). However, the steps outlined below allow for similar debugging of slow steps within Configuration Framework Service (CFS)-initiated sessions.
 Inspect BOS, BOA, and CFS job logs in order to obtain information that is critical for boot troubleshooting.</description>
    </item>
    
    <item>
      <title>Troubleshoot Compute Node Boot Issues Related To Trivial File Transfer Protocol (tftp)</title>
      <link>/docs-csm/en-13/operations/boot_orchestration/troubleshoot_compute_node_boot_issues_related_to_trivial_file_transfer_protocol_tftp/</link>
      <pubDate>Mon, 17 Jul 2023 03:16:47 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/boot_orchestration/troubleshoot_compute_node_boot_issues_related_to_trivial_file_transfer_protocol_tftp/</guid>
      <description>Troubleshoot Compute Node Boot Issues Related to Trivial File Transfer Protocol (TFTP) TFTP issues can result in node boot failures. Use this procedure to investigate and resolve such issues.
Prerequisites This procedure requires administrative privileges.
Limitations Encryption of compute node logs is not enabled, so the passwords may be passed in clear text.
  Log onto a non-compute node (NCN) as root.
  Check that the TFTP service is running.</description>
    </item>
    
    <item>
      <title>BOS Session Templates</title>
      <link>/docs-csm/en-13/operations/boot_orchestration/session_templates/</link>
      <pubDate>Mon, 17 Jul 2023 03:16:46 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/boot_orchestration/session_templates/</guid>
      <description>BOS Session Templates Session templates in the Boot Orchestration Service (BOS) are a reusable collection of boot, configuration, and component information. After creation they can be combined with a boot operation to create a BOS session that will apply the desired changes to the specified components. Session templates can be created via the API by providing JSON data or via the CLI by writing the JSON data to a file, which can then be referenced using the --file parameter.</description>
    </item>
    
    <item>
      <title>BOS Sessions</title>
      <link>/docs-csm/en-13/operations/boot_orchestration/sessions/</link>
      <pubDate>Mon, 17 Jul 2023 03:16:46 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/boot_orchestration/sessions/</guid>
      <description>BOS Sessions The Boot Orchestration Service (BOS) creates a session when it is asked to perform an operation on a session template. Sessions provide a way to track the status of many nodes at once as they perform the same operation with the same session template information. When creating a session, both the operation and session template are required parameters.
 BOS sessions in v2  Sessions and status   BOS sessions in v1  BOA functionality BOS v1 session limitations    BOS sessions in v2 The v2 version of BOS supports these operations:</description>
    </item>
    
    <item>
      <title>Redeploy The IPXE And Tftp Services</title>
      <link>/docs-csm/en-13/operations/boot_orchestration/redeploy_the_ipxe_and_tftp_services/</link>
      <pubDate>Mon, 17 Jul 2023 03:16:46 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/boot_orchestration/redeploy_the_ipxe_and_tftp_services/</guid>
      <description>Redeploy the iPXE and TFTP Services Redeploy the iPXE and TFTP services if a pod with a ceph-fs Process Virtualization Service (PVS) on a Kubernetes worker node is causing a HEALTH_WARN error.
Resolve issues with ceph-fs and ceph-mds by restarting the iPXE and TFTP services. The Ceph cluster will return to a healthy state after this procedure.
Prerequisites This procedure requires administrative privileges.
Procedure   Find the iPXE and TFTP deployments.</description>
    </item>
    
    <item>
      <title>Rolling Upgrades Using BOS</title>
      <link>/docs-csm/en-13/operations/boot_orchestration/rolling_upgrades/</link>
      <pubDate>Mon, 17 Jul 2023 03:16:46 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/boot_orchestration/rolling_upgrades/</guid>
      <description>Rolling Upgrades using BOS  NOTE This section is for Boot Orchestration Service (BOS) v2 only.
  NOTE This feature is the replacement for the Compute Rolling Upgrade Service (CRUS). CRUS was deprecated in CSM 1.2.0 and it will be removed in CSM 1.5.0. See Deprecated Features.
 BOS v2 allows users to stage boot artifacts, configuration, and an operation such as a reboot. The workload manager can later trigger the operation through BOS to apply that staged information, allowing rolling updates when nodes have no job running on them.</description>
    </item>
    
    <item>
      <title>Staging Changes With BOS</title>
      <link>/docs-csm/en-13/operations/boot_orchestration/stage_changes_with_bos/</link>
      <pubDate>Mon, 17 Jul 2023 03:16:46 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/boot_orchestration/stage_changes_with_bos/</guid>
      <description>Staging Changes with BOS In v2 of the Boot Orchestration Service (BOS), it is possible to stage changes when creating a session. These changes will not immediately take effect, and will instead be applied when the applystaged endpoint is called.
This is a BOS v2 feature only. For suggestions on working around this in v1, see Stage changes without BOS.
 Creating a staged session Applying a staged state Stage changes without BOS  Stage boot artifacts Stage a configuration    Creating a staged session (ncn-mw#) Creating a staged session is no different than creating a normal session, with one exception: the staged value should be set to True.</description>
    </item>
    
    <item>
      <title>BOS Options</title>
      <link>/docs-csm/en-13/operations/boot_orchestration/options/</link>
      <pubDate>Mon, 17 Jul 2023 03:16:45 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/boot_orchestration/options/</guid>
      <description>BOS Options  NOTE This section is for Boot Orchestration Service (BOS) v2 only.
 BOS provides a global service options endpoint for modifying the base configuration of the service itself. These options are available only for the BOS v2 API and only affect v2 functionality.
 Viewing the current option values Updating the option values BOS options details  Viewing the current option values View the options with the following command:</description>
    </item>
    
    <item>
      <title>Log File Locations And Ports Used In Compute Node Boot Troubleshooting</title>
      <link>/docs-csm/en-13/operations/boot_orchestration/log_file_locations_and_ports_used_in_compute_node_boot_troubleshooting/</link>
      <pubDate>Mon, 17 Jul 2023 03:16:45 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/boot_orchestration/log_file_locations_and_ports_used_in_compute_node_boot_troubleshooting/</guid>
      <description>Log File Locations and Ports Used in Compute Node Boot Troubleshooting This section includes the port IDs and log file locations of components associated with the node boot process.
Log File Locations The log file locations for ConMan, DHCP, and TFTP.
  ConMan logs are located within the conman pod at /var/log/conman.log.
  DHCP:
kubectl logs DHCP_POD_ID   TFTP:
kubectl logs -n services TFTP_POD_ID   Port IDs The following table includes the port IDs for DHCP and TFTP.</description>
    </item>
    
    <item>
      <title>Manage A BOS Session</title>
      <link>/docs-csm/en-13/operations/boot_orchestration/manage_a_bos_session/</link>
      <pubDate>Mon, 17 Jul 2023 03:16:45 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/boot_orchestration/manage_a_bos_session/</guid>
      <description>Manage a BOS Session Once a Boot Orchestration Service (BOS) session template is created, users can perform operations on nodes, such as boot, reboot, and shutdown.
To find the API versions of any commands listed, add -vvv to the end of the CLI command, and the CLI will print the underlying call to the API in the output.
 Create a new v2 session Create a new v1 session List all sessions Show details for a session Delete a session  Create a new v2 session Creating a new BOS v2 session requires the following command-line options:</description>
    </item>
    
    <item>
      <title>Manage A Session Template</title>
      <link>/docs-csm/en-13/operations/boot_orchestration/manage_a_session_template/</link>
      <pubDate>Mon, 17 Jul 2023 03:16:45 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/boot_orchestration/manage_a_session_template/</guid>
      <description>Manage a Session Template A session template must be created before starting a session with the Boot Orchestration Service (BOS).
This page shows Cray CLI commands for managing BOS session templates. To find the API versions of any commands listed, add -vvv to the end of the CLI command, and the CLI will print the underlying call to the API in the output.
 Session template framework Create a session template List all session templates View a session template Delete a session template  Session template framework When creating a new BOS session template, it can be helpful to start with a framework and then edit it as needed.</description>
    </item>
    
    <item>
      <title>Node Boot Root Cause Analysis</title>
      <link>/docs-csm/en-13/operations/boot_orchestration/node_boot_root_cause_analysis/</link>
      <pubDate>Mon, 17 Jul 2023 03:16:45 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/boot_orchestration/node_boot_root_cause_analysis/</guid>
      <description>Node Boot Root Cause Analysis The first step in debugging compute node boot-related issues is to determine the underlying cause, and the stage that the issue was encountered at.
The ConMan tool collects compute node logs. To learn more about ConMan, refer to ConMan.
A node&amp;rsquo;s console data can be accessed through its log file, as described in Access Compute Node Logs). This information can also be accessed by connecting to the node&amp;rsquo;s console with ipmitool.</description>
    </item>
    
    <item>
      <title>BOS Limitations For Gigabyte BMC Hardware</title>
      <link>/docs-csm/en-13/operations/boot_orchestration/limitations_for_gigabyte_bmc_hardware/</link>
      <pubDate>Mon, 17 Jul 2023 03:16:44 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/boot_orchestration/limitations_for_gigabyte_bmc_hardware/</guid>
      <description>BOS Limitations for Gigabyte BMC Hardware  NOTE This section is for Boot Orchestration Service (BOS) v1 only. BOS v2 does not use Cray Advanced Platform Monitoring and Control (CAPMC).
 Special steps need to be taken when using BOS to boot, reboot, or shutdown Gigabyte hardware. Gigabyte hardware treats power off and power on requests as successful, regardless of if actually successfully completed. The power on/off requests are ignored by CAPMC if they are received within a short period of time, which is typically around 60 seconds per operation.</description>
    </item>
    
    <item>
      <title>Export And Importing BOS Data</title>
      <link>/docs-csm/en-13/operations/boot_orchestration/exporting_and_importing_bos_data/</link>
      <pubDate>Mon, 17 Jul 2023 03:16:44 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/boot_orchestration/exporting_and_importing_bos_data/</guid>
      <description>Export and Importing BOS Data  Prerequisites Automated procedures  Export Import   Manual procedures  Export Import   BOS database PVCs  Prerequisites  Ensure that the cray command line interface (CLI) is authenticated and configured to talk to system management services.  See Configure the Cray CLI.   In order to use the automated procedures, the latest CSM documentation RPM must be installed on the node where the procedure is being performed.</description>
    </item>
    
    <item>
      <title>Healthy Compute Node Boot Process</title>
      <link>/docs-csm/en-13/operations/boot_orchestration/healthy_compute_node_boot_process/</link>
      <pubDate>Mon, 17 Jul 2023 03:16:44 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/boot_orchestration/healthy_compute_node_boot_process/</guid>
      <description>Healthy Compute Node Boot Process In order to investigate node boot-related issues, it is important to understand the flow of a healthy boot process and the associated components. This section outlines the normal flow of components that play a role in booting compute nodes, including DHCP, BSS, and TPTP.
DHCP A healthy DHCP exchange between server and client looks like the following:
   Traffic Description Sender     DHCP Discover A broadcast request from the client requesting an IP address.</description>
    </item>
    
    <item>
      <title>Kernel Boot Parameters</title>
      <link>/docs-csm/en-13/operations/boot_orchestration/kernel_boot_parameters/</link>
      <pubDate>Mon, 17 Jul 2023 03:16:44 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/boot_orchestration/kernel_boot_parameters/</guid>
      <description>Kernel Boot Parameters The Image Management Service (IMS) extracts kernel boot parameters from the /boot/kernel-parameters file in the image, if that file exists, and stores them in S3. IMS already stores the other boot artifacts (kernel, initrd, and rootfs) in S3. When told to boot an image, the Boot Orchestration Service (BOS) will extract these parameters and deliver them to the Boot Script Service (BSS) so they can be used during the next boot of a node.</description>
    </item>
    
    <item>
      <title>Limit The Scope Of A BOS Session</title>
      <link>/docs-csm/en-13/operations/boot_orchestration/limit_the_scope_of_a_bos_session/</link>
      <pubDate>Mon, 17 Jul 2023 03:16:44 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/boot_orchestration/limit_the_scope_of_a_bos_session/</guid>
      <description>Limit the Scope of a BOS Session The Boot Orchestration Service (BOS) supports an optional &amp;ndash;limit parameter when creating a session. This parameter can be used to further limit the nodes that BOS runs against, and is applied to all boot sets.
The --limit parameter takes a comma-separated list of nodes, groups, or roles in any combination. The BOS session will be limited to run against components that match both the boot set information and one or more of the nodes, groups, or roles listed in the limit.</description>
    </item>
    
    <item>
      <title>Compute Node Boot Sequence</title>
      <link>/docs-csm/en-13/operations/boot_orchestration/compute_node_boot_sequence/</link>
      <pubDate>Mon, 17 Jul 2023 03:16:43 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/boot_orchestration/compute_node_boot_sequence/</guid>
      <description>Compute Node Boot Sequence Provides an overview of the compute node boot process and touches upon the fact that issues can be encountered during this process.
The following is a high-level overview of the boot sequence for compute nodes:
  The compute node is powered on.
  The BIOS issues a DHCP discover request.
  DHCP responds with the following:
 next-server, which is the IP address of the TFTP server.</description>
    </item>
    
    <item>
      <title>Configure The BOS Timeout When Booting Compute Nodes</title>
      <link>/docs-csm/en-13/operations/boot_orchestration/configure_the_bos_timeout_when_booting_nodes/</link>
      <pubDate>Mon, 17 Jul 2023 03:16:43 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/boot_orchestration/configure_the_bos_timeout_when_booting_nodes/</guid>
      <description>Configure the BOS Timeout When Booting Compute Nodes  NOTE This section is for Boot Orchestration Service (BOS) v1 only. For similar functionality in BOS v2 see the BOS v2 Options.
 Manually update the boa-job-template ConfigMap to tune the timeout and sleep intervals for the Boot Orchestration Agent (BOA). Correcting the timeout value is a good troubleshooting option for when BOS sessions hang waiting for nodes to be in a Ready state.</description>
    </item>
    
    <item>
      <title>Create A Session Template To Boot Compute Nodes With Cps</title>
      <link>/docs-csm/en-13/operations/boot_orchestration/create_a_session_template_to_boot_compute_nodes_with_cps/</link>
      <pubDate>Mon, 17 Jul 2023 03:16:43 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/boot_orchestration/create_a_session_template_to_boot_compute_nodes_with_cps/</guid>
      <description>Create a Session Template to Boot Compute Nodes with CPS When compute nodes are booted, the Content Projection Service (CPS) and Data Virtualization Service (DVS) project the root file system (rootfs) over the network to the compute nodes by default.
Another option when compute nodes are booted is to download their rootfs into RAM.
Procedure   Use the CLI to create a session template.
Refer to Manage a Session Template for more information about creating a session template.</description>
    </item>
    
    <item>
      <title>Customize IPXE Binary Names</title>
      <link>/docs-csm/en-13/operations/boot_orchestration/customize_ipxe_binary_names/</link>
      <pubDate>Mon, 17 Jul 2023 03:16:43 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/boot_orchestration/customize_ipxe_binary_names/</guid>
      <description>Customize iPXE Binary Names The default behavior for the cray-ipxe service is to build iPXE binaries with a well known name. However, to help prevent untrusted access to the iPXE binaries, sites may manually customize the iPXE binary names to a site-specific value. The site may further change the iPXE binary names periodically to further obfuscate and prevent access.
Prerequisites This procedure requires administrative privileges.
Procedure   Edit the cray-ipxe-settings ConfigMap using one of the following options.</description>
    </item>
    
    <item>
      <title>Edit The IPXE Embedded Boot Script</title>
      <link>/docs-csm/en-13/operations/boot_orchestration/edit_the_ipxe_embedded_boot_script/</link>
      <pubDate>Mon, 17 Jul 2023 03:16:43 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/boot_orchestration/edit_the_ipxe_embedded_boot_script/</guid>
      <description>Edit the iPXE Embedded Boot Script Manually adjust the iPXE embedded boot script to change the order of network interfaces for DHCP request. Changing the order of network interfaces for DHCP requests helps improve boot time performance.
Prerequisites This procedure requires administrative privileges.
Procedure   Edit the ConfigMap using one of the following options.
 NOTE Save a backup of the ConfigMap before making any changes.
 The following is an example of creating a backup:</description>
    </item>
    
    <item>
      <title>BOS Components</title>
      <link>/docs-csm/en-13/operations/boot_orchestration/components/</link>
      <pubDate>Mon, 17 Jul 2023 03:16:42 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/boot_orchestration/components/</guid>
      <description>BOS Components  NOTE This section is for Boot Orchestration Service (BOS) v2 only.
 BOS v2 provides a components endpoint where BOS tracks the status of individual components. This includes information on the desired state and some information on the current state, status, and any session the component is part of.
Component records are created automatically and will include any components found in the Hardware State Manager (HSM).
 BOS component fields  actual_state desired_state staged_state enabled error event_stats last_action session status   Managing BOS components  List all components Show details for a component Update a component    BOS component fields actual_state Stores information on what BOS believes is the current boot artifacts the component is booted with.</description>
    </item>
    
    <item>
      <title>Clean Up Logs After A Boa Kubernetes Job</title>
      <link>/docs-csm/en-13/operations/boot_orchestration/clean_up_logs_after_a_boa_kubernetes_job/</link>
      <pubDate>Mon, 17 Jul 2023 03:16:42 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/boot_orchestration/clean_up_logs_after_a_boa_kubernetes_job/</guid>
      <description>Clean Up Logs After a BOA Kubernetes Job  NOTE This section is for Boot Orchestration Service (BOS) v1 only. BOS v2 does not use Boot Orchestration Agent (BOA) jobs and does not require cleanup.
 Delete log entries from previous boot orchestration jobs. BOS launches a BOA Kubernetes job. BOA then launches a Configuration Framework Service (CFS) session, resulting in a CFS-BOA Kubernetes job. Thus, there are two separate sets of jobs that can be removed.</description>
    </item>
    
    <item>
      <title>Component Status</title>
      <link>/docs-csm/en-13/operations/boot_orchestration/component_status/</link>
      <pubDate>Mon, 17 Jul 2023 03:16:42 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/boot_orchestration/component_status/</guid>
      <description>Component Status  NOTE This section is for Boot Orchestration Service (BOS) v2 only.
 For each component, BOS tracks information on what actions are expected to be happening (the phase) and what actions have recently happened (the last_action). This information is then aggregated into an overall status.
 Phases Last actions Status Status transitions  Phases A component&amp;rsquo;s phase is the high level part of the boot process that the component is currently on.</description>
    </item>
    
    <item>
      <title>Compute Node Boot Issue Symptom Duplicate Address Warnings And Declined DHCP Offers In Logs</title>
      <link>/docs-csm/en-13/operations/boot_orchestration/compute_node_boot_issue_symptom_duplicate_address_warnings_and_declined_dhcp_offers_in_logs/</link>
      <pubDate>Mon, 17 Jul 2023 03:16:42 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/boot_orchestration/compute_node_boot_issue_symptom_duplicate_address_warnings_and_declined_dhcp_offers_in_logs/</guid>
      <description>Compute Node Boot Issue Symptom: Duplicate Address Warnings and Declined DHCP Offers in Logs If the DHCP and node logs show duplicate address warnings and indicate declined DHCP offers, it may be because another component owns the IP address that DHCP is trying to assign to a node. If this happens, the node will not accept the IP address and will repeatedly submit a DHCP discover request. As a result, the node and DHCP become entangled in a loop of requesting and rejecting.</description>
    </item>
    
    <item>
      <title>Compute Node Boot Issue Symptom Message About Invalid Eeprom Checksum In Node Console Or Log</title>
      <link>/docs-csm/en-13/operations/boot_orchestration/compute_node_boot_issue_symptom_message_about_invalid_eeprom_checksum_in_node_console_or_log/</link>
      <pubDate>Mon, 17 Jul 2023 03:16:42 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/boot_orchestration/compute_node_boot_issue_symptom_message_about_invalid_eeprom_checksum_in_node_console_or_log/</guid>
      <description>Compute Node Boot Issue Symptom: Message About Invalid EEPROM Checksum in Node Console or Log On rare occasions, the processor hardware may lose the Serial Over Lan (SOL) connections and may need to be reseated to allow the node to successfully boot.
Symptoms This issue can be identified if the following is displayed in the node&amp;rsquo;s console or log:
console.38:2018-09-08 04:54:51 [ 16.721165] ixgbe 0000:18:00.0: The EEPROM Checksum Is Not Valid console.</description>
    </item>
    
    <item>
      <title>Compute Node Boot Issue Symptom Node Is Not Able To Download The Required Artifacts</title>
      <link>/docs-csm/en-13/operations/boot_orchestration/compute_node_boot_issue_symptom_node_is_not_able_to_download_the_required_artifacts/</link>
      <pubDate>Mon, 17 Jul 2023 03:16:42 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/boot_orchestration/compute_node_boot_issue_symptom_node_is_not_able_to_download_the_required_artifacts/</guid>
      <description>Compute Node Boot Issue Symptom: Node is Not Able to Download the Required Artifacts If either or both of the kernel or the initrd boot artifacts are missing from the artifact repository, Boot Script Service (BSS), or both, the node will not be able to download the required boot artifacts and will fail to boot.
Symptoms The node&amp;rsquo;s console or log will display lines beginning with, &amp;lsquo;&#39;Could not start download&#39;&amp;rsquo;. Refer to the image below for an example of this error message.</description>
    </item>
    
    <item>
      <title>Boot Orchestration</title>
      <link>/docs-csm/en-13/operations/boot_orchestration/boot_orchestration/</link>
      <pubDate>Mon, 17 Jul 2023 03:16:41 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/boot_orchestration/boot_orchestration/</guid>
      <description>Boot Orchestration The Boot Orchestration Service (BOS) is responsible for booting, configuring, and shutting down collections of nodes.
There are currently two supported API versions for BOS. BOS v1 is strictly session based and launches a Boot Orchestration Agent (BOA) that fulfills boot requests. BOS v2 takes a more flexible approach and relies on a number of permanent operators to guide components through state transitions in an independent manner. For more information, see BOS Workflows.</description>
    </item>
    
    <item>
      <title>Boot UANs</title>
      <link>/docs-csm/en-13/operations/boot_orchestration/boot_uans/</link>
      <pubDate>Mon, 17 Jul 2023 03:16:41 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/boot_orchestration/boot_uans/</guid>
      <description>Boot UANs Boot UANs with an image so that they are ready for user logins.
Prerequisites UAN boot images and a BOS session template have been created. See Create UAN Boot Images.
Procedure   (ncn-mw#) Create a BOS session to boot the UAN nodes.
cray bos v2 sessions create --template-name uan-sessiontemplate-PRODUCT_VERSION \  --operation reboot --format json | tee session.json Example output:
{ &amp;#34;components&amp;#34;: &amp;#34;&amp;#34;, &amp;#34;limit&amp;#34;: &amp;#34;&amp;#34;, &amp;#34;name&amp;#34;: &amp;#34;9fea7f3f-0a77-40b9-892d-37712de51d65&amp;#34;, &amp;#34;operation&amp;#34;: &amp;#34;boot&amp;#34;, &amp;#34;stage&amp;#34;: false, &amp;#34;status&amp;#34;: { &amp;#34;end_time&amp;#34;: null, &amp;#34;error&amp;#34;: null, &amp;#34;start_time&amp;#34;: &amp;#34;2022-08-22T14:44:27&amp;#34;, &amp;#34;status&amp;#34;: &amp;#34;pending&amp;#34; }, &amp;#34;template_name&amp;#34;: &amp;#34;cle-1.</description>
    </item>
    
    <item>
      <title>BOS Commands Cheat Sheet</title>
      <link>/docs-csm/en-13/operations/boot_orchestration/cheatsheet/</link>
      <pubDate>Mon, 17 Jul 2023 03:16:41 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/boot_orchestration/cheatsheet/</guid>
      <description>BOS Commands Cheat Sheet This page is a quick reference for common BOS CLI commands.
To find the API versions of any commands listed, add -vvv to the end of the CLI command, and the CLI will print the underlying call to the API in the output.
BOS v2 commands Full system commands (v2) (ncn-mw#) Boot all nodes in a template:
cray bos v2 sessions create --template-name SESSION_TEMPLATE_NAME --operation Boot (ncn-mw#) Reboot all nodes in a template:</description>
    </item>
    
    <item>
      <title>Check The Progress Of BOS Session Operations</title>
      <link>/docs-csm/en-13/operations/boot_orchestration/check_the_progress_of_bos_session_operations/</link>
      <pubDate>Mon, 17 Jul 2023 03:16:41 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/boot_orchestration/check_the_progress_of_bos_session_operations/</guid>
      <description>Check the Progress of BOS Session Operations  NOTE This section is for BOS v1 only. For similar functionality in BOS v2, refer to View the Status of a BOS Session.
 This page describes how to view the logs of BOS operations with Kubernetes.
 Overview Find the BOA Kubernetes pod and job View the BOA log View the CFS log View the BOS log  Overview When a Boot Orchestration Service (BOS) session is created, it will return a job ID.</description>
    </item>
    
    <item>
      <title>Clean Up After A BOS/boa Job Is Completed Or CANcelled</title>
      <link>/docs-csm/en-13/operations/boot_orchestration/clean_up_after_a_bos-boa_job_is_completed_or_cancelled/</link>
      <pubDate>Mon, 17 Jul 2023 03:16:41 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/boot_orchestration/clean_up_after_a_bos-boa_job_is_completed_or_cancelled/</guid>
      <description>Clean Up After a BOS/BOA Job is Completed or Cancelled  NOTE This section is for Boot Orchestration Service (BOS) v1 only. BOS v2 does not use Boot Orchestration Agent (BOA) jobs and does not require cleanup.
 When a BOS session is created, there are a number of items created on the system. When a session is cancelled or completed, these items need to be cleaned up to ensure there is not lingering content from the session on the system.</description>
    </item>
    
    <item>
      <title>Boot Issue Symptom Node HSN Interface Does Not Appear Or Show Detected Links Detected</title>
      <link>/docs-csm/en-13/operations/boot_orchestration/boot_issue_symptom_node_hsn_interface_does_not_appear_or_shows_no_link_detected/</link>
      <pubDate>Mon, 17 Jul 2023 03:16:40 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/boot_orchestration/boot_issue_symptom_node_hsn_interface_does_not_appear_or_shows_no_link_detected/</guid>
      <description>Boot Issue Symptom: Node HSN Interface Does Not Appear or Show Detected Links Detected A node may fail to boot if the HSN interface is experiencing issues, or if it is not able to detect any links.
Symptom The node&amp;rsquo;s HSN interface does not appear in the output of the ip addr command or the output of the ethtool interface command shows no link detected.
Resolution Reseat the node&amp;rsquo;s PCIe card.</description>
    </item>
    
    <item>
      <title>Boot Orchestration</title>
      <link>/docs-csm/en-13/operations/boot_orchestration/bos_api_versions/</link>
      <pubDate>Mon, 17 Jul 2023 03:16:40 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/boot_orchestration/bos_api_versions/</guid>
      <description>Boot Orchestration The Boot Orchestration Service (BOS) currently supports two API versions, v1 and v2, that have different APIs and underlying mechanisms for performing operations on nodes. The following is a summary of the changes, and the upgrade path, for users wishing to compare the two.
BOS v2 improvements BOS v2 makes significant improvements to boot times, retries, and error handling, by allowing nodes to proceed through the boot process at their own pace.</description>
    </item>
    
    <item>
      <title>BOS Services</title>
      <link>/docs-csm/en-13/operations/boot_orchestration/bos_services/</link>
      <pubDate>Mon, 17 Jul 2023 03:16:40 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/boot_orchestration/bos_services/</guid>
      <description>BOS Services The Boot Orchestration Service (BOS) consists of many different micro-services and other components, including the API, operators, and the BOS state reporter.
 BOS API Boot Orchestration Agent (BOA) BOS operators  actual-state-cleanup configuration discovery power-off-forceful power-off-graceful power-on session-cleanup session-completion session-setup status   BOS state reporter  BOS API The API is the point of contact for the user and all other services, including BOS&#39; own services, that want to query or update BOS data.</description>
    </item>
    
    <item>
      <title>BOS Workflows</title>
      <link>/docs-csm/en-13/operations/boot_orchestration/bos_workflows/</link>
      <pubDate>Mon, 17 Jul 2023 03:16:40 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/boot_orchestration/bos_workflows/</guid>
      <description>BOS Workflows The following workflows present a high-level overview of common Boot Orchestration Service (BOS) operations. These workflows depict how services interact with each other when booting, configuring, or shutting down nodes. They also help provide a quicker and deeper understanding of how the system functions.
 Terminology BOS v2 workflows  Boot nodes Reboot nodes Power off nodes   BOS v1 workflows  Boot and configure nodes Reconfigure nodes Power off nodes    Terminology The following are mentioned in the workflows:</description>
    </item>
    
    <item>
      <title>Compute Node Boot Issue Symptom Node Console Or Logs Indicate That The Server Response Has Timed Out</title>
      <link>/docs-csm/en-13/operations/boot_orchestration/boot_issue_symptom_node_console_or_logs_indicate_that_the_server_response_has_timed_out/</link>
      <pubDate>Mon, 17 Jul 2023 03:16:40 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/boot_orchestration/boot_issue_symptom_node_console_or_logs_indicate_that_the_server_response_has_timed_out/</guid>
      <description>Compute Node Boot Issue Symptom: Node Console or Logs Indicate that the Server Response has Timed Out If the TFTP request is able to access the TFTP service pod but is unable to find its way back to the node, it may be because the kernel is not tracking established TFTP connections.
Symptoms The following image, which is tcpdump data from within the TFTP pod, shows what happens when the TFTP request cannot find a route back to the node that sent the request.</description>
    </item>
    
    <item>
      <title>Bare-metal Steps</title>
      <link>/docs-csm/en-13/operations/bare_metal/bare-metal/</link>
      <pubDate>Mon, 17 Jul 2023 03:16:39 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/bare_metal/bare-metal/</guid>
      <description>Bare-Metal Steps This section provides information on what needs to be done before an initial install of CSM.
Air-Cooled BMC Credentials It is necessary to set the default credentials of all air-cooled BMCs so that CSM Hardware Management can interact with Redfish.
This procedure is outlined in Change Air-Cooled BMC Credentials.
Liquid-Cooled BMC Credentials As with air-cooled BMCs, liquid-cooled BMCs also need their credentials changed.
This procedure is outlined in Change Liquid-Cooled BMC Credentials.</description>
    </item>
    
    <item>
      <title>Fresh Install Setting NodeBMC And Routerbmc Redfish Credentials</title>
      <link>/docs-csm/en-13/operations/bare_metal/change_river_bmc_credentials/</link>
      <pubDate>Mon, 17 Jul 2023 03:16:39 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/bare_metal/change_river_bmc_credentials/</guid>
      <description>Fresh Install: Setting NodeBMC and RouterBMC Redfish Credentials These steps are performed before the installation of Shasta System Management or HPCM management software stacks. The goal is to set the BMC Redfish credentials to values that the management software will be expecting so that all software systems work smoothly with the Redfish hardware.
Prerequisites Before doing these operations, the following is assumed:
 There is a workstation or laptop which can access all target BMCs.</description>
    </item>
    
    <item>
      <title>Generate Temporary S3 Credentials</title>
      <link>/docs-csm/en-13/operations/artifact_management/generate_temporary_s3_credentials/</link>
      <pubDate>Mon, 17 Jul 2023 03:16:39 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/artifact_management/generate_temporary_s3_credentials/</guid>
      <description>Generate Temporary S3 Credentials Cray provides a simple token service (STS) via the API gateway for administrators to generate temporary Simple Storage Service (S3) credentials for use with S3 buckets. Temporary S3 credentials are generated using either cURL or Python.
The generated S3 credentials will expire after one hour.
 Retrieve temporary S3 credentials with cURL Retrieve temporary S3 credentials with Python  Retrieve temporary S3 credentials with cURL   Obtain a JWT token.</description>
    </item>
    
    <item>
      <title>Manage Artifacts With The Cray Cli</title>
      <link>/docs-csm/en-13/operations/artifact_management/manage_artifacts_with_the_cray_cli/</link>
      <pubDate>Mon, 17 Jul 2023 03:16:39 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/artifact_management/manage_artifacts_with_the_cray_cli/</guid>
      <description>Manage Artifacts with the Cray CLI The artifacts (objects) available for use on the system are created and managed with the Cray CLI. The cray artifacts command provides the ability to manage any given artifact. The Cray CLI automatically authenticates users and provides Simple Storage Service (S3) credentials.
All operations with the cray artifacts command assume that the user has already been authenticated. If the user has not been authenticated with the Cray CLI, run the following command:</description>
    </item>
    
    <item>
      <title>Use S3 Libraries And Clients</title>
      <link>/docs-csm/en-13/operations/artifact_management/use_s3_libraries_and_clients/</link>
      <pubDate>Mon, 17 Jul 2023 03:16:39 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/artifact_management/use_s3_libraries_and_clients/</guid>
      <description>Use S3 Libraries and Clients Several command line clients and language-specific libraries are available in addition to the Simple Storage Service (S3) RESTful API. Developers and system administrators can interact with artifacts in the S3 object store with these tools.
To learn more, refer to the following links:
 https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/s3.html - S3 Python client https://docs.aws.amazon.com/sdk-for-go/api/service/s3/ - Go client https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-welcome.html - Amazon Web Services (AWS) S3 CLI  </description>
    </item>
    
    <item>
      <title>Artifact Management</title>
      <link>/docs-csm/en-13/operations/artifact_management/artifact_management/</link>
      <pubDate>Mon, 17 Jul 2023 03:16:38 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/artifact_management/artifact_management/</guid>
      <description>Artifact Management The Ceph Object Gateway Simple Storage Service (S3) API is used for artifact management. The RESTful API that Ceph provides via the gateway is compatible with the basic data access model of the Amazon S3 API. See the https://docs.ceph.com/en/pacific/radosgw/s3/ for more information about compatibility. The object gateway is also referred to as the RADOS gateway or simply RGW.
S3 is an object storage service that provides high-level performance, scalability, security, and data availability.</description>
    </item>
    
    <item>
      <title>Using Argo Workflows</title>
      <link>/docs-csm/en-13/operations/argo/using_argo_workflows/</link>
      <pubDate>Mon, 17 Jul 2023 03:16:38 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/argo/using_argo_workflows/</guid>
      <description>Using Argo Workflows This page provides information on operating Argo workflows in CSM and describes how Argo workflows behave once starting.
 Basic behavior Starting a new workflow after a failed workflow  Basic behavior   Once a workflow is started, it will proceed through multiple steps in a set order. Most steps depend on previous steps and will wait for its dependencies to finish before starting.
  If any step fails, by default, that step will be continuously retried until it succeeds.</description>
    </item>
    
    <item>
      <title>Using The Argo Ui</title>
      <link>/docs-csm/en-13/operations/argo/using_the_argo_ui/</link>
      <pubDate>Mon, 17 Jul 2023 03:16:38 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/argo/using_the_argo_ui/</guid>
      <description>Using the Argo UI This page provides information about using the Argo UI with CSM. The Argo UI is useful for watching the progress of an install or upgrade and debugging. The UI is read-only and will not accept write operations.
 Access the Argo UI View logs  Access the Argo UI The Argo UI is accessed through a URL. The URL for a system can be found by the following command.</description>
    </item>
    
    <item>
      <title>View A UAI Class</title>
      <link>/docs-csm/en-13/operations/uas_user_and_admin_topics/view_a_uai_class/</link>
      <pubDate>Mon, 17 Jul 2023 03:16:38 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/uas_user_and_admin_topics/view_a_uai_class/</guid>
      <description>View a UAI Class Display all the information for a specific UAI class by referencing its class ID.
Prerequisites  The administrator must be logged into an NCN or a host that has administrative access to the HPE Cray EX System API Gateway The administrator must have the HPE Cray EX System CLI (cray command) installed on the above host The HPE Cray EX System CLI must be configured (initialized - cray init command) to reach the HPE Cray EX System API Gateway The administrator must be logged in as an administrator to the HPE Cray EX System CLI (cray auth login command) The administrator must know the Class ID of a UAI class: List Available UAI Classes  Procedure View all the information about a specific UAI class.</description>
    </item>
    
    <item>
      <title>Volumes</title>
      <link>/docs-csm/en-13/operations/uas_user_and_admin_topics/volumes/</link>
      <pubDate>Mon, 17 Jul 2023 03:16:38 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/uas_user_and_admin_topics/volumes/</guid>
      <description>Volumes Volumes provide a way to connect UAIs to external data, whether they be Kubernetes managed objects, external file systems or files, host node files and directories, or remote networked data to be used within the UAI.
The following are examples of how volumes are commonly used by UAIs:
 To connect UAIs to configuration files like /etc/localtime maintained by the host node To connect End-User UAIs to Slurm or PBS Professional Workload Manager configuration shared through Kubernetes To connect End-User UAIs to Programming Environment libraries and tools hosted on the UAI host nodes To connect End-User UAIs to Lustre or other external storage for user data To connect Broker UAIs to a directory service (see Configure a Broker UAI Class) or SSH configuration (see Customize the Broker UAI Image) needed to authenticate and redirect user sessions  Any kind of volume recognized by the Kubernetes installation can be installed as a volume within UAS.</description>
    </item>
    
    <item>
      <title>UAS And UAI Legacy Mode Health Checks</title>
      <link>/docs-csm/en-13/operations/uas_user_and_admin_topics/uas_and_uai_health_checks/</link>
      <pubDate>Mon, 17 Jul 2023 03:16:37 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/uas_user_and_admin_topics/uas_and_uai_health_checks/</guid>
      <description>UAS and UAI Legacy Mode Health Checks Check the health of UAS and UAI to validate installation / upgrade of an HPE Cray EX system. This is a legacy mode procedure that can be run at installation / upgrade time to make sure that the following are true:
 UAS is installed and running correctly UAI images are installed and registered correctly UAIs can be created in legacy mode  Initialize and Authorize the CLI The procedures below use the CLI as an authorized user and run on two separate node types.</description>
    </item>
    
    <item>
      <title>UAS Limitations</title>
      <link>/docs-csm/en-13/operations/uas_user_and_admin_topics/uas_limitations/</link>
      <pubDate>Mon, 17 Jul 2023 03:16:37 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/uas_user_and_admin_topics/uas_limitations/</guid>
      <description>UAS Limitations Functionality that is currently not supported while using UAS.
Functionality Not Currently Supported by the User Access Service  Lustre (lfs) commands within the UAS service pod Executing Singularity containers within the UAS service Building Docker containers within the UAS environment Building containerd containers within the UAS environment dmesg cannot run inside a UAI because of container security limitations Users cannot SSH from ncn-w001 to a UAI because UAIs use LoadBalancer IP addresses on the Customer Access Network (CAN) instead of NodePorts and the LoadBalancer IP addresses are not accessible from ncn-w001  Other Limitations  There is a known issue where X11 traffic may not forward DISPLAY correctly if the user logs into an NCN node before logging into a UAI The cray uas uais commands are not restricted to operating on UAIs owned by the user authenticated with cray auth login  Limitations Related To Restarts Changes made to a running UAI will be lost if the UAI is restarted or deleted.</description>
    </item>
    
    <item>
      <title>Update A Resource Specification</title>
      <link>/docs-csm/en-13/operations/uas_user_and_admin_topics/update_a_resource_specification/</link>
      <pubDate>Mon, 17 Jul 2023 03:16:37 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/uas_user_and_admin_topics/update_a_resource_specification/</guid>
      <description>Update a Resource Specification Modify a specific UAI resource specification using the resource_id of that specification.
Prerequisites  The administrator must be logged into an NCN or a host that has administrative access to the HPE Cray EX System API Gateway The administrator must have the HPE Cray EX System CLI (cray command) installed on the above host The HPE Cray EX System CLI must be configured (initialized - cray init command) to reach the HPE Cray EX System API Gateway The administrator must be logged in as an administrator to the HPE Cray EX System CLI (cray auth login command) The administrator must know the Resource ID of the resource specification to be updated: List UAI Resource Specifications  Procedure To modify a particular resource specification, use a command of the following form:</description>
    </item>
    
    <item>
      <title>Update A UAI Image Registration</title>
      <link>/docs-csm/en-13/operations/uas_user_and_admin_topics/update_a_uai_image_registration/</link>
      <pubDate>Mon, 17 Jul 2023 03:16:37 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/uas_user_and_admin_topics/update_a_uai_image_registration/</guid>
      <description>Update a UAI Image Registration Modify the UAS registration information of a UAI image.
Prerequisites  The administrator must be logged into an NCN or a host that has administrative access to the HPE Cray EX System API Gateway The administrator must have the HPE Cray EX System CLI (cray command) installed on the above host The HPE Cray EX System CLI must be configured (initialized - cray init command) to reach the HPE Cray EX System API Gateway The administrator must be logged in as an administrator to the HPE Cray EX System CLI (cray auth login command) The administrator must know the Image ID of the UAI Image Registration to be updated: List UAI Registered Images  Procedure Once a UAI image has been registered, it may be necessary to change its attributes.</description>
    </item>
    
    <item>
      <title>Update A UAS Volume</title>
      <link>/docs-csm/en-13/operations/uas_user_and_admin_topics/update_a_uas_volume/</link>
      <pubDate>Mon, 17 Jul 2023 03:16:37 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/uas_user_and_admin_topics/update_a_uas_volume/</guid>
      <description>Update a UAS Volume Modify the configuration of an already-registered UAS volume. Almost any part of the configuration of a UAS volume can be modified.
Prerequisites  The administrator must be logged into an NCN or a host that has administrative access to the HPE Cray EX System API Gateway The administrator must have the HPE Cray EX System CLI (cray command) installed on the above host The HPE Cray EX System CLI must be configured (initialized - cray init command) to reach the HPE Cray EX System API Gateway The administrator must be logged in as an administrator to the HPE Cray EX System CLI (cray auth login command) The administrator must know UAS volume ID of a volume; perform List Volumes Registered in UAS if needed The administrator should be familiar with Add a Volume to UAS; the options and caveats for updating volumes are the same as for creating volumes  Procedure Modify the configuration of a UAS volume.</description>
    </item>
    
    <item>
      <title>UAI Image Customization</title>
      <link>/docs-csm/en-13/operations/uas_user_and_admin_topics/uai_image_customization/</link>
      <pubDate>Mon, 17 Jul 2023 03:16:36 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/uas_user_and_admin_topics/uai_image_customization/</guid>
      <description>UAI Image Customization This section covers common customizations of both End-User UAIs and Broker UAIs.
Refer to the following topics for more information:
 Customize the Broker UAI Image Customize End-User UAI Images  Top: User Access Service (UAS)
Next Topic: Customize the Broker UAI Image</description>
    </item>
    
    <item>
      <title>UAI Images</title>
      <link>/docs-csm/en-13/operations/uas_user_and_admin_topics/uai_images/</link>
      <pubDate>Mon, 17 Jul 2023 03:16:36 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/uas_user_and_admin_topics/uai_images/</guid>
      <description>UAI Images There are three kinds of UAI images used by UAS:
 A pre-packaged Broker UAI image provided with the UAS A pre-packaged basic End-User UAI Image provided with the UAS Custom End-User UAI images created on site, usually based on compute node contents  UAS provides two stock UAI images when installed. The first is a standard End-User UAI Image that has the necessary software installed in it to support a basic Linux distribution login experience.</description>
    </item>
    
    <item>
      <title>UAI Macvlans Network Attachments</title>
      <link>/docs-csm/en-13/operations/uas_user_and_admin_topics/uai_macvlans_network_attachments/</link>
      <pubDate>Mon, 17 Jul 2023 03:16:36 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/uas_user_and_admin_topics/uai_macvlans_network_attachments/</guid>
      <description>UAI macvlans Network Attachments UAIs need to be able to reach compute nodes across the HPE Cray EX internal networks. When the compute node networks are structured as multiple subnets, this requires routing from the UAIs to those subnets. The default route in a UAI goes to the public network through the Customer Access Network (CAN) so that will not work for reaching compute nodes. To solve this problem, UAS installs Kubernetes Network Attachments within the Kubernetes user namespace.</description>
    </item>
    
    <item>
      <title>UAI Management</title>
      <link>/docs-csm/en-13/operations/uas_user_and_admin_topics/uai_management/</link>
      <pubDate>Mon, 17 Jul 2023 03:16:36 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/uas_user_and_admin_topics/uai_management/</guid>
      <description>UAI Management UAS supports two manual methods and one automated method of UAI management:
 Direct administrative UAI management Legacy mode user driven UAI management UAI broker mode UAI management  Direct administrative UAI management is available mostly to allow administrators to set up Broker UAIs for the Broker Mode UAI Management and to control UAIs that are created under one of the other two methods. It is unlikely that a site will choose to create End-User UAIs this way, but it is possible to do.</description>
    </item>
    
    <item>
      <title>UAI Network Attachment Customization</title>
      <link>/docs-csm/en-13/operations/uas_user_and_admin_topics/uai_network_attachments/</link>
      <pubDate>Mon, 17 Jul 2023 03:16:36 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/uas_user_and_admin_topics/uai_network_attachments/</guid>
      <description>UAI Network Attachment Customization The UAI network attachment configuration flows from the Cray Site Initializer (CSI) localization data through customizations.yaml into the UAS Helm chart and, ultimately, into Kubernetes in the form of a &amp;ldquo;network-attachment-definition&amp;rdquo;.
This section describes the data at each of those stages to show how the final network attachment gets created. Customization of the network attachments may be needed by some sites to, for example, increase the size of the reserved sub-net used for UAI macvlan attachments.</description>
    </item>
    
    <item>
      <title>Troubleshoot UAS By Viewing Log Output</title>
      <link>/docs-csm/en-13/operations/uas_user_and_admin_topics/troubleshoot_uas_by_viewing_log_output/</link>
      <pubDate>Mon, 17 Jul 2023 03:16:35 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/uas_user_and_admin_topics/troubleshoot_uas_by_viewing_log_output/</guid>
      <description>Troubleshoot UAS by Viewing Log Output At times there will be problems with UAS. Usually this takes the form of errors showing up on CLI commands that are not immediately interpretable as some sort of input error. It is sometimes useful to examine the UAS service logs to find out what is wrong.
Procedure   Find the names of the Kubernetes pods running UAS:
ncn-m001-kubectl get po -n services | grep uas | grep -v etcd Example output:</description>
    </item>
    
    <item>
      <title>Troubleshoot UAS Issues</title>
      <link>/docs-csm/en-13/operations/uas_user_and_admin_topics/troubleshoot_uas_issues/</link>
      <pubDate>Mon, 17 Jul 2023 03:16:35 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/uas_user_and_admin_topics/troubleshoot_uas_issues/</guid>
      <description>Troubleshoot UAS Issues This section provides examples of some commands that can be used to troubleshoot UAS-related issues.
Troubleshoot Connection Issues packet_write_wait: Connection to 203.0.113.0 port 30841: Broken pipe If an error message related to broken pipes returns, enable keep-alives on the client side. The admin should update the /etc/ssh/sshd_config and /etc/ssh/ssh_config files to add the following:
TCPKeepAlive yes ServerAliveInterval 120 ServerAliveCountMax 720 Invalid Credentials ncn-w001 # cray auth login --username USER --password WRONGPASSWORD Example output:</description>
    </item>
    
    <item>
      <title>UAI Classes</title>
      <link>/docs-csm/en-13/operations/uas_user_and_admin_topics/uai_classes/</link>
      <pubDate>Mon, 17 Jul 2023 03:16:35 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/uas_user_and_admin_topics/uai_classes/</guid>
      <description>UAI Classes UAI Classes provide templates for the creation of UAIs. They permit precise configuration of the behavior, volumes, resources, and other elements of the UAI. When a UAI is created using a UAI Class, it is configured to use exactly what that UAI Class has in it at the time the UAI was created. UIA Classes permit Broker UAIs to create different kinds of UAIs based on the UAI Creation Class setting of the Broker UAI.</description>
    </item>
    
    <item>
      <title>UAI Host Node Selection</title>
      <link>/docs-csm/en-13/operations/uas_user_and_admin_topics/uai_host_node_selection/</link>
      <pubDate>Mon, 17 Jul 2023 03:16:35 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/uas_user_and_admin_topics/uai_host_node_selection/</guid>
      <description>UAI Host Node Selection When selecting UAI host nodes, it is a good idea to take into account the amount of combined load users and system services will bring to those nodes. UAIs run by default at a lower priority than system services on worker nodes which means that, if the combined load exceeds the capacity of the nodes, Kubernetes will eject UAIs and/or refuse to schedule them to protect system services.</description>
    </item>
    
    <item>
      <title>UAI Host Nodes</title>
      <link>/docs-csm/en-13/operations/uas_user_and_admin_topics/uai_host_nodes/</link>
      <pubDate>Mon, 17 Jul 2023 03:16:35 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/uas_user_and_admin_topics/uai_host_nodes/</guid>
      <description>UAI Host Nodes UAIs run as Kubernetes pods on Kubernetes worker nodes. UAS provides a a mechanism using Kubernetes labels to prevent UAIs from running on a specific worker nodes, but any Kubernetes node that is not labeled to prevent UAIs from running on it is considered eligible to host UAIs. The administrator of a given site may control the set of UAI host nodes by labeling Kubernetes worker nodes appropriately.</description>
    </item>
    
    <item>
      <title>Troubleshoot Missing Or Incorrect UAI Images</title>
      <link>/docs-csm/en-13/operations/uas_user_and_admin_topics/troubleshoot_missing_or_incorrect_uai_images/</link>
      <pubDate>Mon, 17 Jul 2023 03:16:34 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/uas_user_and_admin_topics/troubleshoot_missing_or_incorrect_uai_images/</guid>
      <description>Troubleshoot Missing or Incorrect UAI Images If a UAI shows a uai_status of Waiting and a uai_msg of ImagePullBackOff, that indicates that the UAI or the UAI class is configured to use an image that is not in the image registry.
Either obtaining and pushing the image to the image registry, or correcting the name or version of the image in the UAS configuration will usually resolve this.
Top: User Access Service (UAS)</description>
    </item>
    
    <item>
      <title>Troubleshoot Stale Brokered UAIs</title>
      <link>/docs-csm/en-13/operations/uas_user_and_admin_topics/troubleshoot_stale_brokered_uais/</link>
      <pubDate>Mon, 17 Jul 2023 03:16:34 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/uas_user_and_admin_topics/troubleshoot_stale_brokered_uais/</guid>
      <description>Troubleshoot Stale Brokered UAIs When a Broker UAI terminates and restarts, the SSH key used to forward SSH sessions to End-User UAIs changes (this is a known problem) and subsequent Broker UAIs are unable to forward sessions to End-User UAIs. The symptom of this is that a user logging into a Broker UAI will receive a password prompt from the End-User UAI and be unable to log in even if providing the correct password.</description>
    </item>
    
    <item>
      <title>Troubleshoot UAI Stuck In Containercreating</title>
      <link>/docs-csm/en-13/operations/uas_user_and_admin_topics/troubleshoot_uai_stuck_in_containercreating/</link>
      <pubDate>Mon, 17 Jul 2023 03:16:34 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/uas_user_and_admin_topics/troubleshoot_uai_stuck_in_containercreating/</guid>
      <description>Troubleshoot UAI Stuck in ContainerCreating Resolve an issue causing UAIs to show a uai_status field of Waiting, and a uai_msg field of ContainerCreating. It is possible that this is just a matter of starting the UAI taking longer than normal, perhaps as it pulls in a new UAI image from a registry. If the issue persists for a long time, it is worth investigating.
Prerequisites  The administrator must be logged into an NCN or a host that has administrative access to the HPE Cray EX System API Gateway The administrator must have the HPE Cray EX System CLI (cray command) installed on the above host The HPE Cray EX System CLI must be configured (initialized - cray init command) to reach the HPE Cray EX System API Gateway The administrator must be logged in as an administrator to the HPE Cray EX System CLI (cray auth login command) The administrator must be on an NCN or host that has Kubernetes (kubectl command) access to the HPE Cray EX System  Symptoms The UAI has been in the ContainerCreating status for several minutes.</description>
    </item>
    
    <item>
      <title>Troubleshoot UAIs By Viewing Log Output</title>
      <link>/docs-csm/en-13/operations/uas_user_and_admin_topics/troubleshoot_uais_by_viewing_log_output/</link>
      <pubDate>Mon, 17 Jul 2023 03:16:34 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/uas_user_and_admin_topics/troubleshoot_uais_by_viewing_log_output/</guid>
      <description>Troubleshoot UAIs by Viewing Log Output Sometimes a UAI will come up and run but will not work correctly. It is possible to see errors reported by elements of the UAI entrypoint script using the kubectl logs command.
Procedure   Find the UAI of interest.
This starts by identifying the UAI name using the CLI:
ncn-m001-cray uas admin uais list Example output:
[[results]] uai_age = &amp;#34;4h30m&amp;#34; uai_connect_string = &amp;#34;ssh broker@10.</description>
    </item>
    
    <item>
      <title>Troubleshoot UAIs With Administrative Access</title>
      <link>/docs-csm/en-13/operations/uas_user_and_admin_topics/troubleshoot_uais_with_administrative_access/</link>
      <pubDate>Mon, 17 Jul 2023 03:16:34 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/uas_user_and_admin_topics/troubleshoot_uais_with_administrative_access/</guid>
      <description>Troubleshoot UAIs with Administrative Access Sometimes there is no better way to figure out a problem with a UAI than to get inside it and look around as an administrator. This is done using kubectl exec to start a shell inside the running container as root (in the container). With this an administrator can diagnose problems, make changes to the running UAI, and find solutions. It is important to remember that any change made inside a UAI is transitory.</description>
    </item>
    
    <item>
      <title>Troubleshoot UAS / Cli Authentication Issues</title>
      <link>/docs-csm/en-13/operations/uas_user_and_admin_topics/troubleshoot_uai_authentication_issues/</link>
      <pubDate>Mon, 17 Jul 2023 03:16:34 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/uas_user_and_admin_topics/troubleshoot_uai_authentication_issues/</guid>
      <description>Troubleshoot UAS / CLI Authentication Issues Several troubleshooting steps related to authentication in a UAI.
Internal Server Error An error was encountered while accessing Keycloak because of an invalid token.
cray uas create --publickey ~/.ssh/id_rsa.pub Usage: cray uas create [OPTIONS] Try &amp;#34;cray uas create --help&amp;#34; for help. Error: Internal Server Error: An error was encountered while accessing Keycloak The uas-mgr logs show:
2020-03-06 18:52:07,642 - uas_auth - ERROR - &amp;lt;class &amp;#39;requests.</description>
    </item>
    
    <item>
      <title>Broker UAI Resiliency And Load Balancing</title>
      <link>/docs-csm/en-13/operations/uas_user_and_admin_topics/setting_up_multi-replica_brokers/</link>
      <pubDate>Mon, 17 Jul 2023 03:16:33 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/uas_user_and_admin_topics/setting_up_multi-replica_brokers/</guid>
      <description>Broker UAI Resiliency and Load Balancing Broker UAI resiliency and load balancing is achieved through the use of Multi-Replica Broker UAIs. The procedures and data involved in configuring a UAI Class to create Multi-Replica Broker UAIs can be found in UAI Classes. This page describes some of the reasons to use Multi-Replica Broker UAIs and some of the implications of doing so.
When a Broker UAI runs with multiple replicas, access to the broker remains channeled through a single external IP address, but the connections are load balanced and dispatched to multiple Kubernetes pods where the Broker UAI functionality is running.</description>
    </item>
    
    <item>
      <title>Special Purpose UAIs</title>
      <link>/docs-csm/en-13/operations/uas_user_and_admin_topics/special_purpose_uais/</link>
      <pubDate>Mon, 17 Jul 2023 03:16:33 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/uas_user_and_admin_topics/special_purpose_uais/</guid>
      <description>Special Purpose UAIs Even though most UAIs are End-User UAIs, UAI classes make it possible to construct UAIs to serve special purposes that are not strictly end-user oriented.
One kind of special purpose UAI is the Broker UAI, which provides on demand End-User UAI launch and management. While no other specialty UAI types currently exist, other applications are expected to arise and sites are encouraged to innovate as needed.
Top: User Access Service (UAS)</description>
    </item>
    
    <item>
      <title>Start A Broker UAI</title>
      <link>/docs-csm/en-13/operations/uas_user_and_admin_topics/start_a_broker_uai/</link>
      <pubDate>Mon, 17 Jul 2023 03:16:33 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/uas_user_and_admin_topics/start_a_broker_uai/</guid>
      <description>Start a Broker UAI Create a Broker UAI after a Broker UAI class has been created.
Prerequisites  The administrator must be logged into an NCN or a host that has administrative access to the HPE Cray EX System API Gateway The administrator must have the HPE Cray EX System CLI (cray command) installed on the above host The HPE Cray EX System CLI must be configured (initialized - cray init command) to reach the HPE Cray EX System API Gateway The administrator must be logged in as an administrator to the HPE Cray EX System CLI (cray auth login command) There must be an appropriate Broker UAI Class defined: Configure a Broker UAI Class The administrator must know the Class ID of the desired Broker UAI Class: List UAI Classes  Optional: the administrator may choose a site defined name for the Broker UAI to be used in conjunction with the HPE Cray EX System External DNS mechanism.</description>
    </item>
    
    <item>
      <title>Troubleshoot Broker UAI Sssd CANnot Use /etc/sssd/sssd.conf</title>
      <link>/docs-csm/en-13/operations/uas_user_and_admin_topics/troubleshoot_broker_sssd_cant_use_sssd_conf/</link>
      <pubDate>Mon, 17 Jul 2023 03:16:33 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/uas_user_and_admin_topics/troubleshoot_broker_sssd_cant_use_sssd_conf/</guid>
      <description>Troubleshoot Broker UAI SSSD Cannot Use /etc/sssd/sssd.conf Symptom A Broker UAI has been created using an SSSD configuration in a secret and volume as described in Configure a Broker UAI Class, but logging into the Broker UAI does not work.
Diagnose the problem as follows:
  Find the UAI name of the Broker UAI in a list of existing UAIs:
cray uas admin uais list --format yaml   Find the Broker UAI pod name by looking for a pod with the UAI name as the first part of its name in the list of Broker UAI pods:</description>
    </item>
    
    <item>
      <title>Troubleshoot Common Mistakes When Creating A Custom End-user UAI Image</title>
      <link>/docs-csm/en-13/operations/uas_user_and_admin_topics/troubleshoot_common_mistakes_when_creating_a_custom_end-user_uai_image/</link>
      <pubDate>Mon, 17 Jul 2023 03:16:33 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/uas_user_and_admin_topics/troubleshoot_common_mistakes_when_creating_a_custom_end-user_uai_image/</guid>
      <description>Troubleshoot Common Mistakes when Creating a Custom End-User UAI Image There a several problems that may occur while making or working with a custom end-user UAI images. The following are some basic troubleshooting questions to ask:
 Does SESSION_NAME match an actual entry in cray bos v1 sessiontemplate list? Is the SESSION_ID set to an appropriate UUID format? Did the awk command not parse the UUID correctly? Did the file /etc/security/limits.</description>
    </item>
    
    <item>
      <title>Troubleshoot Duplicate Mount Paths In A UAI</title>
      <link>/docs-csm/en-13/operations/uas_user_and_admin_topics/troubleshoot_duplicate_mount_paths_in_a_uai/</link>
      <pubDate>Mon, 17 Jul 2023 03:16:33 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/uas_user_and_admin_topics/troubleshoot_duplicate_mount_paths_in_a_uai/</guid>
      <description>Troubleshoot Duplicate Mount Paths in a UAI If a user attempts to create a UAI in the legacy mode and cannot create the UAI at all, a good place to look is at volumes. Duplicate mount_path specifications in the list of volumes in a UAI will cause a failure that looks like this:
ncn-m001-cray uas create --publickey ~/.ssh/id_rsa.pub Example output:
Usage: cray uas create [OPTIONS] Try &amp;#39;cray uas create --help&amp;#39; for help.</description>
    </item>
    
    <item>
      <title>Resource Specifications</title>
      <link>/docs-csm/en-13/operations/uas_user_and_admin_topics/resource_specifications/</link>
      <pubDate>Mon, 17 Jul 2023 03:16:32 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/uas_user_and_admin_topics/resource_specifications/</guid>
      <description>Resource Specifications Kubernetes uses resource limits and resource requests, to manage the system resources available to pods. Because UAIs run as pods under Kubernetes, UAS takes advantage of Kubernetes to manage the system resources available to UAIs.
In the UAS configuration, resource specifications contain that configuration. A UAI that is assigned a resource specification will use that instead of the default resource limits or requests on the Kubernetes namespace containing the UAI.</description>
    </item>
    
    <item>
      <title>Retrieve Resource Specification Details</title>
      <link>/docs-csm/en-13/operations/uas_user_and_admin_topics/retrieve_resource_specification_details/</link>
      <pubDate>Mon, 17 Jul 2023 03:16:32 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/uas_user_and_admin_topics/retrieve_resource_specification_details/</guid>
      <description>Retrieve Resource Specification Details Display a specific resource specification using the resource_id of that specification.
Prerequisites  The administrator must be logged into an NCN or a host that has administrative access to the HPE Cray EX System API Gateway The administrator must have the HPE Cray EX System CLI (cray command) installed on the above host The HPE Cray EX System CLI must be configured (initialized - cray init command) to reach the HPE Cray EX System API Gateway The administrator must be logged in as an administrator to the HPE Cray EX System CLI (cray auth login command) The administrator must know the Resource ID of the resource specification to be retrieved: List Resource Specifications  Procedure Retrieve a resource specification.</description>
    </item>
    
    <item>
      <title>Retrieve UAI Image Registration Information</title>
      <link>/docs-csm/en-13/operations/uas_user_and_admin_topics/retrieve_uai_image_registration_information/</link>
      <pubDate>Mon, 17 Jul 2023 03:16:32 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/uas_user_and_admin_topics/retrieve_uai_image_registration_information/</guid>
      <description>Retrieve UAI Image Registration Information Use this procedure to obtain the default and imagename values for a registered UAI image. This procedure can also be used to confirm that a specific image ID is still registered with UAS.
This procedure returns the same information as List Registered UAI Images, but only for one image.
Prerequisites  The administrator must be logged into an NCN or a host that has administrative access to the HPE Cray EX System API Gateway The administrator must have the HPE Cray EX System CLI (cray command) installed on the above host The HPE Cray EX System CLI must be configured (initialized - cray init command) to reach the HPE Cray EX System API Gateway The administrator must be logged in as an administrator to the HPE Cray EX System CLI (cray auth login command) The administrator must know the Image ID of the UAI Image Registration to be retrieved: List UAI Images  Procedure   Obtain the image ID for a UAI that has been registered with UAS.</description>
    </item>
    
    <item>
      <title>Setting UAI Timeouts</title>
      <link>/docs-csm/en-13/operations/uas_user_and_admin_topics/setting_uai_timeouts/</link>
      <pubDate>Mon, 17 Jul 2023 03:16:32 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/uas_user_and_admin_topics/setting_uai_timeouts/</guid>
      <description>Setting UAI Timeouts The procedures and specific values used for setting UAI timeouts are explained in the UAI Classes section. Please refer to that section.
On systems where UAIs are used as part of normal user activities, the number of UAIs can grow large. Stale UAIs (i.e. UAIs that sit idle for long periods of time) can prevent creation of fresh UAIs for users who are actually active on the system.</description>
    </item>
    
    <item>
      <title>Clear UAS Configuration</title>
      <link>/docs-csm/en-13/operations/uas_user_and_admin_topics/reset_the_uas_configuration_to_original_installed_settings/</link>
      <pubDate>Mon, 17 Jul 2023 03:16:31 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/uas_user_and_admin_topics/reset_the_uas_configuration_to_original_installed_settings/</guid>
      <description>Clear UAS Configuration WARNING: The procedure described here will remove all UAS configuration including some configuration that is installed upon installation / upgrade of the HPE Cray EX system. If this procedure is used, the update-uas Helm chart must be removed and re-deployed to restore the full HPE provided configuration. This procedure should only be used in an extreme situation where the UAS configuration has become corrupted to the point where it can no longer be managed.</description>
    </item>
    
    <item>
      <title>Modify A UAI Class</title>
      <link>/docs-csm/en-13/operations/uas_user_and_admin_topics/modify_a_uai_class/</link>
      <pubDate>Mon, 17 Jul 2023 03:16:31 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/uas_user_and_admin_topics/modify_a_uai_class/</guid>
      <description>Modify a UAI Class Update a UAI class with a modified configuration.
Prerequisites  The administrator must be logged into an NCN or a host that has administrative access to the HPE Cray EX System API Gateway The administrator must have the HPE Cray EX System CLI (cray command) installed on the above host The HPE Cray EX System CLI must be configured (initialized - cray init command) to reach the HPE Cray EX System API Gateway The administrator must be logged in as an administrator to the HPE Cray EX System CLI (cray auth login command) The administrator must know the Class ID of the UAI Class to be modified: List UAI Classes  Limitations The ID of the UAI class cannot be modified.</description>
    </item>
    
    <item>
      <title>Obtain The Configuration Of A UAS Volume</title>
      <link>/docs-csm/en-13/operations/uas_user_and_admin_topics/obtain_configuration_of_a_uas_volume/</link>
      <pubDate>Mon, 17 Jul 2023 03:16:31 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/uas_user_and_admin_topics/obtain_configuration_of_a_uas_volume/</guid>
      <description>Obtain the Configuration of a UAS Volume View the configuration information of a specific UAS volume. This procedure requires the volume_ID of that volume.
Prerequisites  The administrator must be logged into an NCN or a host that has administrative access to the HPE Cray EX System API Gateway The administrator must have the HPE Cray EX System CLI (cray command) installed on the above host The HPE Cray EX System CLI must be configured (initialized - cray init command) to reach the HPE Cray EX System API Gateway The administrator must be logged in as an administrator to the HPE Cray EX System CLI (cray auth login command) The administrator must know the Volume ID of the volume to be retrieved: List Volumes Registered in UAS  Procedure View the configuration of a specific UAS volume.</description>
    </item>
    
    <item>
      <title>Register A UAI Image</title>
      <link>/docs-csm/en-13/operations/uas_user_and_admin_topics/register_a_uai_image/</link>
      <pubDate>Mon, 17 Jul 2023 03:16:31 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/uas_user_and_admin_topics/register_a_uai_image/</guid>
      <description>Register a UAI Image Register a UAI image with UAS. Registration tells UAS where to locate the image and whether to use the image as the default for UAIs.
Prerequisites  The administrator must be logged into an NCN or a host that has administrative access to the HPE Cray EX System API Gateway The administrator must have the HPE Cray EX System CLI (cray command) installed on the above host The HPE Cray EX System CLI must be configured (initialized - cray init command) to reach the HPE Cray EX System API Gateway The administrator must be logged in as an administrator to the HPE Cray EX System CLI (cray auth login command) The UAI image must be created and uploaded to the container registry: Customize End-User UAI Images  Procedure Register a UAI image with UAS.</description>
    </item>
    
    <item>
      <title>This Page Has Moved</title>
      <link>/docs-csm/en-13/operations/uas_user_and_admin_topics/login_to_a_users_uai_to_troubleshoot_issues/</link>
      <pubDate>Mon, 17 Jul 2023 03:16:31 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/uas_user_and_admin_topics/login_to_a_users_uai_to_troubleshoot_issues/</guid>
      <description>This Page Has Moved The information on this page has moved to Troubleshoot UAIs with Administrative Access. Refer to that page for these procedures.</description>
    </item>
    
    <item>
      <title>User Access Service (UAS)</title>
      <link>/docs-csm/en-13/operations/uas_user_and_admin_topics/readme/</link>
      <pubDate>Mon, 17 Jul 2023 03:16:31 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/uas_user_and_admin_topics/readme/</guid>
      <description>User Access Service (UAS) The User Access Service (UAS) is a service that manages User Access Instances (UAIs) which are containerized services under Kubernetes that provide application developers and users with a lightweight login environment in which to create and run user applications. UAIs run on non-compute nodes (NCN), specifically Kubernetes Worker nodes.
At a high level, there are two ways to configure UAS with respect to allowing users access to UAIs.</description>
    </item>
    
    <item>
      <title>List UAI Resource Specifications</title>
      <link>/docs-csm/en-13/operations/uas_user_and_admin_topics/list_uai_resource_specifications/</link>
      <pubDate>Mon, 17 Jul 2023 03:16:30 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/uas_user_and_admin_topics/list_uai_resource_specifications/</guid>
      <description>List UAI Resource Specifications Obtain a list of all the UAI resource specifications registered with UAS.
Prerequisites  The administrator must be logged into an NCN or a host that has administrative access to the HPE Cray EX System API Gateway The administrator must have the HPE Cray EX System CLI (cray command) installed on the above host The HPE Cray EX System CLI must be configured (initialized - cray init command) to reach the HPE Cray EX System API Gateway The administrator must be logged in as an administrator to the HPE Cray EX System CLI (cray auth login command)  Procedure List all the resource specifications registered in UAS.</description>
    </item>
    
    <item>
      <title>List UAIs</title>
      <link>/docs-csm/en-13/operations/uas_user_and_admin_topics/list_uais/</link>
      <pubDate>Mon, 17 Jul 2023 03:16:30 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/uas_user_and_admin_topics/list_uais/</guid>
      <description>List UAIs There are two ways to list UAIs in UAS. One of these is an administrative action and provides access to all currently running UAIs. The other is associated with the Legacy UAI Management mode and provides authorized users access to their own UAIs. Both of these are shown here.
View the details of every UAI that is running by using a direct UAS administrative command.
Prerequisites For administrative procedures:</description>
    </item>
    
    <item>
      <title>List UAS Version Information</title>
      <link>/docs-csm/en-13/operations/uas_user_and_admin_topics/list_uas_information/</link>
      <pubDate>Mon, 17 Jul 2023 03:16:30 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/uas_user_and_admin_topics/list_uas_information/</guid>
      <description>List UAS Version Information Use the cray uas mgr-info list command to determine the version and service name of UAS.
List UAS Version with cray uas mgr-info list cray uas mgr-info list Example output:
service_name = &amp;#34;cray-uas-mgr&amp;#34; version = &amp;#34;1.16.1&amp;#34; Top: User Access Service (UAS)
Next-Topic: End-User UAIs</description>
    </item>
    
    <item>
      <title>List Volumes Registered In UAS</title>
      <link>/docs-csm/en-13/operations/uas_user_and_admin_topics/list_volumes_registered_in_uas/</link>
      <pubDate>Mon, 17 Jul 2023 03:16:30 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/uas_user_and_admin_topics/list_volumes_registered_in_uas/</guid>
      <description>List Volumes Registered in UAS List the details of all volumes registered in UAS with the cray uas admin config volumes list command. Use this command to obtain the volume_id value of volume, which is required for other UAS administrative commands.
Prerequisites  The administrator must be logged into an NCN or a host that has administrative access to the HPE Cray EX System API Gateway The administrator must have the HPE Cray EX System CLI (cray command) installed on the above host The HPE Cray EX System CLI must be configured (initialized - cray init command) to reach the HPE Cray EX System API Gateway The administrator must be logged in as an administrator to the HPE Cray EX System CLI (cray auth login command)  Procedure The volume registrations in the UAS configuration can be quite extensive and sometimes difficult to read in the default TOML format used by the cray administrative CLI.</description>
    </item>
    
    <item>
      <title>Log In To A Broker UAI</title>
      <link>/docs-csm/en-13/operations/uas_user_and_admin_topics/log_in_to_a_broker_uai/</link>
      <pubDate>Mon, 17 Jul 2023 03:16:30 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/uas_user_and_admin_topics/log_in_to_a_broker_uai/</guid>
      <description>Log in to a Broker UAI SSH to log into a Broker UAI and reach the End-User UAIs on demand.
Prerequisites  The user must be logged into a host that can reach the external IP address of the Broker UAI The user must know the external IP address or DNS host name of the Broker UAI  Procedure   Log in to the Broker UAI.
The following example is the first login for the vers user:</description>
    </item>
    
    <item>
      <title>End-user UAIs</title>
      <link>/docs-csm/en-13/operations/uas_user_and_admin_topics/end_user_uais/</link>
      <pubDate>Mon, 17 Jul 2023 03:16:29 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/uas_user_and_admin_topics/end_user_uais/</guid>
      <description>End-User UAIs UAIs used for interactive logins are called End-User UAIs. End-User UAIs can be seen as lightweight User Access Nodes (UANs), but there are important differences between UAIs and UANs.
End-User UAIs are not dedicated hardware like UANs. They are implemented as containers orchestrated by Kubernetes, which makes them subject to Kubernetes scheduling and resource management rules. One key element of Kubernetes orchestration is impermanence. While End-User UAIs are often long running, Kubernetes can reschedule or recreate them as needed to meet resource and node availability constraints.</description>
    </item>
    
    <item>
      <title>Examine A UAI Using A Direct Administrative Command</title>
      <link>/docs-csm/en-13/operations/uas_user_and_admin_topics/examine_a_uai_using_a_direct_administrative_command/</link>
      <pubDate>Mon, 17 Jul 2023 03:16:29 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/uas_user_and_admin_topics/examine_a_uai_using_a_direct_administrative_command/</guid>
      <description>Examine a UAI Using a Direct Administrative Command Print out information about a UAI.
Prerequisites  The administrator must be logged into an NCN or a host that has administrative access to the HPE Cray EX System API Gateway. The administrator must have the HPE Cray EX System CLI (cray command) installed on the above host. The HPE Cray EX System CLI must be configured (initialized with cray init command) to reach the HPE Cray EX System API Gateway.</description>
    </item>
    
    <item>
      <title>Legacy Mode User-driven UAI Management</title>
      <link>/docs-csm/en-13/operations/uas_user_and_admin_topics/legacy_mode_user-driven_uai_management/</link>
      <pubDate>Mon, 17 Jul 2023 03:16:29 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/uas_user_and_admin_topics/legacy_mode_user-driven_uai_management/</guid>
      <description>Legacy Mode User-Driven UAI Management In the legacy mode, users create and manage their own UAIs through the Cray CLI. A user may create, list, and delete only UAIs owned by the user. The user may not create a UAI for another user, nor may the user see or delete UAIs owned by another user. Once created, the information describing the UAI gives the user the information needed to reach the UAI using SSH and log into it.</description>
    </item>
    
    <item>
      <title>List Available UAI Classes</title>
      <link>/docs-csm/en-13/operations/uas_user_and_admin_topics/list_available_uai_classes/</link>
      <pubDate>Mon, 17 Jul 2023 03:16:29 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/uas_user_and_admin_topics/list_available_uai_classes/</guid>
      <description>List Available UAI Classes View all the details of every available UAI class. Use this information to select a class to apply to one or more UAIs.
Prerequisites  The administrator must be logged into an NCN or a host that has administrative access to the HPE Cray EX System API Gateway The administrator must have the HPE Cray EX System CLI (cray command) installed on the above host The HPE Cray EX System CLI must be configured (initialized - cray init command) to reach the HPE Cray EX System API Gateway The administrator must be logged in as an administrator to the HPE Cray EX System CLI (cray auth login command)  Procedure List all available UAI classes.</description>
    </item>
    
    <item>
      <title>List Available UAI Images In Legacy Mode</title>
      <link>/docs-csm/en-13/operations/uas_user_and_admin_topics/list_available_uai_images_in_legacy_mode/</link>
      <pubDate>Mon, 17 Jul 2023 03:16:29 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/uas_user_and_admin_topics/list_available_uai_images_in_legacy_mode/</guid>
      <description>List Available UAI Images in Legacy Mode A user can list the UAI images available for creating a UAI with a command of the form:
user&amp;gt; cray uas images list For example:
vers&amp;gt; cray uas images list default_image = &amp;#34;registry.local/cray/cray-uai-sles15sp2:1.2.4&amp;#34; image_list = [ &amp;#34;registry.local/cray/cray-uai-sles15sp2:1.2.4&amp;#34;, &amp;#34;registry.local/cray/cray-uai-sanity-test:1.2.4&amp;#34;, &amp;#34;registry.local/cray/cray-uai-broker:1.2.4&amp;#34;,] Top: User Access Service (UAS)
Next Topic: Create UAIs From Specific UAI Images in Legacy Mode</description>
    </item>
    
    <item>
      <title>List Registered UAI Images</title>
      <link>/docs-csm/en-13/operations/uas_user_and_admin_topics/list_registered_uai_images/</link>
      <pubDate>Mon, 17 Jul 2023 03:16:29 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/uas_user_and_admin_topics/list_registered_uai_images/</guid>
      <description>List Registered UAI Images Administrators can use the cray uas admin config images list command to see the list of registered images. This command also displays the UAS registration information about each image.
While Registering a UAI image name with UAS is necessary for UAIs to use the image, simply registering the image is not sufficient. The registered image must also be created and stored appropriately in its container registry. The basic HPE supplied UAI image is both installed and registered at UAS installation or upgrade time by the update-uas Kubernetes job when the update-uas Helm chart is deployed, upgraded or downgraded.</description>
    </item>
    
    <item>
      <title>Delete A UAI Class</title>
      <link>/docs-csm/en-13/operations/uas_user_and_admin_topics/delete_a_uai_class/</link>
      <pubDate>Mon, 17 Jul 2023 03:16:28 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/uas_user_and_admin_topics/delete_a_uai_class/</guid>
      <description>Delete a UAI Class Delete a UAI class. After deletion, the class will no longer be available for creation of UAIs. Existing UAIs are unaffected.
Prerequisites  The administrator must be logged into an NCN or a host that has administrative access to the HPE Cray EX System API Gateway The administrator must have the HPE Cray EX System CLI (cray command) installed on the above host The HPE Cray EX System CLI must be configured (initialized - cray init command) to reach the HPE Cray EX System API Gateway The administrator must be logged in as an administrator to the HPE Cray EX System CLI (cray auth login command) The administrator must know the Class ID of the UAI Class to be deleted: List UAI Classes  Procedure Delete a UAI Class by using a command of the following form:</description>
    </item>
    
    <item>
      <title>Delete A UAI Image Registration</title>
      <link>/docs-csm/en-13/operations/uas_user_and_admin_topics/delete_a_uai_image_registration/</link>
      <pubDate>Mon, 17 Jul 2023 03:16:28 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/uas_user_and_admin_topics/delete_a_uai_image_registration/</guid>
      <description>Delete a UAI Image Registration Unregister a UAI image from UAS.
Prerequisites  The administrator must be logged into an NCN or a host that has administrative access to the HPE Cray EX System API Gateway The administrator must have the HPE Cray EX System CLI (cray command) installed on the above host The HPE Cray EX System CLI must be configured (initialized - cray init command) to reach the HPE Cray EX System API Gateway The administrator must be logged in as an administrator to the HPE Cray EX System CLI (cray auth login command) The administrator must know the name of the UAI Image Registration to be deleted: List Registered UAI Images  Procedure Deleting a UAI image from UAS removes the UAI image registration from UAS.</description>
    </item>
    
    <item>
      <title>Delete A UAI Resource Specification</title>
      <link>/docs-csm/en-13/operations/uas_user_and_admin_topics/delete_a_uai_resource_specification/</link>
      <pubDate>Mon, 17 Jul 2023 03:16:28 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/uas_user_and_admin_topics/delete_a_uai_resource_specification/</guid>
      <description>Delete a UAI Resource Specification Delete a specific UAI resource specification using the resource_id of that specification. Once deleted, UAIs will no longer be able to use that specification for creation. Existing UAIs are not affected by the change.
Prerequisites  The administrator must be logged into an NCN or a host that has administrative access to the HPE Cray EX System API Gateway The administrator must have the HPE Cray EX System CLI (cray command) installed on the above host The HPE Cray EX System CLI must be configured (initialized - cray init command) to reach the HPE Cray EX System API Gateway The administrator must be logged in as an administrator to the HPE Cray EX System CLI (cray auth login command) The administrator must know the Resource ID of the resource specification to be deleted: List Resource Specifications  Procedure To delete a particular resource specification, use a command of the following form:</description>
    </item>
    
    <item>
      <title>Delete A Volume Configuration</title>
      <link>/docs-csm/en-13/operations/uas_user_and_admin_topics/delete_a_volume_configuration/</link>
      <pubDate>Mon, 17 Jul 2023 03:16:28 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/uas_user_and_admin_topics/delete_a_volume_configuration/</guid>
      <description>Delete a Volume Configuration Delete an existing volume configuration. This procedure does not delete the underlying object referred to by the UAS volume configuration.
Prerequisites  The administrator must be logged into an NCN or a host that has administrative access to the HPE Cray EX System API Gateway The administrator must have the HPE Cray EX System CLI (cray command) installed on the above host The HPE Cray EX System CLI must be configured (initialized - cray init command) to reach the HPE Cray EX System API Gateway The administrator must be logged in as an administrator to the HPE Cray EX System CLI (cray auth login command) The administrator must know the Volume ID of the UAS volume to be deleted: List Volumes Registered in UAS  Procedure Delete the target volume configuration.</description>
    </item>
    
    <item>
      <title>Elements Of A UAI</title>
      <link>/docs-csm/en-13/operations/uas_user_and_admin_topics/elements_of_a_uai/</link>
      <pubDate>Mon, 17 Jul 2023 03:16:28 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/uas_user_and_admin_topics/elements_of_a_uai/</guid>
      <description>Elements of a UAI All UAIs can have the following attributes associated with them:
 A required container image An optional set of volumes An optional resource specification An optional collection of other configuration items  This topic explains each of these attributes.
UAI container image The container image for a UAI (UAI image) defines and provides the basic environment available to the user. This environment includes, among other things:</description>
    </item>
    
    <item>
      <title>Create A UAI With Additional Ports</title>
      <link>/docs-csm/en-13/operations/uas_user_and_admin_topics/create_a_uai_with_additional_ports/</link>
      <pubDate>Mon, 17 Jul 2023 03:16:27 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/uas_user_and_admin_topics/create_a_uai_with_additional_ports/</guid>
      <description>Create a UAI with Additional Ports In legacy mode UAI creation, an option is available to expose UAI ports to the customer user network in addition to the the port used for SSH access. These ports are restricted to ports 80, 443, and 8888. This procedure allows a user or administrator to create a new UAI with these additional ports.
Prerequisites  The user must be logged into a host that has user access to the HPE Cray EX System API Gateway The user must have an installed initialized cray CLI and network access to the API Gateway The user must have the HPE Cray EX System CLI (cray command) installed on the above host The HPE Cray EX System CLI must be configured (initialized - cray init command) to reach the HPE Cray EX System API Gateway The user must be logged in as to the HPE Cray EX System CLI (cray auth login command) The user must have a public SSH key configured on the host from which SSH connections to the UAI will take place The user must have access to a file containing the above public SSH key  Limitations Only ports 80, 443, and 8888 can be exposed.</description>
    </item>
    
    <item>
      <title>Create And Use Default UAIs In Legacy Mode</title>
      <link>/docs-csm/en-13/operations/uas_user_and_admin_topics/create_and_use_default_uais_in_legacy_mode/</link>
      <pubDate>Mon, 17 Jul 2023 03:16:27 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/uas_user_and_admin_topics/create_and_use_default_uais_in_legacy_mode/</guid>
      <description>Create and Use Default UAIs in Legacy Mode Create a UAI using the default UAI image or the default UAI class in legacy mode.
Procedure   (user&amp;gt;) Create a UAI with a command of the following form:
cray uas create --public-key &amp;#39;&amp;lt;path&amp;gt;&amp;#39; &amp;lt;path&amp;gt; is the path to a file containing an SSH public-key matched to the SSH private key belonging to the user.
  (user&amp;gt;) Watch the UAI and see when it is ready for logins.</description>
    </item>
    
    <item>
      <title>Customize End-user UAI Images</title>
      <link>/docs-csm/en-13/operations/uas_user_and_admin_topics/customize_end-user_uai_images/</link>
      <pubDate>Mon, 17 Jul 2023 03:16:27 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/uas_user_and_admin_topics/customize_end-user_uai_images/</guid>
      <description>Customize End-User UAI Images The provided end-user UAI image is a basic UAI image that includes an up-to-date version of the SLES Linux distribution. It provides an entry point to using UAIs and an easy way for administrators to experiment with UAS configurations. To support building software to be run in compute nodes, or other HPC and Analytics workflows, it is necessary to create a custom end-user UAI image and use that.</description>
    </item>
    
    <item>
      <title>Customize The Broker UAI Image</title>
      <link>/docs-csm/en-13/operations/uas_user_and_admin_topics/customize_the_broker_uai_image/</link>
      <pubDate>Mon, 17 Jul 2023 03:16:27 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/uas_user_and_admin_topics/customize_the_broker_uai_image/</guid>
      <description>Customize the Broker UAI Image The Broker UAI image that comes with UAS is the image used to construct Broker UAIs.
The key pieces of the Broker UAI image are:
 An entrypoint shell script that initializes the container and starts the SSH daemon running. An SSH configuration that forces logged in users into the switchboard command which creates / selects End-User UAIs and redirects connections.  The primary way to customize the Broker UAI image is by defining volumes and connecting them to the Broker UAI class for a given broker.</description>
    </item>
    
    <item>
      <title>Delete A UAI</title>
      <link>/docs-csm/en-13/operations/uas_user_and_admin_topics/delete_a_uai/</link>
      <pubDate>Mon, 17 Jul 2023 03:16:27 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/uas_user_and_admin_topics/delete_a_uai/</guid>
      <description>Delete a UAI There are two procedures described here. The first shows how an administrator can manually delete arbitrary UAIs or delete UAIs belonging to a given user or created using a given UAI Class. The second shows how an authorized user on can delete UAIs created in the legacy UIA creation mode.
When a UAI is deleted, any running WLM sessions associated with the owner of the UAI are left intact and can be interacted with through future UAIs owned by the same user or from UANs.</description>
    </item>
    
    <item>
      <title>Configure A Default UAI Class For Legacy Mode</title>
      <link>/docs-csm/en-13/operations/uas_user_and_admin_topics/configure_a_default_uai_class_for_legacy_mode/</link>
      <pubDate>Mon, 17 Jul 2023 03:16:26 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/uas_user_and_admin_topics/configure_a_default_uai_class_for_legacy_mode/</guid>
      <description>Configure a Default UAI Class for Legacy Mode Using a default UAI class is optional but recommended for any site using the legacy UAI management mode that wants to have some control over UAIs created by users. UAI classes used for this purpose need to have certain minimum configuration in them:
 The image_id field set to identify the image used to construct UAIs The volume_list field set to the list of volumes to mount in UAIs The public_ip field set to true The uai_compute_network flag set to true (if workload management will be used) The default flag set to true to make this the default UAI class  To make UAIs useful, there is a minimum set of volumes that should be defined in the UAS configuration:</description>
    </item>
    
    <item>
      <title>Create A UAI</title>
      <link>/docs-csm/en-13/operations/uas_user_and_admin_topics/create_a_uai/</link>
      <pubDate>Mon, 17 Jul 2023 03:16:26 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/uas_user_and_admin_topics/create_a_uai/</guid>
      <description>Create a UAI The UAS allows either administrators or authorized users using the Legacy Mode of UAI management to create UAIs. This section shows both methods.
It is rare that an an administrator would hand-craft an End-User UAI using this administrative procedure, but it is possible. This is, however, the procedure used to create Broker UAIs for Broker Mode UAI Management.
Prerequisites For administrative procedures:
 The administrator must be logged into an NCN or a host that has administrative access to the HPE Cray EX System API Gateway The administrator must have the HPE Cray EX System CLI (cray command) installed on the above host The HPE Cray EX System CLI must be configured (initialized - cray init command) to reach the HPE Cray EX System API Gateway The administrator must be logged in as an administrator to the HPE Cray EX System CLI (cray auth login command) For the administrative procedure,  the administrator must know at least the UAI Class ID to use in creating the UAI, or A default UAI Class must be defined that creates the desired class of UAI    Optional: the administrator may choose a site defined name for the UAI to be used in conjunction with the HPE Cray EX System External DNS mechanism.</description>
    </item>
    
    <item>
      <title>Create A UAI Class</title>
      <link>/docs-csm/en-13/operations/uas_user_and_admin_topics/create_a_uai_class/</link>
      <pubDate>Mon, 17 Jul 2023 03:16:26 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/uas_user_and_admin_topics/create_a_uai_class/</guid>
      <description>Create a UAI Class Add a new User Access Instance (UAI) class to the User Access Service (UAS) so that the class can be used to configure UAIs.
Prerequisites  The administrator must be logged into an NCN or a host that has administrative access to the HPE Cray EX System API Gateway The administrator must have the HPE Cray EX System CLI (cray command) installed on the above host The HPE Cray EX System CLI must be configured (initialized - cray init command) to reach the HPE Cray EX System API Gateway The administrator must be logged in as an administrator to the HPE Cray EX System CLI (cray auth login command)  Procedure Add a UAI class by using the command in the following example.</description>
    </item>
    
    <item>
      <title>Create A UAI Resource Specification</title>
      <link>/docs-csm/en-13/operations/uas_user_and_admin_topics/create_a_uai_resource_specification/</link>
      <pubDate>Mon, 17 Jul 2023 03:16:26 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/uas_user_and_admin_topics/create_a_uai_resource_specification/</guid>
      <description>Create a UAI Resource Specification Add a resource specification to UAS. Once added, a resource specification can be used to request or limit specific resource consumption on a host node or gain access to host node features managed by Kubernetes resources. The examples in this documentation focus on memory and CPU usage, but Kubernetes does use resources in some configurations to manage access to other kinds of resources.
Prerequisites  The administrator must be logged into an NCN or a host that has administrative access to the HPE Cray EX System API Gateway The administrator must have the HPE Cray EX System CLI (cray command) installed on the above host The HPE Cray EX System CLI must be configured (initialized - cray init command) to reach the HPE Cray EX System API Gateway The administrator must be logged in as an administrator to the HPE Cray EX System CLI (cray auth login command)  Procedure Add a resource specification.</description>
    </item>
    
    <item>
      <title>Create UAIs From Specific Uai Images In Legacy Mode</title>
      <link>/docs-csm/en-13/operations/uas_user_and_admin_topics/create_uais_from_specific_uai_images_in_legacy_mode/</link>
      <pubDate>Mon, 17 Jul 2023 03:16:26 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/uas_user_and_admin_topics/create_uais_from_specific_uai_images_in_legacy_mode/</guid>
      <description>Create UAIs From Specific UAI Images in Legacy Mode A user can create a UAI from a specific UAI image (assuming no default UAI class exists) using a command of the form:
user&amp;gt; cray uas create --publickey &amp;lt;path&amp;gt; --imagename &amp;lt;image-name&amp;gt; &amp;lt;image-name&amp;gt; is the name shown above in the list of UAI images.
For example:
vers&amp;gt; cray uas images list default_image = &amp;#34;registry.local/cray/cray-uai-sles15sp2:1.2.4&amp;#34; image_list = [ &amp;#34;registry.local/cray/cray-uai-sles15sp2:1.2.4&amp;#34;, &amp;#34;registry.local/cray/cray-uai-sanity-test:1.2.4&amp;#34;, &amp;#34;registry.local/cray/cray-uai-broker:1.2.4&amp;#34;,] vers&amp;gt; cray uas create --publickey ~/.</description>
    </item>
    
    <item>
      <title>Broker Mode UAI Management</title>
      <link>/docs-csm/en-13/operations/uas_user_and_admin_topics/broker_mode_uai_management/</link>
      <pubDate>Mon, 17 Jul 2023 03:16:25 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/uas_user_and_admin_topics/broker_mode_uai_management/</guid>
      <description>Broker Mode UAI Management A Broker UAI is a special kind of UAI whose job is not to host users directly but to accept attempts to reach a UAI, locate or create a UAI for the user making the attempt, and then pass the user&amp;rsquo;s connection on to the correct UAI. Multiple Broker UAIs can be created, each serving users with UAIs of a different classes. This makes it possible to set up UAIs for varying workflows and environments as needed.</description>
    </item>
    
    <item>
      <title>Choosing UAI Resource Settings</title>
      <link>/docs-csm/en-13/operations/uas_user_and_admin_topics/choosing_uai_resource_settings/</link>
      <pubDate>Mon, 17 Jul 2023 03:16:25 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/uas_user_and_admin_topics/choosing_uai_resource_settings/</guid>
      <description>Choosing UAI Resource Settings The Resource Specifications and UAI Classes sections describe how to set up resource specifications to be used with UAIs. Refer to those sections for all procedures and specific data structures associated with resources. In this section the question of how and why to configure UAI resources is addressed. While Kubernetes resource requests and limits can be used for other things, this section focuses on memory requests and limits and CPU requests and limits.</description>
    </item>
    
    <item>
      <title>Common UAI Configuration</title>
      <link>/docs-csm/en-13/operations/uas_user_and_admin_topics/common_uai_config/</link>
      <pubDate>Mon, 17 Jul 2023 03:16:25 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/uas_user_and_admin_topics/common_uai_config/</guid>
      <description>Common UAI Configuration This section provides guidance on common UAI configuration activities. Specific procedures and settings are covered elsewhere, but each topic provides links to the appropriate information as well as guidance on using that particular configuration.
Refer to the following configuration topics:
 Choosing UAI Resource Settings Setting End-User UAI Timeouts Broker UAI Resiliency and Load Balancing  Top: User Access Service (UAS)
Next Topic: Choosing UAI Resource Settings</description>
    </item>
    
    <item>
      <title>Configure A Broker UAI Class</title>
      <link>/docs-csm/en-13/operations/uas_user_and_admin_topics/configure_a_broker_uai_class/</link>
      <pubDate>Mon, 17 Jul 2023 03:16:25 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/uas_user_and_admin_topics/configure_a_broker_uai_class/</guid>
      <description>Configure a Broker UAI Class Configuring a Broker UAI class consists of the following actions:
 Create volumes to hold any site-specific authentication, SSH, or other configuration required Choose the End-User UAI class for which the Broker UAI will serve instances Create a UAI Class with (at a minimum):  namespace set to uas default set to false volume_mounts set to the list of customization volume-ids created above public_ip set to true uai_compute_network set to false uai_creation_class set to the class-id of the End-User UAI class    The basic contents of a Broker UAI Class is discussed in UAI Classes.</description>
    </item>
    
    <item>
      <title>Configure End-user UAI Classes For Broker Mode</title>
      <link>/docs-csm/en-13/operations/uas_user_and_admin_topics/configure_end-user_uai_classes_for_broker_mode/</link>
      <pubDate>Mon, 17 Jul 2023 03:16:25 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/uas_user_and_admin_topics/configure_end-user_uai_classes_for_broker_mode/</guid>
      <description>Configure End-User UAI Classes for Broker Mode Each UAI broker will create and manage a single class of End-User UAIs. A site using the Broker Mode of UAI management must create a Brokered End-User UAI Class for each distinct type of End-User UAI it wants served by a Broker UAI. Information on what should be configured for a Brokered End-User UAI Class can be found in UAI Classes.
Top: User Access Service (UAS)</description>
    </item>
    
    <item>
      <title>Configure UAIs In UAS</title>
      <link>/docs-csm/en-13/operations/uas_user_and_admin_topics/configure_uais_in_uas/</link>
      <pubDate>Mon, 17 Jul 2023 03:16:25 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/uas_user_and_admin_topics/configure_uais_in_uas/</guid>
      <description>Configure UAIs in UAS The sub-topics in this section cover the four main elements of UAI configuration in UAS, and provide Links to procedures for listing, adding, examining, updating, and deleting each kind of element.
Options for the elements of a UAI are maintained in the UAS configuration. The following can be configured in UAS:
 UAI images Volumes Resource Specifications UAI Classes  Only users who are defined as administrators in an HPE Cray EX system and are logged in using the administrative CLI (cray command) can configure UAS.</description>
    </item>
    
    <item>
      <title>Add A Volume To UAS</title>
      <link>/docs-csm/en-13/operations/uas_user_and_admin_topics/add_a_volume_to_uas/</link>
      <pubDate>Mon, 17 Jul 2023 03:16:24 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/uas_user_and_admin_topics/add_a_volume_to_uas/</guid>
      <description>Add a Volume to UAS This procedure registers and configures a volume in UAS so that the volume can be mounted in UAIs.
See List Volumes Registered in UAS for examples of valid volume configurations. Refer to Elements of a UAI for descriptions of the volume configuration fields and values.
Note the following caveats about adding volumes to UAS:
 A volume description may specify an underlying directory that is NFS-mounted on the UAI host nodes.</description>
    </item>
    
    <item>
      <title>Component Names (xnames)</title>
      <link>/docs-csm/en-13/operations/component_names_xnames/</link>
      <pubDate>Mon, 17 Jul 2023 03:16:24 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/component_names_xnames/</guid>
      <description>Component Names (xnames) Component names (xnames) identify the geolocation for hardware components in the HPE Cray EX system. Every component is uniquely identified by these component names. Some, like the system cabinet number or the CDU number, can be changed by site needs. There is no geolocation encoded within the cabinet number, such as an X-Y coordinate system to relate to the floor layout of the cabinets. Other component names refer to the location within a cabinet and go down to the port on a card or switch or the socket holding a processor or a memory DIMM location.</description>
    </item>
    
    <item>
      <title>Cray System Management Administration Guide</title>
      <link>/docs-csm/en-13/operations/readme/</link>
      <pubDate>Mon, 17 Jul 2023 03:16:24 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/readme/</guid>
      <description>Cray System Management (CSM) Administration Guide The Cray System Management (CSM) operational activities are administrative procedures required to operate an HPE Cray EX system with CSM software installed.
The following administrative topics can be found in this guide:
 CSM product management Bare-metal Image management Boot orchestration System power off procedures System power on procedures Power management Artifact management Compute rolling upgrades Configuration management Kubernetes Package repository management Security and authentication Resiliency ConMan Utility storage System management health System Layout Service (SLS) System configuration service Hardware State Manager (HSM) Hardware Management (HM) collector HPE Power Distribution Unit (PDU) Node management Network  Management network Customer accessible networks (CMN/CAN/CHN) Dynamic Host Configuration Protocol (DHCP) Domain Name Service (DNS) External DNS MetalLB in BGP-mode   Spire Update firmware with FAS User Access Service (UAS) System Admin Toolkit (SAT) Argo Backup and recovery Multi-tenancy  CSM product management Important procedures for configuring, managing, and validating the CSM environment.</description>
    </item>
    
    <item>
      <title>Remove Artifacts From Product Installations</title>
      <link>/docs-csm/en-13/operations/csm_product_management/remove_artifacts_from_product_installations/</link>
      <pubDate>Mon, 17 Jul 2023 03:16:24 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/csm_product_management/remove_artifacts_from_product_installations/</guid>
      <description>Remove Artifacts from Product Installations Remove product artifacts that were imported from various Cray products. These instructions provide guidance for removing Image Management Service (IMS) images, IMS recipes, and Git repositories present in the Cray Product Catalog from the system.
The examples in this procedure show how to remove the product artifacts for the Cray System Management (CSM) product.
WARNING: If individual Cray products have removal procedures, those instructions supersede this procedure.</description>
    </item>
    
    <item>
      <title>Validate Signed Rpms</title>
      <link>/docs-csm/en-13/operations/csm_product_management/validate_signed_rpms/</link>
      <pubDate>Mon, 17 Jul 2023 03:16:24 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/csm_product_management/validate_signed_rpms/</guid>
      <description>Validate Signed RPMs The HPE Cray EX system signs RPMs to provide an extra level of security. Use the following procedure to import a key from either My HPE Software Center or a Kubernetes Secret, and then use that key to validate the RPM package signatures on each node type.
The RPMs will vary on compute, application, worker, master, and storage nodes. Check each node type to ensure the RPMs are correctly signed.</description>
    </item>
    
    <item>
      <title>Configure Keycloak Account</title>
      <link>/docs-csm/en-13/operations/csm_product_management/configure_keycloak_account/</link>
      <pubDate>Mon, 17 Jul 2023 03:16:23 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/csm_product_management/configure_keycloak_account/</guid>
      <description>Configure Keycloak Account Installation of CSM software includes a default account for administrative access to keycloak.
Depending on choices made during the installation, there may be a federated connection to an external Identity Provider (IdP), such as an LDAP or AD server, which enables the use of external accounts in keycloak.
However, if the external accounts are not available, then an &amp;ldquo;internal user account&amp;rdquo; could be created in keycloak. Having a usable account in keycloak with administrative authorization enables the use of the cray CLI for many administrative commands, such as those used to Validate CSM Health and general operation of the management services via the API gateway.</description>
    </item>
    
    <item>
      <title>Configure Non-compute Nodes With CFS</title>
      <link>/docs-csm/en-13/operations/csm_product_management/configure_non-compute_nodes_with_cfs/</link>
      <pubDate>Mon, 17 Jul 2023 03:16:23 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/csm_product_management/configure_non-compute_nodes_with_cfs/</guid>
      <description>Configure Non-Compute Nodes with CFS Non-compute node (NCN) personalization applies post-boot configuration to the HPE Cray EX management nodes. Several HPE Cray EX product environments outside of CSM require NCN personalization to function. Consult the manual for each product to configure them on NCNs by referring to the HPE Cray EX System Software Getting Started Guide S-8000 on the HPE Customer Support Center.
This procedure defines the NCN personalization process for the CSM product using the Configuration Framework Service (CFS).</description>
    </item>
    
    <item>
      <title>Perform NCN Personalization</title>
      <link>/docs-csm/en-13/operations/csm_product_management/perform_ncn_personalization/</link>
      <pubDate>Mon, 17 Jul 2023 03:16:23 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/csm_product_management/perform_ncn_personalization/</guid>
      <description>Perform NCN Personalization NCN personalization is the process of applying product-specific configuration to NCNs post-boot.
Prerequisites Prior to running this procedure, gather the following information required by CFS to create a configuration layer:
 HTTP clone URL for the configuration repository in VCS Path to the Ansible play to run in the repository Commit ID in the repository for CFS to pull and run on the nodes  Products may supply multiple plays to run, in which case multiple configuration layers must be created.</description>
    </item>
    
    <item>
      <title>Post-install Customizations</title>
      <link>/docs-csm/en-13/operations/csm_product_management/post_install_customizations/</link>
      <pubDate>Mon, 17 Jul 2023 03:16:23 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/csm_product_management/post_install_customizations/</guid>
      <description>Post-Install Customizations Post-install customizations may be needed as systems scale. These customizations also need to persist across future installs or upgrades. Not all resources can be customized post-install; common scenarios are documented in the following sections.
The following is a guide for determining where issues may exist, how to adjust the resources, and how to ensure the changes will persist. Different values may be be needed for systems as they scale.</description>
    </item>
    
    <item>
      <title>Redeploying A Chart</title>
      <link>/docs-csm/en-13/operations/csm_product_management/redeploying_a_chart/</link>
      <pubDate>Mon, 17 Jul 2023 03:16:23 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/csm_product_management/redeploying_a_chart/</guid>
      <description>Redeploying a Chart Administrators are able to customize many aspects of the system in order to address problems or tailor it to better suit their requirements. Often this requires redeploying one or more Helm charts. This page outlines the procedure for doing this in CSM. Other parts of the CSM documentation will reference this page if you are instructed to redeploy a chart. In those cases, the source page that links to this one should specify which charts should be redeployed and what customizations (if any) should be made to them.</description>
    </item>
    
    <item>
      <title>Change Passwords And Credentials</title>
      <link>/docs-csm/en-13/operations/csm_product_management/change_passwords_and_credentials/</link>
      <pubDate>Mon, 17 Jul 2023 03:16:22 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/csm_product_management/change_passwords_and_credentials/</guid>
      <description>Change Passwords and Credentials This is an overarching procedure to change all credentials managed by Cray System Management (CSM) in HPE Cray EX system to new values.
There are many passwords and credentials used in different contexts to manage the system. These can be changed as needed. Their initial settings are documented, so it is recommended to change them during or soon after a CSM software installation.
Prerequisites  Review procedures in Manage System Passwords.</description>
    </item>
    
    <item>
      <title>Configure Packages With CFS</title>
      <link>/docs-csm/en-13/operations/csm_product_management/configure_csm_packages_with_cfs/</link>
      <pubDate>Mon, 17 Jul 2023 03:16:22 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/csm_product_management/configure_csm_packages_with_cfs/</guid>
      <description>Configure CSM packages with CFS CSM includes a playbook that should be applied to Compute and Application node images. The csm_packages.yml playbook installs the packages for both the CFS and BOS reporters. These packages are necessary for CFS and BOS to run, so a configuration layer containing the playbook must be included in the image customization for any nodes that are expected to be managed with CFS and BOS.
Setting up the CSM configuration layer To setup the compute configuration layer, first gather the following information:</description>
    </item>
    
    <item>
      <title>Documentation Conventions</title>
      <link>/docs-csm/en-13/introduction/documentation_conventions/</link>
      <pubDate>Mon, 17 Jul 2023 03:16:22 +0000</pubDate>
      
      <guid>/docs-csm/en-13/introduction/documentation_conventions/</guid>
      <description>Documentation Conventions This outlines conventions and standards that are used in this documentation.
 Markdown format File formats Typographic conventions Command prompt conventions  Host and user name in command prompts Node abbreviations Command prompt reference Command prompt location Command prompts inside new shells  chroot example kubectl exec example Combined ssh and chroot example   Directory path in command prompt   Ability to pause and resume procedures  Bad example Good example    Markdown format This documentation is in Markdown format.</description>
    </item>
    
    <item>
      <title>Security Hardening</title>
      <link>/docs-csm/en-13/operations/csm_product_management/apply_security_hardening/</link>
      <pubDate>Mon, 17 Jul 2023 03:16:22 +0000</pubDate>
      
      <guid>/docs-csm/en-13/operations/csm_product_management/apply_security_hardening/</guid>
      <description>Security Hardening This is an overarching guide to further harden the security posture of a Cray System Management (CSM) system.
If a subset of the steps in this procedure were completed as a consequence of an install, upgrade, or other guidance, then it is safe to skip that subset following a review.
Prerequisites None.
Procedure   Change passwords and credentials.
Perform procedure(s) in Change Passwords and Credentials.
  Limit Kubernetes API audit log Retention.</description>
    </item>
    
    <item>
      <title>CAPMC Deprecation Notice</title>
      <link>/docs-csm/en-13/introduction/deprecated_features/capmc_deprecation_notice/</link>
      <pubDate>Mon, 17 Jul 2023 03:16:21 +0000</pubDate>
      
      <guid>/docs-csm/en-13/introduction/deprecated_features/capmc_deprecation_notice/</guid>
      <description>CAPMC Deprecation Notice Many CAPMC v1 REST API and CLI features are being deprecated as of CSM version 1.0. Full removal of the following deprecated CAPMC features will happen in a future CSM release. Further development of CAPMC service and CLI has stopped. CAPMC has entered end-of-life but will still be generally available. CAPMC is going to be replaced with the Power Control Service (PCS) in a future release. The current API/CLI portfolio for CAPMC is being pruned to better align with the future direction of PCS.</description>
    </item>
    
    <item>
      <title>Deprecated Features</title>
      <link>/docs-csm/en-13/introduction/deprecated_features/readme/</link>
      <pubDate>Mon, 17 Jul 2023 03:16:21 +0000</pubDate>
      
      <guid>/docs-csm/en-13/introduction/deprecated_features/readme/</guid>
      <description>Deprecated Features This page lists deprecated features in CSM. They are no longer being actively developed and are planned to be removed in a future CSM release.
When a feature is first deprecated, it may not yet be announced in which CSM version that feature will be fully removed. When such a decision has been made, that information will be available on this page. For any deprecated features listed on this page that do not yet have an announced CSM version for their planned removal, customers are still strongly encouraged to make plans to migrate away from the deprecated feature.</description>
    </item>
    
    <item>
      <title>Introduction To Installation</title>
      <link>/docs-csm/en-13/introduction/readme/</link>
      <pubDate>Mon, 17 Jul 2023 03:16:21 +0000</pubDate>
      
      <guid>/docs-csm/en-13/introduction/readme/</guid>
      <description>Introduction to CSM Installation This document provides an introduction to the Cray System Management (CSM) installation documentation for an HPE Cray EX system.
Topics  CSM overview Installing or upgrading CSM CSM product stream updates CSM operational activities Deprecated Features Documentation Conventions  CSM overview The CSM installation prepares and deploys a distributed system across a group of management nodes organized into a Kubernetes cluster which uses Ceph for utility storage.</description>
    </item>
    
    <item>
      <title>Overview</title>
      <link>/docs-csm/en-13/introduction/csm_overview/</link>
      <pubDate>Mon, 17 Jul 2023 03:16:21 +0000</pubDate>
      
      <guid>/docs-csm/en-13/introduction/csm_overview/</guid>
      <description>CSM Overview This CSM Overview describes the Cray System Management ecosystem with its hardware, software, and network. It describes how to access these services and components.
The CSM installation prepares and deploys a distributed system across a group of management nodes organized into a Kubernetes cluster which uses Ceph for utility storage. These nodes perform their function as Kubernetes master nodes, Kubernetes worker nodes, or utility storage nodes with the Ceph storage.</description>
    </item>
    
    <item>
      <title>Ceph Csi Troubleshooting</title>
      <link>/docs-csm/en-13/install/troubleshooting_ceph_csi/</link>
      <pubDate>Mon, 17 Jul 2023 03:16:20 +0000</pubDate>
      
      <guid>/docs-csm/en-13/install/troubleshooting_ceph_csi/</guid>
      <description>Ceph CSI Troubleshooting If there has been a failure to initialize all Ceph CSI components on ncn-s001, then the storage node cloud-init may need to be rerun.
Topics  Verify Ceph CSI Rerun storage node cloud-init  1. Verify Ceph CSI Verify that the ceph-csi requirements are in place.
  (ncn-s001#) Log in to ncn-s001 and run the following command.
ceph -s If it returns a connection error, then assume Ceph is not installed.</description>
    </item>
    
    <item>
      <title>PXE Boot Troubleshooting</title>
      <link>/docs-csm/en-13/install/troubleshooting_pxe_boot/</link>
      <pubDate>Mon, 17 Jul 2023 03:16:20 +0000</pubDate>
      
      <guid>/docs-csm/en-13/install/troubleshooting_pxe_boot/</guid>
      <description>PXE Boot Troubleshooting This page is designed to cover various issues that arise when trying to PXE boot nodes in an HPE Cray EX system.
 Configuration required for PXE booting Switch configuration  Aruba configuration Mellanox configuration   Next steps  Node iPXE retries and NIC order Restart BSS Restart Kea Missing BSS data    In order for PXE booting to work successfully, the management network switches need to be configured correctly.</description>
    </item>
    
    <item>
      <title>Troubleshooting Installation Problems</title>
      <link>/docs-csm/en-13/install/troubleshooting_installation/</link>
      <pubDate>Mon, 17 Jul 2023 03:16:20 +0000</pubDate>
      
      <guid>/docs-csm/en-13/install/troubleshooting_installation/</guid>
      <description>Troubleshooting Installation Problems The installation of the Cray System Management (CSM) product requires knowledge of the various nodes and switches for the HPE Cray EX system. The procedures in this section should be referenced during the CSM install for additional information on system hardware, troubleshooting, and administrative tasks related to CSM.
Topics  Reset root password on a LiveCD USB PXE boot troubleshooting Restart network services and interfaces on NCNs Utility storage node installation troubleshooting Ceph CSI troubleshooting Postgres troubleshooting  Reset root password on a LiveCD USB If the root password on the LiveCD needs to be changed, then see Reset root Password on a LiveCD USB.</description>
    </item>
    
    <item>
      <title>Troubleshooting Unused Drives On Storage Nodes</title>
      <link>/docs-csm/en-13/install/troubleshooting_unused_drives_on_storage_nodes/</link>
      <pubDate>Mon, 17 Jul 2023 03:16:20 +0000</pubDate>
      
      <guid>/docs-csm/en-13/install/troubleshooting_unused_drives_on_storage_nodes/</guid>
      <description>Troubleshooting Unused Drives on Storage Nodes  NOTE: This page is only applicable to storage NCNs on Gigabyte or HPE hardware.
 Utility storage nodes (also known as storage NCNs) are expected to have a particular number of OSDs based on the type of NCN hardware. This page describes how to validate that the storage NCNs have the expected number of OSDs, and provides remediation steps if this is not the case.</description>
    </item>
    
    <item>
      <title>Utility Storage Installation Troubleshooting</title>
      <link>/docs-csm/en-13/install/troubleshooting_utility_storage_node_installation/</link>
      <pubDate>Mon, 17 Jul 2023 03:16:20 +0000</pubDate>
      
      <guid>/docs-csm/en-13/install/troubleshooting_utility_storage_node_installation/</guid>
      <description>Utility Storage Installation Troubleshooting If there is a failure in the creation of Ceph storage on the utility storage nodes for one of these scenarios, the Ceph storage might need to be reinitialized.
Topics  Scenario 1 (Shasta v1.4 only) Scenario 2 (Shasta v1.5 only)  Details Scenario 1 (Shasta 1.4 only) IMPORTANT (FOR NODE INSTALLS/REINSTALLS ONLY): If the Ceph install failed, check the following:
ceph osd tree ID CLASS WEIGHT TYPE NAME STATUS REWEIGHT PRI-AFF -1 83.</description>
    </item>
    
    <item>
      <title>Prepare Compute Nodes</title>
      <link>/docs-csm/en-13/install/prepare_compute_nodes/</link>
      <pubDate>Mon, 17 Jul 2023 03:16:19 +0000</pubDate>
      
      <guid>/docs-csm/en-13/install/prepare_compute_nodes/</guid>
      <description>Prepare Compute Nodes  Configure HPE Apollo 6500 XL645d Gen10 Plus compute nodes  Gather information Configure iLO Configure switch port Cleanup Kea Cleanup HSM Update BIOS time   Next topic  Configure HPE Apollo 6500 XL645d Gen10 Plus compute nodes The HPE Apollo 6500 XL645d Gen10 Plus compute node uses a NIC/shared iLO network port. The NIC is also referred to as the Embedded LOM (LAN On Motherboard) and is available to the booted OS.</description>
    </item>
    
    <item>
      <title>Prepare Site Init</title>
      <link>/docs-csm/en-13/install/prepare_site_init/</link>
      <pubDate>Mon, 17 Jul 2023 03:16:19 +0000</pubDate>
      
      <guid>/docs-csm/en-13/install/prepare_site_init/</guid>
      <description>Prepare Site Init These procedures guide administrators through setting up the site-init directory which contains important customizations for various products.
 Background Create and Initialize Site-Init Directory Create Baseline System Customizations  Setup LDAP configuration End of LDAP configuration   Customer-Specific Customizations  1. Background The shasta-cfg directory included in the CSM release tarball includes relatively static, installation-centric artifacts, such as:
 Cluster-wide network configuration settings required by Helm charts deployed by product stream Loftsman manifests Sealed Secrets Sealed Secret Generate Blocks &amp;ndash; a form of plain-text input that renders to a Sealed Secret Helm chart value overrides that are merged into Loftsman manifests by product stream installers  2.</description>
    </item>
    
    <item>
      <title>Re-installation</title>
      <link>/docs-csm/en-13/install/re-installation/</link>
      <pubDate>Mon, 17 Jul 2023 03:16:19 +0000</pubDate>
      
      <guid>/docs-csm/en-13/install/re-installation/</guid>
      <description>Re-Installation This page details steps to take prior to starting new installation, these pages assume that all the NCNs have been deployed (e.g. there is no more PIT node).
Topics  Quiesce compute and application nodes Disable DHCP service Set IPMI credentials Power off booted nodes Set node BMCs to DHCP Power off the PIT node Configure DNS Check disk space  Quiesce application and compute nodes  NOTE Skip this section if compute nodes and application nodes are not booted.</description>
    </item>
    
    <item>
      <title>Shcd Hmn Tab/hmn Connections Rules</title>
      <link>/docs-csm/en-13/install/shcd_hmn_connections_rules/</link>
      <pubDate>Mon, 17 Jul 2023 03:16:19 +0000</pubDate>
      
      <guid>/docs-csm/en-13/install/shcd_hmn_connections_rules/</guid>
      <description>SHCD HMN Tab/HMN Connections Rules  Introduction  Chassis selection   Compute node  Dense four-node chassis - Gigabyte or Intel chassis Single-node chassis - Apollo 6500 XL675D Dual-node chassis - Apollo 6500 XL645D   Chassis Management Controller (CMC) Management node  Master Worker Storage   Application node  Single-node chassis  Building component names (xnames) for nodes in a single application node chassis   Dual-node chassis  Building component names (xnames) for nodes in a dual application node chassis     Columbia Slingshot switch PDU cabinet controller Cooling door Management switches  Introduction The HMN tab of the SHCD describes the air-cooled hardware present in the system and how these devices are connected to the Hardware Management Network (HMN).</description>
    </item>
    
    <item>
      <title>Accessing Livecd Usb Device After Reboot</title>
      <link>/docs-csm/en-13/install/livecd/access_livecd_usb_device_after_reboot/</link>
      <pubDate>Mon, 17 Jul 2023 03:16:18 +0000</pubDate>
      
      <guid>/docs-csm/en-13/install/livecd/access_livecd_usb_device_after_reboot/</guid>
      <description>Accessing LiveCD USB Device After Reboot This is a procedure that only applies to the LiveCD USB device after the PIT node has been rebooted.
 USB ONLY If the installation above was done from a Remote ISO.
 After deploying the LiveCD&amp;rsquo;s NCN, the LiveCD USB itself is unharmed and available to an administrator.
Procedure   Mount and view the USB device.
mkdir -pv /mnt/{cow,pitdata} mount -vL cow /mnt/cow mount -vL PITDATA /mnt/pitdata ls -ld /mnt/cow/rw/* Example output:</description>
    </item>
    
    <item>
      <title>Boot Livecd Remoteiso</title>
      <link>/docs-csm/en-13/install/livecd/boot_livecd_remoteiso/</link>
      <pubDate>Mon, 17 Jul 2023 03:16:18 +0000</pubDate>
      
      <guid>/docs-csm/en-13/install/livecd/boot_livecd_remoteiso/</guid>
      <description>Boot LiveCD RemoteISO This page will guide a user on booting the LiveCD .iso file directly onto a BMC.
Topics  Prerequisites BMCs&#39; virtual mounts  HPE iLO BMCs Gigabyte BMCs   Configuring  Backing up the overlay COW FS Restoring from an overlay COW FS backup    Prerequisites A Cray Pre-Install Toolkit ISO is required for this process. This ISO can be obtained from:
 The Cray Pre-Install Toolkit ISO included in a CSM release tar file.</description>
    </item>
    
    <item>
      <title>Boot Livecd Usb</title>
      <link>/docs-csm/en-13/install/livecd/boot_livecd_usb/</link>
      <pubDate>Mon, 17 Jul 2023 03:16:18 +0000</pubDate>
      
      <guid>/docs-csm/en-13/install/livecd/boot_livecd_usb/</guid>
      <description>Boot LiveCD USB These steps provide a bootable USB capable of installing this CSM release.
Topics  Create the Bootable Media Boot the LiveCD  Create the Bootable Media Cray Site Init will create the bootable LiveCD. Before creating the media, identify which device will be used for it.
  (external) Set up the initial typescript.
SCRIPT_FILE=&amp;#34;$(pwd)/csm-install-usb.$(date +%Y-%m-%d).txt&amp;#34; script -af &amp;#34;${SCRIPT_FILE}&amp;#34; export PS1=&amp;#39;\u@\H \D{%Y-%m-%d} \t \w # &amp;#39; export OUT_DIR=$(pwd)/csm-temp   (external#) Identify the USB device.</description>
    </item>
    
    <item>
      <title>Pre-installation</title>
      <link>/docs-csm/en-13/install/pre-installation/</link>
      <pubDate>Mon, 17 Jul 2023 03:16:18 +0000</pubDate>
      
      <guid>/docs-csm/en-13/install/pre-installation/</guid>
      <description>Pre-Installation The page walks a user through setting up the Cray LiveCD with the intention of installing Cray System Management (CSM).
 Boot installation environment  Prepare installation environment server Boot the LiveCD First log in Prepare the data partition Set reusable environment variables Exit the console and log in with SSH   Import CSM tarball  Download CSM tarball Import tarball assets   Create system configuration  Validate SHCD Generate topology files Customize system_config.</description>
    </item>
    
    <item>
      <title>Reinstall Livecd</title>
      <link>/docs-csm/en-13/install/livecd/reinstall_livecd_usb/</link>
      <pubDate>Mon, 17 Jul 2023 03:16:18 +0000</pubDate>
      
      <guid>/docs-csm/en-13/install/livecd/reinstall_livecd_usb/</guid>
      <description>Reinstall LiveCD Setup a re-install of LiveCD on a node using the previous configuration.
  (pit#) Backup to the data partition:
mkdir -pv /var/www/ephemeral/backup pushd /var/www/ephemeral/backup tar -czvf &amp;#34;dnsmasq-data-$(date &amp;#39;+%Y-%m-%d_%H-%M-%S&amp;#39;).tar.gz&amp;#34; /etc/dnsmasq.* tar -czvf &amp;#34;network-data-$(date &amp;#39;+%Y-%m-%d_%H-%M-%S&amp;#39;).tar.gz&amp;#34; /etc/sysconfig/network/* cp -pv /etc/hosts ./ popd umount -v /var/www/ephemeral   Unplug the USB device.
The USB device should now contain all the information already loaded, as well as the backups of the initialized files.</description>
    </item>
    
    <item>
      <title>Reset Root Password On A Livecd Usb</title>
      <link>/docs-csm/en-13/install/livecd/reset_root_password_on_a_livecd_usb/</link>
      <pubDate>Mon, 17 Jul 2023 03:16:18 +0000</pubDate>
      
      <guid>/docs-csm/en-13/install/livecd/reset_root_password_on_a_livecd_usb/</guid>
      <description>Reset root Password on a LiveCD USB It may become desirable to clear the password on the LiveCD.
The root password is preserved within the COW partition at cow:rw/etc/shadow. This is the modified copy of the /etc/shadow file used by the operating system.
If a site/user needs to reset/clear the password for root, they can mount their USB on another machine and remove this file from the COW partition. When next booting from the USB it will reinitialize to an empty password for root, and again at next login it will require the password to be changed.</description>
    </item>
    
    <item>
      <title>Deploy Final NCN</title>
      <link>/docs-csm/en-13/install/deploy_final_non-compute_node/</link>
      <pubDate>Mon, 17 Jul 2023 03:16:17 +0000</pubDate>
      
      <guid>/docs-csm/en-13/install/deploy_final_non-compute_node/</guid>
      <description>Deploy Final NCN The following procedure contains information for rebooting and deploying the management node that is currently hosting the LiveCD. At the end of this procedure, the LiveCD will no longer be active. The node it was using will join the Kubernetes cluster as the final of three master nodes, forming a quorum.
IMPORTANT: While the node is rebooting, it will only be available through Serial-Over-LAN (SOL) and local terminals.</description>
    </item>
    
    <item>
      <title>Deploy Management Nodes</title>
      <link>/docs-csm/en-13/install/deploy_non-compute_nodes/</link>
      <pubDate>Mon, 17 Jul 2023 03:16:17 +0000</pubDate>
      
      <guid>/docs-csm/en-13/install/deploy_non-compute_nodes/</guid>
      <description>Deploy Management Nodes The following procedure deploys Linux and Kubernetes software to the management NCNs. Deployment of the nodes starts with booting the storage nodes, followed by the master nodes and worker nodes together.
After the operating system boots on each node, there are some configuration actions which take place. Watching the console or the console log for certain nodes can help to understand what happens and when. When the process completes for all nodes, the Ceph storage is initialized and the Kubernetes cluster is created and ready for a workload.</description>
    </item>
    
    <item>
      <title>Install Services</title>
      <link>/docs-csm/en-13/install/install_csm_services/</link>
      <pubDate>Mon, 17 Jul 2023 03:16:17 +0000</pubDate>
      
      <guid>/docs-csm/en-13/install/install_csm_services/</guid>
      <description>Install CSM Services This procedure will install CSM applications and services into the CSM Kubernetes cluster.
 NOTE Check the information in Known issues before starting this procedure to be warned about possible problems.
  Install CSM services Create base BSS global boot parameters Wait for everything to settle Next topic Known issues   Deploy CSM Applications and Services known issues Setup Nexus known issues  1. Install CSM services  NOTE: During this step, only on systems with only three worker nodes (typically Testing and Development Systems (TDS)), the customizations.</description>
    </item>
    
    <item>
      <title>Services Install Fails Because Of Missing Secret</title>
      <link>/docs-csm/en-13/install/csm_installation_failure/</link>
      <pubDate>Mon, 17 Jul 2023 03:16:17 +0000</pubDate>
      
      <guid>/docs-csm/en-13/install/csm_installation_failure/</guid>
      <description>CSM Services Install Fails Because of Missing Secret When running the install script in the Install CSM Services procedure, it may fail due to a timing-related issue. This page documents how to determine if this problem was the cause of an install script failure, and the appropriate remediation steps to take if it is encountered.
How to determine if an install hit this issue   Verify that the installation script output contains an error about a missing secret.</description>
    </item>
    
    <item>
      <title>Configure Management Network</title>
      <link>/docs-csm/en-13/install/configure_management_network/</link>
      <pubDate>Mon, 17 Jul 2023 03:16:16 +0000</pubDate>
      
      <guid>/docs-csm/en-13/install/configure_management_network/</guid>
      <description>Configure Management Network HPE Cray EX systems can have network switches in many roles: spine switches, leaf switches, leaf-bmc switches, and CDU switches. Newer systems have HPE Aruba switches, while older systems have Dell and Mellanox switches. Switch IP addresses are generated by Cray Site Init (CSI).
Documentation for the Management Network can be found in the HPE Cray EX Management Network Installation and Configuration Guide.
The configuration for these switches will be generated from CSM Automated Network Utility (CANU).</description>
    </item>
    
    <item>
      <title>Create Application Node Config Yaml</title>
      <link>/docs-csm/en-13/install/create_application_node_config_yaml/</link>
      <pubDate>Mon, 17 Jul 2023 03:16:16 +0000</pubDate>
      
      <guid>/docs-csm/en-13/install/create_application_node_config_yaml/</guid>
      <description>Create Application Node Config YAML This topic provides directions on constructing the application_node_config.yaml file. This file controls how the csi config init command finds and treats application nodes discovered in the hmn_connections.json file when generating configuration files for the system.
 Prerequisites Background Directions  Prerequisites The application_node_config.yaml file can be constructed from information from one of the following sources:
 The SHCD Excel spreadsheet for the system The hmn_connections.json file generated from the system&amp;rsquo;s SHCD  Background SHCD and hmn_connections.</description>
    </item>
    
    <item>
      <title>Create Cabinets Yaml</title>
      <link>/docs-csm/en-13/install/create_cabinets_yaml/</link>
      <pubDate>Mon, 17 Jul 2023 03:16:16 +0000</pubDate>
      
      <guid>/docs-csm/en-13/install/create_cabinets_yaml/</guid>
      <description>Create Cabinets YAML This page provides directions on constructing the optional cabinets.yaml file. This file lists cabinet IDs for any systems with non-contiguous cabinet ID numbers and controls how the csi config init command treats cabinet IDs.
The following example file is manually created and follows this format. Each type of cabinet can have several fields:
 total_number of cabinets of this type. starting_id for this cabinet type, and a list of the IDs.</description>
    </item>
    
    <item>
      <title>Create Hmn Connections Json File</title>
      <link>/docs-csm/en-13/install/create_hmn_connections_json/</link>
      <pubDate>Mon, 17 Jul 2023 03:16:16 +0000</pubDate>
      
      <guid>/docs-csm/en-13/install/create_hmn_connections_json/</guid>
      <description>Create HMN Connections JSON File Use this procedure to generate the hmn_connections.json from the system&amp;rsquo;s SHCD Excel document. This process is typically needed when generating the hmn_connections.json file for a new system, or regenerating it when a system&amp;rsquo;s SHCD file is changed (specifically the HMN tab). The hms-shcd-parser tool can be used to generate the hmn_connections.json file.
The SHCD/HMN Connections Rules document explains the expected naming conventions and rules for the HMN tab of the SHCD file, and for the hmn_connections.</description>
    </item>
    
    <item>
      <title>Create NCN Metadata Csv</title>
      <link>/docs-csm/en-13/install/create_ncn_metadata_csv/</link>
      <pubDate>Mon, 17 Jul 2023 03:16:16 +0000</pubDate>
      
      <guid>/docs-csm/en-13/install/create_ncn_metadata_csv/</guid>
      <description>Create NCN Metadata CSV The information in the ncn_metadata.csv file identifies each of the management nodes, assigns the function as a master, worker, or storage node, and provides the MAC address information needed to identify the BMC and the NIC which will be used to boot the node.
Some of the data in the ncn_metadata.csv can be found in the SHCD in the HMN tab. However, the hardest data to collect is the MAC addresses for the node&amp;rsquo;s BMC, the node&amp;rsquo;s bootable network interface, and the pair of network interfaces which will become the bonded interface bond0.</description>
    </item>
    
    <item>
      <title>Create Switch Metadata Csv</title>
      <link>/docs-csm/en-13/install/create_switch_metadata_csv/</link>
      <pubDate>Mon, 17 Jul 2023 03:16:16 +0000</pubDate>
      
      <guid>/docs-csm/en-13/install/create_switch_metadata_csv/</guid>
      <description>Create Switch Metadata CSV This page provides directions on constructing the switch_metadata.csv file.
Prerequisites   The SHCD file for the system.
Check the description for component names while mapping names between the SHCD and the switch_metadata.csv file. See Component Names (xnames).
  Overview This file is manually created to include information about all spine, LeafBMC, CDU, and leaf switches in the system. None of the Slingshot switches for the HSN should be included in this file.</description>
    </item>
    
    <item>
      <title>Collect Mac Addresses For NCNs</title>
      <link>/docs-csm/en-13/install/collect_mac_addresses_for_ncns/</link>
      <pubDate>Mon, 17 Jul 2023 03:16:15 +0000</pubDate>
      
      <guid>/docs-csm/en-13/install/collect_mac_addresses_for_ncns/</guid>
      <description>Collect MAC Addresses for NCNs Topics  Set up networking Collect MAC addresses  1. Set up networking This assumes that the HMN is not setup on the PIT node; these steps cater to bare-metal and configured switches.
  (pit#) Change into the preparation directory.
 NOTE If PITDATA is not defined, then see Set reusable environment variables.
 cd ${PITDATA}/prep   (pit#) Confirm that the ncn_metadata.csv file in this directory has the new information.</description>
    </item>
    
    <item>
      <title>Collecting NCN Mac Addresses</title>
      <link>/docs-csm/en-13/install/collecting_ncn_mac_addresses/</link>
      <pubDate>Mon, 17 Jul 2023 03:16:15 +0000</pubDate>
      
      <guid>/docs-csm/en-13/install/collecting_ncn_mac_addresses/</guid>
      <description>Collecting NCN MAC Addresses This procedure details how to collect the NCN MAC addresses from an HPE Cray EX system. The MAC addresses needed for the Bootstrap MAC, Bond0 MAC0, and Bond0 MAC1 columns in ncn_metadata.csv will be collected. This data will feed into the cloud-init metadata.
The Bootstrap MAC address will be used for identification of this node during the early part of the PXE boot process, before the bonded interface can be established.</description>
    </item>
    
    <item>
      <title>Collecting The BMC Mac Addresses</title>
      <link>/docs-csm/en-13/install/collecting_bmc_mac_addresses/</link>
      <pubDate>Mon, 17 Jul 2023 03:16:15 +0000</pubDate>
      
      <guid>/docs-csm/en-13/install/collecting_bmc_mac_addresses/</guid>
      <description>Collecting the BMC MAC Addresses This guide will detail how to collect BMC MAC addresses from an HPE Cray EX system with configured switches. The BMC MAC address is the exclusive, dedicated LAN for the onboard BMC.
Prerequisites  There is a configured switch with SSH access or unconfigured with COM access (Serial Over LAN/DB-9). A file is available to record the collected BMC information.  Procedure   (pit#) Start a session on the leaf-bmc switch, either using SSH or a USB serial cable.</description>
    </item>
    
    <item>
      <title>Configure Administrative Access</title>
      <link>/docs-csm/en-13/install/configure_administrative_access/</link>
      <pubDate>Mon, 17 Jul 2023 03:16:15 +0000</pubDate>
      
      <guid>/docs-csm/en-13/install/configure_administrative_access/</guid>
      <description>Configure Administrative Access There are several operations which configure administrative access to different parts of the system. Ensuring that the cray CLI can be used with administrative credentials enables use of many management services via commands. The management nodes can be locked from accidental manipulation by the cray capmc and cray fas commands when the intent is to work on the entire system except the management nodes. The cray scsd command can change the SSH keys, NTP server, syslog server, and BMC/controller passwords.</description>
    </item>
    
    <item>
      <title>Cray System Management Install</title>
      <link>/docs-csm/en-13/install/readme/</link>
      <pubDate>Mon, 17 Jul 2023 03:16:15 +0000</pubDate>
      
      <guid>/docs-csm/en-13/install/readme/</guid>
      <description>Cray System Management Install This page will guide an administrator through installing Cray System Management (CSM) on an HPE Cray EX system.
The CSM services provide essential software infrastructure including the API gateway and many micro-services with REST APIs for managing the system.
Fresh-installations on bare-metal or re-installations of CSM must follow this guide in procedural order.
After completing an installation, the CSM product&amp;rsquo;s installed state will need to be validated with various health checks before operational tasks or other product installs (such as Slingshot) can commence.</description>
    </item>
    
    <item>
      <title>Glossary</title>
      <link>/docs-csm/en-13/glossary/</link>
      <pubDate>Mon, 17 Jul 2023 03:16:12 +0000</pubDate>
      
      <guid>/docs-csm/en-13/glossary/</guid>
      <description>Glossary Glossary of terms used in CSM documentation.
 Ansible Execution Environment (AEE) Application Node (AN) Baseboard Management Controller (BMC) Blade Switch Controller (sC) Boot Orchestration Service (BOS) Boot Script Service (BSS) Cabinet Cooling Group Cabinet Environmental Controller (CEC) CEC microcontroller (eC) Chassis Management Module (CMM) Compute Node (CN) Compute Rolling Upgrade Service (CRUS) Configuration Framework Service (CFS) Content Projection Service (CPS) Cray Advanced Platform Monitoring and Control (CAPMC) Cray CLI (cray) Cray Operating System (COS) Cray Programming Environment (CPE) Cray Security Token Service (STS) Cray Site Init (CSI) Cray System Management (CSM) Customer Access Network (CAN) Data Virtualization Service (DVS) EX Compute Cabinet EX TDS Cabinet Fabric Firmware Action Service (FAS) Floor Standing CDU Hardware Management Network (HMN) Hardware Management Notification Fanout Daemon (HMNFD) Hardware State Manager (HSM) Heartbeat Tracker Daemon (HBTD) Hierarchical Namespace Controller (HNC) High Speed Network (HSN) Image Management Service (IMS) Management Nodes Mountain Cabinet Mountain Endpoint Discovery Service (MEDS) NCN Lifecycle Service (NLS) NIC Mezzanine Card (NMC) Node Controller (nC) Node Management Network (NMN) Non-Compute Node (NCN) Olympus Cabinet Power Distribution Unit (PDU) Pre-Install Toolkit (PIT)  LiveCD RemoteISO   Rack-Mounted CDU Rack System Compute Cabinet Redfish Translation Service (RTS) River Cabinet River Endpoint Discovery Service (REDS) Rosetta ASIC Service/IO Cabinet Simple Storage Service (S3) Slingshot Slingshot Blade Switch Slingshot Top of Rack (ToR) Switch Shasta Cabling Diagram (SHCD) Supply/Return Cutoff Valves System Admin Toolkit (SAT) System Configuration Service (SCSD) System Layout Service (SLS) System Management Network (SMNet) System Management Services (SMS) System Management Services (SMS) nodes Tenant and Partition Management System (TAPMS) Top of Rack Switch Controller (sC-ToR) User Access Instance (UAI) User Access Node (UAN) User Access Service (UAS) Version Control Service (VCS) xname  Ansible Execution Environment (AEE) A component used by the Configuration Framework Service (CFS) to execute Ansible code from its configuration layers.</description>
    </item>
    
    <item>
      <title>Kernel</title>
      <link>/docs-csm/en-13/background/ncn_kernel/</link>
      <pubDate>Mon, 17 Jul 2023 03:16:11 +0000</pubDate>
      
      <guid>/docs-csm/en-13/background/ncn_kernel/</guid>
      <description>Kernel This page provides information on the Linux kernel in the NCN.
 Locks Module Blacklisting Parameters  biosdevname ifname ip psi pcie_ports transparent_hugepage console iommu metal.server metal.no-wipe ds rootfallback initrd root rd.live.ram rd.writable.fsimg rd.skipfsck rd.live.squashimg rd.live.overlay rd.live.overlay.thin rd.live.overlay.overlayfs rd.luks rd.luks.crypttab rd.lvm.conf rd.lvm rd.auto rd.md rd.dm rd.neednet rd.peerdns rd.md.waitclean rd.multipath rd.md.conf rd.bootif hostname rd.net.timeout.carrier rd.net.timeout.ifup rd.net.timeout.iflink rd.net.dhcp.retry rd.net.timeout.ipv6auto rd.net.timeout.ipv6dad append nosplash quiet crashkernel log_buf_len rd.retry rd.shell xname   Versioning  Locks The Kernel version is controlled by the kernel-default package, and this package is locked.</description>
    </item>
    
    <item>
      <title>NCN Mounts And Filesystems</title>
      <link>/docs-csm/en-13/background/ncn_mounts_and_filesystems/</link>
      <pubDate>Mon, 17 Jul 2023 03:16:11 +0000</pubDate>
      
      <guid>/docs-csm/en-13/background/ncn_mounts_and_filesystems/</guid>
      <description>NCN Mounts and Filesystems The management nodes use drive storage for persistence and block storage. This page outlines reference information for these disks, their partition tables, and their management.
 Disk layout quick-reference tables OverlayFS and persistence  SQFSRAID and ROOTRAID overlays Helpful commands OverlayFS example  mount command losetup command lsblk command   Persistent directories  Layering: Upper and lower directory Layering: Real world example   OverlayFS control  Reset toggles Reset on next boot Reset on every boot Re-sizing the persistent overlay Thin overlay feature     metalfs Old/retired FS labels  Disk layout quick-reference tables The table below represents all recognizable FS labels on any given management node, varying slightly by node role (Kubernetes master or Kubernetes worker).</description>
    </item>
    
    <item>
      <title>NCN Networking</title>
      <link>/docs-csm/en-13/background/ncn_networking/</link>
      <pubDate>Mon, 17 Jul 2023 03:16:11 +0000</pubDate>
      
      <guid>/docs-csm/en-13/background/ncn_networking/</guid>
      <description>NCN Networking Non-compute nodes and compute nodes have different network interfaces used for booting; this topic focuses on the network interfaces for management nodes.
Topics  NCN network interfaces Device naming Vendor and bus ID identification  NCN network interfaces The following table includes information about the different NCN network interfaces:
   Name Type MTU     mgmt0 Port 1 Slot 1 on the SMNET card 9000   mgmt1 Port 1 Slot 2 on the SMNET card 9000   bond0 LACP link aggregate of mgmt0 and mgmt1 9000   bond0.</description>
    </item>
    
    <item>
      <title>NCN Operating System Releases</title>
      <link>/docs-csm/en-13/background/ncn_operating_system_releases/</link>
      <pubDate>Mon, 17 Jul 2023 03:16:11 +0000</pubDate>
      
      <guid>/docs-csm/en-13/background/ncn_operating_system_releases/</guid>
      <description>NCN Operating System Releases The NCNs define their products per image layer:
 Management node SquashFS images are always SLE_HPC (SuSE High Performance Computing) Utility Storage nodes Ceph Images are always SLE_HPC (SuSE High Performance Computing) with SES (SuSE Enterprise Storage)  The sles-release RPM is uninstalled for NCNs, and instead, the sle_HPC-release RPM is installed. These both provide the same files, but differ for os-release and /etc/product.d/baseproduct.
The ses-release RPM is installed on top of the sle_HPC-release RPM in the Ceph images.</description>
    </item>
    
    <item>
      <title>Plan Of Record</title>
      <link>/docs-csm/en-13/background/ncn_plan_of_record/</link>
      <pubDate>Mon, 17 Jul 2023 03:16:11 +0000</pubDate>
      
      <guid>/docs-csm/en-13/background/ncn_plan_of_record/</guid>
      <description>Plan of Record This document outlines the hardware necessary to meet CSM&amp;rsquo;s Plan of Record (PoR). This serves as the minimum, necessary pieces required per each server in the management plane.
 If the system&amp;rsquo;s NICs do not align to the PoR NICs outlined below (e.g. Onboard NICs are used instead of PCIe), then follow Customize PCIe Hardware before booting the NCN(s). If there are more disks than what is listed below in the PoR for disks, then follow Customize Disk Hardware before booting the NCN(s).</description>
    </item>
    
    <item>
      <title>Firmware</title>
      <link>/docs-csm/en-13/background/ncn_firmware/</link>
      <pubDate>Mon, 17 Jul 2023 03:16:10 +0000</pubDate>
      
      <guid>/docs-csm/en-13/background/ncn_firmware/</guid>
      <description>Firmware This page describes the non-compute node&amp;rsquo;s (NCN) firmware; the minimum versions, and how to interrogate information from an NCN.
 Minimum Versions BMC Interrogation  Minimum Versions This section outlines the minimum firmware versions for a CSM install.
 NOTE Minimum firmware versions refers to versions that CSM has tested and verified to work for a CSM installation. Using older versions is not recommended, and user experience may vary.</description>
    </item>
    
    <item>
      <title>Kernel Dumps</title>
      <link>/docs-csm/en-13/background/ncn_kdump/</link>
      <pubDate>Mon, 17 Jul 2023 03:16:10 +0000</pubDate>
      
      <guid>/docs-csm/en-13/background/ncn_kdump/</guid>
      <description>Kernel Dumps  What is kdump? Usage  Configuration Dracut Enabling / disabling   Analyzing a dump Troubleshooting  kdump has hung Resetting kdump    What is kdump? At a high-level, kdump is a Linux tool that takes a dump of the system memory at the time of a crash for analysis. This dump is taken on a local disk, or it can be taken on a network drive.</description>
    </item>
    
    <item>
      <title>NCN Boot Workflow</title>
      <link>/docs-csm/en-13/background/ncn_boot_workflow/</link>
      <pubDate>Mon, 17 Jul 2023 03:16:10 +0000</pubDate>
      
      <guid>/docs-csm/en-13/background/ncn_boot_workflow/</guid>
      <description>NCN Boot Workflow Non-compute nodes can boot from two sources:
 Network/PXE Disk  Topics  Determine the current boot order Reasons to change the boot order after CSM install Determine if NCNs booted via disk or PXE Set BMCs to DHCP Boot order overview Setting boot order Trimming boot order Example boot orders Reverting changes Locating USB device  Determine the current boot order Under normal operations, the NCNs use the following boot order:</description>
    </item>
    
    <item>
      <title>NCN Images</title>
      <link>/docs-csm/en-13/background/ncn_images/</link>
      <pubDate>Mon, 17 Jul 2023 03:16:10 +0000</pubDate>
      
      <guid>/docs-csm/en-13/background/ncn_images/</guid>
      <description>NCN Images The management nodes boot from NCN images which are created from layers on top of a common base image. The common image is customized with a kubernetes layer for the master nodes and worker nodes. The common image is customized with a storage-ceph layer for the utility storage nodes.
Topics  Overview of NCN Images LiveCD Server  Details Overview of NCN Images There are several flavors of NCN images, each share a common base image.</description>
    </item>
    
    <item>
      <title>Non-compute Node Bios</title>
      <link>/docs-csm/en-13/background/ncn_bios/</link>
      <pubDate>Mon, 17 Jul 2023 03:16:10 +0000</pubDate>
      
      <guid>/docs-csm/en-13/background/ncn_bios/</guid>
      <description>Non-Compute Node BIOS This page specifies the BIOS settings that are desirable for non-compute nodes.
 NOTES
 Any tunables on this page are in the interest of performance and stability. If either of those facets seem to be infringed by any of the content on this page, then contact HPE Cray for support. The table below declares desired settings; unlisted settings should remain at vendor-default. This table may be expanded as new settings are adjusted.</description>
    </item>
    
    <item>
      <title>Certificate Authority</title>
      <link>/docs-csm/en-13/background/certificate_authority/</link>
      <pubDate>Mon, 17 Jul 2023 03:16:09 +0000</pubDate>
      
      <guid>/docs-csm/en-13/background/certificate_authority/</guid>
      <description>Certificate Authority While a system is being installed for the first time, a certificate authority (CA) is needed. This can be generated for a system, or one can be supplied from a customer intermediate CA. Outside of a new installation, there is no supported method to rotate or change the platform CA in this release.
Topics  Overview Use default platform-generated CA Customize platform-generated CA Use external CA  Overview At install time, a PKI certificate authority (CA) can either be generated for a system, or a customer can opt to supply their own intermediate CA.</description>
    </item>
    
    <item>
      <title>Cray System Management - Release Notes</title>
      <link>/docs-csm/en-13/release_notes/</link>
      <pubDate>Mon, 17 Jul 2023 03:16:09 +0000</pubDate>
      
      <guid>/docs-csm/en-13/release_notes/</guid>
      <description>Cray System Management (CSM) - Release Notes CSM 1.3 contains approximately 500 changes spanning bug fixes, new feature development, and documentation improvements. This page lists some of the highlights.
New Monitoring  Temperature hardware monitoring dashboard for NCNs Support for export of SNMP data from multiple switches for population of SNMP Export grafana panel Space monitoring improvements - included volumes other than root file system  DHCP Changed in DHCP  Kea: Fixed a bug that could cause the auto-repair logic to fail due to an incorrect index  DNS Changed in DNS  ExternalDNS: Fixed a bug where cray-externaldns-manager could panic if it couldn&amp;rsquo;t connect to PowerDNS on startup PowerDNS: Changed powerdns-manager SLS error message to debug PowerDNS: Fixed bug that caused powerdns-manager and externaldns-manager to update the same record PowerDNS: Fixed a bug that could cause powerdns-manager to intermittently crash when performing a lookup for an existing TSIG key PowerDNS: powerdns-manager will now retry attempts to add a TSIG key PowerDNS: powerdns-manager will now create PTR records that are not created by external-dns Unbound: Changed cray-dns-unbound MaxUnavailable default from 0 to 1 to avoid issues when evicting pods from NCNs  Management Network Added in Management Network  Documentation: Added procedure to migrate from the customer access network (CAN) to the customer high-speed network (CHN), allowing user traffic over the HSN instead of the NMN (This is an extension of the bi-furcated CAN feature that shipped in CSM 1.</description>
    </item>
    
    <item>
      <title>Cray System Management Documentation</title>
      <link>/docs-csm/en-13/readme/</link>
      <pubDate>Mon, 17 Jul 2023 03:16:09 +0000</pubDate>
      
      <guid>/docs-csm/en-13/readme/</guid>
      <description>Cray System Management Documentation  Scope and audience Table of contents Copyright and license  Scope and audience The documentation included here describes the Cray System Management (CSM) software, how to install or upgrade CSM software, and related supporting operational procedures to manage an HPE Cray EX system. CSM software is the foundation upon which other software product streams for the HPE Cray EX system depend.
The CSM installation prepares and deploys a distributed system across a group of management nodes organized into a Kubernetes cluster which uses Ceph for utility storage.</description>
    </item>
    
    <item>
      <title>Non-compute Nodes</title>
      <link>/docs-csm/en-13/background/readme/</link>
      <pubDate>Mon, 17 Jul 2023 03:16:09 +0000</pubDate>
      
      <guid>/docs-csm/en-13/background/readme/</guid>
      <description>Non-Compute Nodes This page gives a high-level overview of the environment present on the Non-Compute Nodes (NCNs).
Topics  Pre-Install Toolkit Certificate Authority Hardware Requirements  Firmware   Operating System  Kernel Kernel Dumps Kubernetes Python    Pre-Install Toolkit The Pre-Install Toolkit (PIT) is a framework for deploying NCNs from an &amp;ldquo;NCN-like&amp;rdquo; environment. The PIT can be used for:
 bare-metal discovery and deployment fresh installations and reinstallation of Cray System Management (CSM) recovery of one or more NCNs  Certificate Authority For information pertaining to the non-compute node certificate authority (CA), see certificate authority.</description>
    </item>
    
  </channel>
</rss>
